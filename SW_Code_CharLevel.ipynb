{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"07105a38-167d-4080-86f1-e28bbc172b85","_uuid":"b7f3779a-a199-4bcc-b86c-6eba030528bd","trusted":true},"source":["### Установка и импорт всех необходимых зависимостей"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"16d1f1be-6c5b-4cf0-af02-3f589e64f949","_uuid":"4b94f1c3-c161-4222-abfa-e3fb66e4880d","collapsed":false,"execution":{"iopub.execute_input":"2023-12-05T00:24:22.975308Z","iopub.status.busy":"2023-12-05T00:24:22.974964Z","iopub.status.idle":"2023-12-05T00:25:21.009437Z","shell.execute_reply":"2023-12-05T00:25:21.008135Z","shell.execute_reply.started":"2023-12-05T00:24:22.975279Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["!pip install -q razdel\n","!pip install -q pymorphy2\n","!pip install -q git+https://github.com/ahmados/rusynonyms.git\n","!pip install -q natasha"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e8af1228-dfb5-4d7f-bbf6-d1cb72552c66","_uuid":"11ecb4af-671c-4f65-8913-9c95f7f796bd","collapsed":false,"execution":{"iopub.execute_input":"2023-12-05T00:25:21.011988Z","iopub.status.busy":"2023-12-05T00:25:21.011633Z","iopub.status.idle":"2023-12-05T00:25:35.212945Z","shell.execute_reply":"2023-12-05T00:25:35.211995Z","shell.execute_reply.started":"2023-12-05T00:25:21.011954Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["import xml.etree.ElementTree as ET\n","import pandas as pd\n","\n","import nltk\n","from nltk.corpus import stopwords\n","import re\n","import pymorphy2\n","from razdel import tokenize\n","from razdel import sentenize\n","import string\n","from natasha import (\n","    MorphVocab,\n","    NewsMorphTagger,\n","    NewsEmbedding,\n","    Segmenter,\n","    NewsSyntaxParser,\n","    Doc\n",")\n","\n","import torch\n","import tensorflow_hub as hub\n","from torch import nn\n","from torch.utils.data import Dataset, DataLoader\n","import transformers\n","import numpy as np\n","\n","from tqdm import tqdm\n","import os\n","import sys\n","from typing import *\n","\n","from lime.lime_text import LimeTextExplainer\n","import shap\n","\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","rus_stopwords = stopwords.words('russian')\n","punctuation = list(string.punctuation)"]},{"cell_type":"markdown","metadata":{"_cell_guid":"d7cacad4-4f63-43c2-93b1-c9c180bf748d","_uuid":"4351a3ac-2754-4156-96ae-b1866e7f6e82","trusted":true},"source":["### Работа с данными (kaggle)"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"19eb260a-7fe2-42e3-ac1e-9657a713481c","_uuid":"5b40ffa6-7032-4f5c-9c17-3cdf66f4a633","collapsed":false,"execution":{"iopub.execute_input":"2023-12-05T00:25:35.215381Z","iopub.status.busy":"2023-12-05T00:25:35.214305Z","iopub.status.idle":"2023-12-05T00:25:35.219864Z","shell.execute_reply":"2023-12-05T00:25:35.218991Z","shell.execute_reply.started":"2023-12-05T00:25:35.215333Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["datasets_folder = '/kaggle/input/sw-datasets/Russian-Sentiment-Analysis-Evaluation-Datasets'\n","datasets = ['SentiRuEval-2015-telecoms', 'SentiRuEval-2015-banks', 'SentiRuEval-2016-banks', 'SentiRuEval-2016-telecoms']\n","samples = ['test.xml', 'train.xml', 'test_etalon.xml']"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"10ad2541-7c34-4b11-a6d4-8ef30594e6e0","_uuid":"77e7298c-cf2e-4be8-a178-de925ce239f3","collapsed":false,"execution":{"iopub.execute_input":"2023-12-05T00:25:35.223625Z","iopub.status.busy":"2023-12-05T00:25:35.222873Z","iopub.status.idle":"2023-12-05T00:25:36.540186Z","shell.execute_reply":"2023-12-05T00:25:36.539188Z","shell.execute_reply.started":"2023-12-05T00:25:35.223593Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["def extract_data(path: str) -> pd.DataFrame:\n","    \"\"\"\n","    функция для извлечения данных из xml\n","    \"\"\"\n","    tree = ET.parse(path)\n","    root = tree.getroot()\n","    DataFrame = dict()\n","    database = root.findall('database')[0]\n","    DataFrame_columns = list()\n","\n","    for idx, table in enumerate(database.findall('table')):\n","        for column in table.findall('column'):\n","            DataFrame[column.attrib['name']] = list()\n","            DataFrame_columns.append(column.attrib['name'])\n","        if idx == 0:\n","            break\n","\n","    for table in database.findall('table'):\n","        for column in table.findall('column'):\n","            DataFrame[column.attrib['name']].append(column.text)\n","\n","    data = pd.DataFrame(DataFrame, columns=DataFrame_columns)\n","    return data\n","\n","# инициализация всех путей (kaggle)\n","banks_dataset = datasets[2]\n","path2samples = os.path.join(datasets_folder, banks_dataset)\n","banks = ['sberbank', 'vtb', 'gazprom', 'alfabank', 'bankmoskvy', 'raiffeisen', 'uralsib', 'rshb']\n","\n","path2test = os.path.join(path2samples, samples[2])\n","data_test = extract_data(path2test)\n","\n","path2train = os.path.join(path2samples, samples[1])\n","data_train = extract_data(path2train)"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e465b3aa-afb3-4491-ad48-3a8f07c88a7b","_uuid":"7c270cb9-6e6d-44e5-8aa9-5b9a81759385","collapsed":false,"execution":{"iopub.execute_input":"2023-12-05T00:25:36.541732Z","iopub.status.busy":"2023-12-05T00:25:36.541422Z","iopub.status.idle":"2023-12-05T00:25:44.012133Z","shell.execute_reply":"2023-12-05T00:25:44.011345Z","shell.execute_reply.started":"2023-12-05T00:25:36.541705Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["def extract_text_features(data: pd.DataFrame) -> pd.DataFrame:\n","    \"\"\"\n","    функция для первичной обработки текста от лишних символов\n","    \"\"\"\n","    extracted_data = dict()\n","    extracted_data['text'] = list()\n","    extracted_data['0class'] = list()\n","    extracted_data['1class'] = list()\n","\n","    for idx in range(len(data)):\n","        row = data.iloc[idx, :]\n","        banks_review = row[banks]\n","        unique_labels = set(banks_review)\n","        unique_labels.remove('NULL')\n","\n","        # убираем все ненужные знаки\n","        filtered_text = re.sub('http[A-z|:|.|/|0-9]*', '', row['text']).strip()\n","        filtered_text = re.sub('@\\S*', '', filtered_text).strip()\n","        filtered_text = re.sub('#', '', filtered_text).strip()\n","        new_text = filtered_text\n","\n","        # сохраняем только уникальные токены (без придатка xml NULL)\n","        unique_labels = list(unique_labels)\n","        while len(unique_labels) < 2:\n","            unique_labels.append(unique_labels[-1])\n","        extracted_data['text'].append(new_text)\n","        for idx, label in enumerate(unique_labels):\n","            text_label = int(label) + 1\n","            extracted_data[f'{idx}' + 'class'].append(text_label)\n","\n","    extracted_data = pd.DataFrame(extracted_data)\n","    \n","    # возвращаем dataframe\n","    return extracted_data\n","\n","extracted_test = extract_text_features(data_test)\n","extracted_train = extract_text_features(data_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"553036a6-e0cb-4859-9f43-fc8dd418894b","_uuid":"d874a823-0b6d-443d-b0cf-fee72b75b727","collapsed":false,"execution":{"iopub.execute_input":"2023-12-05T00:25:44.013494Z","iopub.status.busy":"2023-12-05T00:25:44.013214Z","iopub.status.idle":"2023-12-05T00:25:44.020766Z","shell.execute_reply":"2023-12-05T00:25:44.019838Z","shell.execute_reply.started":"2023-12-05T00:25:44.013469Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# пример твита из датасета\n","extracted_test.iloc[3308].text"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d6af3578-6851-46ac-81ed-247202502f82","_uuid":"90899eeb-ca26-4010-9d08-888db203f310","collapsed":false,"execution":{"iopub.execute_input":"2023-12-05T00:25:44.022182Z","iopub.status.busy":"2023-12-05T00:25:44.021896Z","iopub.status.idle":"2023-12-05T00:25:44.673339Z","shell.execute_reply":"2023-12-05T00:25:44.672421Z","shell.execute_reply.started":"2023-12-05T00:25:44.022136Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# анализ распределения таргетов на твитах\n","fig, axes = plt.subplots(1, 2, figsize=(8, 5))\n","plt.subplots_adjust(hspace=0.15, wspace=0.3)\n","\n","graph1 = sns.countplot(data=extracted_train, x='0class', ax=axes[0])\n","graph1.set(xlabel='class_num', ylabel='amount of class', title='Amount of classes according 1 label')\n","graph1.grid(True)\n","\n","graph2 = sns.countplot(data=extracted_train, x='1class', ax=axes[1])\n","graph2.set(xlabel='class_num', ylabel='amount of class', title='Amount of classes according 2 label')\n","graph2.grid(True)\n","\n","None"]},{"cell_type":"markdown","metadata":{"_cell_guid":"84c3b206-9192-44d8-b0c3-99030962fbf1","_uuid":"27a39401-ac18-4df4-b723-57d39d511fb7","trusted":true},"source":["### Инициализируем модель (fine-tune) для решения нашей задачи классификации"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d5bd2ed1-e483-4753-8c53-c68e55513a61","_uuid":"dc500355-0d1f-44b3-b218-74d295d154d1","collapsed":false,"execution":{"iopub.execute_input":"2023-12-05T00:25:44.675188Z","iopub.status.busy":"2023-12-05T00:25:44.674891Z","iopub.status.idle":"2023-12-05T00:26:22.336937Z","shell.execute_reply":"2023-12-05T00:26:22.336033Z","shell.execute_reply.started":"2023-12-05T00:25:44.675161Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["learning_rate = 1e-05\n","\n","\n","class BERTmy(torch.nn.Module):\n","    def __init__(self, n_classes: int) -> None:\n","        super(BERTmy, self).__init__()\n","        self.rubert = transformers.AutoModel.from_pretrained(\n","            \"DeepPavlov/rubert-base-cased-sentence\"\n","        )\n","        self.tokenizer = transformers.AutoTokenizer.from_pretrained(\n","            \"DeepPavlov/rubert-base-cased-sentence\", \n","            do_lower_case=True,\n","            add_additional_tokens=True\n","        )\n","        \n","        hidden_size_output = self.rubert.config.hidden_size\n","        self.classifier = torch.nn.Sequential(\n","            torch.nn.Linear(hidden_size_output, hidden_size_output, bias=True),\n","            torch.nn.Dropout(0.05),\n","            torch.nn.ReLU(),\n","            torch.nn.Linear(hidden_size_output, n_classes),\n","        )\n","\n","    def forward(\n","        self, input_ids: torch.Tensor, attention_mask: torch.Tensor, \n","        token_type_ids: torch.Tensor, output_attentions: bool=False\n","    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n","        rubert_output = self.rubert(\n","            input_ids=input_ids,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids,\n","            return_dict=True,\n","            output_attentions=output_attentions\n","        )\n","        if not output_attentions:\n","            pooled = rubert_output['pooler_output']\n","        else:\n","            pooled, attentions = rubert_output['pooler_output'], rubert_output['attentions']\n","\n","        output = self.classifier(pooled)\n","\n","        if not output_attentions:\n","            return output\n","        else:\n","            return output, attentions\n","    \n","    def configure_optimizer(\n","        self, use_scheduler: bool=False\n","    ) -> torch.optim:\n","        # freeze part of params\n","        encoder_size = 0\n","        for param in self.rubert._modules['encoder'].parameters():\n","            encoder_size += 1\n","        encoder_size_half = encoder_size // 2\n","        for idx, param in enumerate(self.rubert._modules['encoder'].parameters()):\n","            param.requires_grad = False\n","            if idx >= encoder_size_half:\n","                break\n","        \n","        # Adam\n","        optimizer = torch.optim.Adam(\n","            params=[\n","                {'params':self.rubert._modules['embeddings'].parameters(), 'lr':4e-6},\n","                {'params':self.rubert._modules['encoder'].parameters(), 'lr':4e-6},\n","                {'params':self.rubert._modules['pooler'].parameters(), 'lr':4e-6},\n","                {'params':self.classifier.parameters(), 'lr':9e-5}\n","            ],\n","            lr=learning_rate\n","        )\n","        if use_scheduler:\n","            # scheduler\n","            scheduler = torch.optim.lr_scheduler.ExponentialLR(\n","                optimizer, gamma=0.96\n","            )\n","        \n","            return optimizer, scheduler\n","        \n","        else:\n","            return optimizer\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","num_cls = len(pd.unique(extracted_train['0class']))\n","bert = BERTmy(num_cls)\n","if torch.cuda.is_available():\n","    bert = bert.cuda()\n","optimizer, scheduler = bert.configure_optimizer(use_scheduler=True)"]},{"cell_type":"markdown","metadata":{"_cell_guid":"a021a9f9-44ed-42a5-84c6-848e064b38a7","_uuid":"46a83749-640d-4bd6-9be0-7623bc16c690","trusted":true},"source":["### Инициализируем class для нашего датасета"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4fe85b1d-d8b2-4be9-8b4e-dde97c2a4d42","_uuid":"9ba9fdee-22c8-4555-a26c-0bcec67ca8a9","collapsed":false,"execution":{"iopub.execute_input":"2023-12-05T00:26:22.338705Z","iopub.status.busy":"2023-12-05T00:26:22.338263Z","iopub.status.idle":"2023-12-05T00:26:22.349789Z","shell.execute_reply":"2023-12-05T00:26:22.348843Z","shell.execute_reply.started":"2023-12-05T00:26:22.338659Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["train_batch_size = 32\n","val_batch_size = 16\n","\n","class SentimentData(Dataset):\n","    # инициализация датасета\n","    def __init__(\n","        self, dataframe: pd.DataFrame, mode: str, \n","        col_name: str, split_param: float=0.9\n","    ) -> None:\n","        self.mode = mode # train/test\n","        self.data = dataframe # data\n","        self.col_name = col_name # column for analyzing\n","        \n","        data_size = self.data.shape[0]\n","        if self.mode in ['val', 'train']:\n","            if self.mode == 'train':\n","                self.data = self.data.iloc[:int(data_size * split_param)]\n","            else:\n","                self.data = self.data.iloc[int(data_size * split_param):]\n","        \n","        assert self.mode in ['val', 'train', 'test']\n","\n","    # для получения размера датасета\n","    def __len__(self) -> int:\n","        return self.data.shape[0]\n","\n","    # для получения элемента по индексу\n","    def __getitem__(\n","        self, index: int\n","    ) -> Dict[str, Union[str, torch.Tensor]]:\n","        text = self.data.iloc[index][self.col_name]\n","        target1 = self.data.iloc[index]['0class']\n","        target2 = self.data.iloc[index]['1class']\n","\n","        return {\n","            'text': text,\n","            'target1': torch.tensor(target1, dtype=torch.long),\n","            'target2': torch.tensor(target2, dtype=torch.long)\n","        }"]},{"cell_type":"markdown","metadata":{"_cell_guid":"ec214cce-6c71-4987-b673-f4f59b6e29f2","_uuid":"d72254d0-e70d-443a-9506-34554ec96efe","trusted":true},"source":["### Инициализируем наши DataLoaders"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b5110fba-0208-4967-90fb-04bc71a68957","_uuid":"be40ab48-96e2-4c10-af39-36066c75dfc4","collapsed":false,"execution":{"iopub.execute_input":"2023-12-05T00:26:22.357073Z","iopub.status.busy":"2023-12-05T00:26:22.356120Z","iopub.status.idle":"2023-12-05T00:26:24.425934Z","shell.execute_reply":"2023-12-05T00:26:24.424890Z","shell.execute_reply.started":"2023-12-05T00:26:22.357035Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["train = SentimentData(\n","    dataframe=extracted_train,\n","    split_param=1.0,\n","    mode='train',\n","    col_name='text'\n",")\n","\n","val = SentimentData(\n","    dataframe=extracted_train,\n","    mode='val',\n","    col_name='text'\n",")\n","\n","test = SentimentData(\n","    dataframe=extracted_test,\n","    mode='test',\n","    col_name='text'\n",")\n","\n","train_loader = DataLoader(train, batch_size=train_batch_size, shuffle=True)\n","# val_loader = DataLoader(val, batch_size=val_batch_size, shuffle=False)\n","loaders = {\n","    'train': train_loader,\n","    # 'val': val_loader\n","}"]},{"cell_type":"markdown","metadata":{"_cell_guid":"f4164662-55b5-43d0-ad49-7c57ea8eb62c","_uuid":"6c111c3b-95c7-4dc8-ba88-8f8bbce229dd","trusted":true},"source":["### Дообучение модели"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8ab49c8f-c3fa-4786-a63b-fe6708927100","_uuid":"c349e8e4-a562-4927-aaa2-ac14f5dcf8df","collapsed":false,"execution":{"iopub.execute_input":"2023-12-05T00:26:24.428149Z","iopub.status.busy":"2023-12-05T00:26:24.427782Z","iopub.status.idle":"2023-12-05T00:26:24.445445Z","shell.execute_reply":"2023-12-05T00:26:24.444487Z","shell.execute_reply.started":"2023-12-05T00:26:24.428115Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["def train_model(\n","    epochs: int, model: torch.nn.Module, loaders: List[DataLoader], \n","    optimizer: torch.optim, scheduler: torch.optim.lr_scheduler\n",") -> torch.nn.Module:\n","    # cross entropy loss\n","    loss_function1 = torch.nn.CrossEntropyLoss()\n","    loss_function2 = torch.nn.CrossEntropyLoss()\n","    rubert_tokenizer = bert.tokenizer\n","    \n","    # извлечение DataLoaders\n","    if len(loaders) > 1:\n","        train_loader = loaders['train']\n","        val_loader = loaders['val']\n","        steps_per_epoch = [('train', train_loader), ('val', val_loader)]\n","    else:\n","        train_loader = loaders['train']\n","        steps_per_epoch = [('train', train_loader)]\n","\n","    # обучение по эпохам\n","    for epoch in range(epochs):\n","        for mode, loader in steps_per_epoch:\n","            # сохранение статистик\n","            train_loss = 0\n","            n_correct = 0\n","            processed_data = 0\n","            \n","            # train/val \n","            if mode == 'train':\n","                model.train()\n","                requires_grad_mode = True\n","            else:\n","                model.eval()\n","                requires_grad_mode = False\n","            \n","            # проход по батчам\n","            for data in tqdm(loader):\n","                # обнуляем градиенты\n","                optimizer.zero_grad()\n","\n","                # извлечение входных данных для модели\n","                inputs = rubert_tokenizer(\n","                    data['text'], padding=True, truncation=True, \n","                    add_special_tokens=True, return_tensors='pt'\n","                )\n","                ids = inputs['input_ids'].to(device)\n","                mask = inputs['attention_mask'].to(device)\n","                token_type_ids = inputs[\"token_type_ids\"].to(device)\n","                target1 = data['target1'].to(device)\n","                target2 = data['target2'].to(device)\n","                \n","                # устанавливаем необходимость вычислять/не_вычислять градиенты\n","                with torch.set_grad_enabled(requires_grad_mode):\n","                    outputs = model(ids, mask, token_type_ids)\n","                    preds = torch.argmax(outputs.data, dim=1)\n","\n","                    # настраиваем модели на конкретный target\n","                    if all(target1 == target2):\n","                        loss1 = loss_function1(outputs, target1)\n","                        train_loss += loss1.item() * outputs.size(0)\n","                        n_correct += torch.sum(preds == target1)\n","                        if mode == 'train':\n","                            # вычисляем градиенты и обновляем веса\n","                            loss1.backward()\n","                            optimizer.step()\n","                    # если у твита более чем 1 метка, то настраиваем на обе\n","                    else:\n","                        loss1 = loss_function1(outputs, target1) * 0.5\n","                        loss2 = loss_function2(outputs, target2) * 0.5\n","                        loss_all = loss1 + loss2\n","                        train_loss += loss_all.item() * outputs.size(0)\n","\n","                        mask_singular = target1 == target2\n","                        mask_multiple = target1 != target2\n","                        singular = preds[mask_singular]\n","                        n_correct += torch.sum(singular == target1[mask_singular])\n","                        multiple = preds[mask_multiple]\n","                        n_correct += torch.sum((multiple == target1[mask_multiple]) & (multiple == target2[mask_multiple]))\n","                        if mode == 'train':\n","                            # вычисляем градиенты и обновляем веса\n","                            loss_all.backward()\n","                            optimizer.step()     \n","                    processed_data += outputs.size(0)\n","\n","            # вычисляем ошибку и точность прогноза на эпохе\n","            loader_loss = train_loss / processed_data\n","            loader_acc = n_correct.cpu().numpy() / processed_data\n","            print(f'{epoch + 1} epoch with {mode} mode has: {loader_loss} loss, {loader_acc} acc')\n","        \n","        # делаем шаг для sheduler оптимайзера\n","        scheduler.step()\n","\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"576a6633-eeda-466e-97fa-f64788acaa7d","_uuid":"d67b77c4-042b-4279-b2ec-ae9a6080515c","collapsed":false,"execution":{"iopub.execute_input":"2023-12-03T19:16:22.833729Z","iopub.status.busy":"2023-12-03T19:16:22.833346Z","iopub.status.idle":"2023-12-03T19:23:23.512936Z","shell.execute_reply":"2023-12-03T19:23:23.511938Z","shell.execute_reply.started":"2023-12-03T19:16:22.833699Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["epochs = 12\n","bert = train_model(epochs, bert, loaders, optimizer, scheduler)"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2fa670b4-8330-4a79-ac62-d867928544f9","_uuid":"19503e86-4d81-4fd8-8a6e-f0f6f9891405","collapsed":false,"execution":{"iopub.execute_input":"2023-12-05T00:26:24.446797Z","iopub.status.busy":"2023-12-05T00:26:24.446511Z","iopub.status.idle":"2023-12-05T00:26:33.900263Z","shell.execute_reply":"2023-12-05T00:26:33.899306Z","shell.execute_reply.started":"2023-12-05T00:26:24.446773Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["mode_process = input('Load weights? (y/n)')\n","if mode_process == 'n':\n","    torch.save(bert.state_dict(), 'bert_weights_pooled.pth')\n","elif mode_process == 'y':\n","    bert.load_state_dict(torch.load('/kaggle/input/bert-weights-better/bert_weights_pooled.pth'))\n","else:\n","    assert mode_process in ['n', 'y']\n","bert.eval()\n","None"]},{"cell_type":"markdown","metadata":{"_cell_guid":"114d3f5e-6292-4964-8e88-7ed63b57100a","_uuid":"ed44b641-9164-4946-ac60-777b1b4afb6c","trusted":true},"source":["### Вычисление итоговых показателей"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"26fee109-eaec-4b0f-a90d-a4d72ea6a9e1","_uuid":"ca969426-e888-4d62-ad59-39920e55edec","collapsed":false,"execution":{"iopub.execute_input":"2023-12-04T23:17:45.982003Z","iopub.status.busy":"2023-12-04T23:17:45.981587Z","iopub.status.idle":"2023-12-04T23:17:45.997726Z","shell.execute_reply":"2023-12-04T23:17:45.996764Z","shell.execute_reply.started":"2023-12-04T23:17:45.981969Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["def calculate_accuracy(\n","    model: torch.nn.Module, SentimentData:Dataset\n",") -> float:\n","    model.eval()\n","    loader = DataLoader(SentimentData, batch_size=10, shuffle=False)\n","    n_correct = 0\n","    processed_data = 0\n","    \n","    for data in tqdm(loader):\n","        inputs = model.tokenizer(\n","            data['text'], padding=True, \n","            add_special_tokens=True, return_tensors='pt'\n","        )\n","        ids = inputs['input_ids'].to(device)\n","        mask = inputs['attention_mask'].to(device)\n","        token_type_ids = inputs[\"token_type_ids\"].to(device)\n","        target1 = data['target1'].to(device)\n","        target2 = data['target2'].to(device)\n","        \n","        with torch.no_grad():\n","            outputs = model(ids, mask, token_type_ids)\n","            preds = torch.argmax(outputs.data, dim=1)\n","            mask_singular = target1 == target2\n","            mask_multiple = target1 != target2\n","            singular = preds[mask_singular]\n","            n_correct += torch.sum(singular == target1[mask_singular])\n","            multiple = preds[mask_multiple]\n","            if len(multiple) > 0:\n","                n_correct += torch.sum((multiple == target1[mask_multiple]) & (multiple == target2[mask_multiple]))\n","            processed_data += outputs.size(0)\n","        \n","    loader_acc = n_correct.cpu().numpy() / processed_data\n","    \n","    return loader_acc\n","\n","def calculate_f1_class(\n","    model: torch.nn.Module, SentimentData: Dataset, class_num: int\n",") -> float:\n","    model.eval()\n","    loader = DataLoader(SentimentData, batch_size=10, shuffle=False)\n","    true_positive = 0\n","    false_positive, false_negative = 0, 0\n","    \n","    for data in tqdm(loader):\n","        inputs = model.tokenizer(\n","            data['text'], padding=True, \n","            add_special_tokens=True, return_tensors='pt'\n","        )\n","        ids = inputs['input_ids'].to(device)\n","        mask = inputs['attention_mask'].to(device)\n","        token_type_ids = inputs[\"token_type_ids\"].to(device)\n","        target1 = data['target1'].to(device)\n","        target2 = data['target2'].to(device)\n","        \n","        with torch.no_grad():\n","            outputs = model(ids, mask, token_type_ids)\n","            \n","            preds = torch.argmax(outputs.data, dim=1)\n","            preds = preds.cpu().numpy()\n","            target1 = target1.cpu().numpy()\n","            \n","            mask_positive = target1 == class_num\n","            mask_negative = target1 != class_num\n","            \n","            true_positive += np.sum(preds[mask_positive] == class_num)\n","            false_positive += np.sum(preds[mask_negative] == class_num)\n","            false_negative += np.sum(preds[mask_positive] != class_num)\n","        \n","    precision = true_positive / (true_positive + false_positive)\n","    recall = true_positive / (true_positive + false_negative)\n","    loader_f1 = 2 * precision * recall / (precision + recall)\n","    \n","    return loader_f1"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f327a64d-c8f8-4bab-b932-0a71697c3ef3","_uuid":"dcf7a5ab-948f-4a3f-a099-20d0781e3a0a","collapsed":false,"execution":{"iopub.execute_input":"2023-12-04T23:17:48.053738Z","iopub.status.busy":"2023-12-04T23:17:48.052882Z","iopub.status.idle":"2023-12-04T23:18:15.685946Z","shell.execute_reply":"2023-12-04T23:18:15.684965Z","shell.execute_reply.started":"2023-12-04T23:17:48.053707Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["test_acc = calculate_accuracy(bert, test)\n","class_neg_f1 = calculate_f1_class(bert, test, 0)\n","class_neu_f1 = calculate_f1_class(bert, test, 1)\n","class_pos_f1 = calculate_f1_class(bert, test, 2)"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"82265fd8-c0bd-44d7-b9d8-aca69d404cb0","_uuid":"9b2b22cf-e842-40f4-b450-2dd7bf72d097","collapsed":false,"execution":{"iopub.execute_input":"2023-12-04T23:18:15.688529Z","iopub.status.busy":"2023-12-04T23:18:15.688128Z","iopub.status.idle":"2023-12-04T23:18:15.694425Z","shell.execute_reply":"2023-12-04T23:18:15.693561Z","shell.execute_reply.started":"2023-12-04T23:18:15.688493Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# общая accuracy и f1 по классам\n","test_acc, class_neg_f1, class_neu_f1, class_pos_f1"]},{"cell_type":"markdown","metadata":{"_cell_guid":"8882bdef-e71a-4445-8823-c6cd3ac91816","_uuid":"cac0e4f4-82dd-4140-9443-c4539d51a316","trusted":true},"source":["### Backdoor attacks on neural network(adversial examples)"]},{"cell_type":"markdown","metadata":{"_cell_guid":"c7a5f905-6e00-430b-a6b1-a9cb1ea55c89","_uuid":"2def7913-246a-4e98-8733-7579e3d96e4c","trusted":true},"source":["#### USE metric for similarity between original sentence and spoiled sentence"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a2237584-a021-4633-a333-8413f2555a5f","_uuid":"e5c67eb2-846d-4e5e-aaf4-57493cfc825b","collapsed":false,"execution":{"iopub.execute_input":"2023-12-04T23:18:15.695871Z","iopub.status.busy":"2023-12-04T23:18:15.695570Z","iopub.status.idle":"2023-12-04T23:18:15.708693Z","shell.execute_reply":"2023-12-04T23:18:15.707872Z","shell.execute_reply.started":"2023-12-04T23:18:15.695846Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["def use_score(original, adversial, use_bert_encoder=False, model=None):\n","    from scipy.spatial.distance import cosine\n","    # Load pre-trained universal sentence encoder model\n","    if not use_bert_encoder:\n","        # using DAN from tensorflow\n","        use_encoder = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n","\n","        sentences_orig = list()\n","        sentences_adv = list()\n","        for pair in zip(original, adversial):\n","            orig, adv = pair\n","            sentences_orig.append(orig)\n","            sentences_adv.append(adv)\n","\n","        # get embs of texts\n","        sentences_orig_emb = use_encoder(sentences_orig)\n","        sentences_adv_emb = use_encoder(sentences_adv)\n","\n","        # calculate use_score with DAN\n","        use_scores = list()\n","        for pair in zip(sentences_orig_emb, sentences_adv_emb):\n","            orig_emb, adv_emb = pair[0], pair[1]\n","            use_score_one = 1 - cosine(orig_emb, adv_emb)\n","            use_scores.append(use_score_one)\n","    else:\n","        # using BERT itself\n","        def get_inputs(text): # get inputs for model\n","            inputs = model.tokenizer(\n","                text, padding=True, \n","                add_special_tokens=True, \n","                return_tensors='pt'\n","            )\n","            ids = inputs['input_ids'].type(torch.long).to(device)\n","            mask = inputs['attention_mask'].type(torch.long).to(device)\n","            token_type_ids = inputs[\"token_type_ids\"].type(torch.long).to(device)\n","            \n","            return ids, mask, token_type_ids\n","\n","        # calculate use_score with BERT\n","        use_scores = list()\n","        for pair in zip(original, adversial):\n","            orig, adv = pair[0], pair[1]\n","            orig_inputs = get_inputs(orig)\n","            adv_inputs = get_inputs(adv)\n","            orig_outputs = model.rubert(*orig_inputs)\n","            adv_outputs = model.rubert(*adv_inputs)\n","            orig_pooled, adv_pooled = orig_outputs[1], adv_outputs[1]\n","            orig_pooled = orig_pooled.cpu().detach().numpy()\n","            adv_pooled = adv_pooled.cpu().detach().numpy()\n","            use_score_one = 1 - cosine(orig_pooled, adv_pooled)\n","            use_scores.append(use_score_one)\n","    \n","    return use_scores, np.mean(use_scores)"]},{"cell_type":"markdown","metadata":{"_cell_guid":"2fd87276-d681-4aa6-8cc4-d5824c8f3642","_uuid":"fdf759b2-374a-4203-892d-13259da64f14","trusted":true},"source":["## Attention visualization"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e3c4968e-a083-4284-a5c4-0b88a9592f00","_uuid":"733e2079-83b7-4298-9626-9e7575f915a9","collapsed":false,"execution":{"iopub.execute_input":"2023-12-04T23:18:15.711328Z","iopub.status.busy":"2023-12-04T23:18:15.711017Z","iopub.status.idle":"2023-12-04T23:18:15.721201Z","shell.execute_reply":"2023-12-04T23:18:15.720290Z","shell.execute_reply.started":"2023-12-04T23:18:15.711294Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["def visualize_attention_one_head(tokens, attention_weights, num_layer, num_head):\n","    # works only with batch_size=1\n","    num_layer -= 1\n","    num_head -= 1\n","    assert num_head >= 0 and num_head < len(attention_weights[0][0])\n","    assert num_layer < len(attention_weights) and num_layer >= 0\n","    \n","    attention_layer = attention_weights[num_layer][0].cpu().detach().numpy()\n","    \n","    fig, ax = plt.subplots(1, 1, figsize=(9, 6))\n","\n","    g = sns.heatmap(attention_layer[num_head], annot=True, linewidth=0.1, fmt='.1g')\n","    # xlabel='weight_for_embed', ylabel='num_embed'\n","    g.set(title=f'layer: {num_layer + 1}; head: {num_head + 1} attention map')\n","    tickvalues = range(0,len(tokens) + 2)\n","    tokens = ['CLS'] + tokens + ['SEP']\n","    g.set_yticks(ticks=tickvalues ,labels=tokens, rotation='horizontal')\n","    g.set_xticks(ticks=tickvalues ,labels=tokens, rotation='vertical')\n","    ax = g\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"63054f35-5904-4e39-8871-37b2cfaf13ac","_uuid":"12f060c2-107c-4121-b231-a4079561b10c","collapsed":false,"execution":{"iopub.execute_input":"2023-12-04T23:18:15.722678Z","iopub.status.busy":"2023-12-04T23:18:15.722428Z","iopub.status.idle":"2023-12-04T23:18:15.785399Z","shell.execute_reply":"2023-12-04T23:18:15.784635Z","shell.execute_reply.started":"2023-12-04T23:18:15.722655Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["adversial_examples_char = pd.read_csv('/kaggle/input/result-data/adversial_examples_char.csv')\n","text_example = extracted_test.iloc[10].text\n","text_example_ins = adversial_examples_char['1_ins_amount_1_SpoiledText'].iloc[10]\n","text_example_del = adversial_examples_char['1_del_amount_1_SpoiledText'].iloc[10]\n","text_example_sub = adversial_examples_char['1_sub_amount_1_SpoiledText'].iloc[10]"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ede9ac5a-c196-4cc1-b7c5-0ed241060268","_uuid":"52147ee1-2a73-4137-bd24-a043e4911b09","collapsed":false,"execution":{"iopub.execute_input":"2023-12-04T23:18:15.786664Z","iopub.status.busy":"2023-12-04T23:18:15.786407Z","iopub.status.idle":"2023-12-04T23:18:15.792027Z","shell.execute_reply":"2023-12-04T23:18:15.791153Z","shell.execute_reply.started":"2023-12-04T23:18:15.786641Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["def visualize_attention_for_text(text, num_layer, num_head):\n","    text_seq = bert.tokenizer(\n","        text,\n","        padding=True,\n","        add_special_tokens=True,\n","        return_tensors='pt'\n","    ).to(device)\n","    logits, attention = bert(**text_seq, output_attentions=True)\n","    tokens = bert.tokenizer.tokenize(text)\n","    visualize_attention_one_head(tokens, attention, num_layer, num_head)"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3c3b0df9-45ac-49e2-84d7-f4241dd80267","_uuid":"8c67df3a-86ce-4bf6-b915-49ec30ea8c11","collapsed":false,"execution":{"iopub.execute_input":"2023-12-04T23:18:15.793690Z","iopub.status.busy":"2023-12-04T23:18:15.793315Z","iopub.status.idle":"2023-12-04T23:18:16.475574Z","shell.execute_reply":"2023-12-04T23:18:16.474547Z","shell.execute_reply.started":"2023-12-04T23:18:15.793657Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["visualize_attention_for_text(text_example_sub, 12, 1)"]},{"cell_type":"markdown","metadata":{"_cell_guid":"c843d9f5-bff2-4a42-a9a0-6c0ee9d461c4","_uuid":"3f317f59-a082-4c84-a625-6a96ed49979c","trusted":true},"source":["### utils for generating adversarial text"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"273f1cd4-10e9-4c45-b3b7-e8da3516c7e5","_uuid":"8ea101b3-c750-46ed-8dfe-10aee7384980","collapsed":false,"execution":{"iopub.execute_input":"2023-12-04T23:18:16.477229Z","iopub.status.busy":"2023-12-04T23:18:16.476847Z","iopub.status.idle":"2023-12-04T23:18:16.490167Z","shell.execute_reply":"2023-12-04T23:18:16.489317Z","shell.execute_reply.started":"2023-12-04T23:18:16.477194Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["key_errors = {\n","    'й': ['ц', 'ы', 'ф'],\n","    'ц': ['й', 'ы', 'у'],\n","    'у': ['ц', 'в', 'к'],\n","    'к': ['у', 'а', 'е'],\n","    'е': ['к', 'п', 'н'],\n","    'н': ['е', 'р', 'г'],\n","    'г': ['н', 'о', 'ш'],\n","    'ш': ['г', 'л', 'щ'],\n","    'щ': ['ш', 'д', 'з'],\n","    'з': ['щ', 'ж'],\n","    'х': ['ъ', 'э', 'з'],\n","    'ъ': ['э', 'х'],\n","    'ф': ['й', 'ы', 'я'],\n","    'ы': ['ц', 'в', 'ч', 'ф'],\n","    'в': ['у', 'а', 'с', 'ы'],\n","    'а': ['к', 'п', 'м', 'в'],\n","    'п': ['е', 'р', 'и', 'а'],\n","    'р': ['н', 'о', 'т', 'п'],\n","    'о': ['г', 'л', 'ь', 'р'],\n","    'л': ['ш', 'д', 'б', 'о'],\n","    'д': ['щ', 'ж', 'ю', 'л', 'б'],\n","    'ж': ['з', 'э', 'ю', 'д'],\n","    'э': ['х', 'ъ', 'ж'],\n","    'я': ['ф', 'ы', 'ч'],\n","    'ч': ['ы', 'в', 'с', 'я'],\n","    'с': ['в', 'а', 'м', 'ч'],\n","    'м': ['а', 'п', 'и', 'с'],\n","    'и': ['п', 'р', 'т', 'м'],\n","    'т': ['р', 'о', 'ь', 'и'],\n","    'ь': ['о', 'л', 'б', 'т'],\n","    'б': ['ь', 'л', 'д', 'ю'],\n","    'ю': ['д', 'ж', 'б'],\n","    'r': ['t', 'f', 'e'],\n","    't': ['y', 'f', 'e'],\n","    '0': ['9', '-'],\n","    '1': ['`', '2'],\n","    '2': ['1', '3'],\n","    '3': ['2', '4'],\n","    '4': ['3', '5'],\n","    '5': ['4', '6'],\n","    '6': ['5', '7'],\n","    '7': ['6', '8'],\n","    '8': ['7', '9'],\n","    '9': ['8', '0'],\n","    '-': ['0', '+'],\n","    'k': ['i', 'j', 'l', 'm'],\n","    '.': [',', '/', 'l', ';']\n","}\n","# получаем словарь формата: буква -> ближайшие буквы на клавиатуре"]},{"cell_type":"markdown","metadata":{},"source":["### Prepare data adversarial generating"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0dd827a1-3317-4285-9dab-79b4e96d78c8","_uuid":"da7eac17-3985-45b6-b1c5-8c0ff4a8950c","collapsed":false,"execution":{"iopub.execute_input":"2023-12-04T23:18:16.491793Z","iopub.status.busy":"2023-12-04T23:18:16.491455Z","iopub.status.idle":"2023-12-04T23:18:16.506957Z","shell.execute_reply":"2023-12-04T23:18:16.506094Z","shell.execute_reply.started":"2023-12-04T23:18:16.491762Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# выбираем текст для генерации состязательных примеров с сохранением исходной пропорции\n","limit_neu = 1300\n","limit_pos = 270\n","limit_neg = 550\n","adversial_examples_pos = extracted_test[extracted_test['0class'] == 2]\n","adversial_examples_neu = extracted_test[extracted_test['0class'] == 1]\n","adversial_examples_neg = extracted_test[extracted_test['0class'] == 0]\n","\n","adversial_examples_pos = adversial_examples_pos.head(limit_pos)\n","adversial_examples_neu = adversial_examples_neu.head(limit_neu)\n","adversial_examples_neg = adversial_examples_neg.head(limit_neg)\n","\n","adversial_examples = pd.concat([adversial_examples_pos, adversial_examples_neu, adversial_examples_neg])\n","adversial_examples_char = adversial_examples.sample(frac=1)\n","\n","print('Размер текста для генерации: ', len(adversial_examples_char))\n","print('Баланс классов: ')\n","print(np.unique(adversial_examples_char['0class'], return_counts=True))"]},{"cell_type":"markdown","metadata":{},"source":["### Work with word importance"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ff82fe94-6beb-4d7b-af9c-702319834adc","_uuid":"be5751b8-0b73-423e-9ae0-4a90461d1648","collapsed":false,"execution":{"iopub.execute_input":"2023-12-04T23:18:16.510486Z","iopub.status.busy":"2023-12-04T23:18:16.510211Z","iopub.status.idle":"2023-12-04T23:18:16.535184Z","shell.execute_reply":"2023-12-04T23:18:16.534323Z","shell.execute_reply.started":"2023-12-04T23:18:16.510462Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["def gather_back_tokens(tokens: List[str], tokens_type: str) -> str:\n","    \"\"\"\n","    для превращения токенов в предложение\n","    tokens: список токенов\n","    tokens_type: natasha или razdel\n","    \"\"\"\n","    assert tokens_type in ['razdel', 'natasha']\n","\n","    sent = ''\n","    prev_end = None\n","    for token in tokens:\n","\n","        if tokens_type == 'natasha':\n","            token_text = token['text']\n","            token_start, token_stop = token['start'], token['stop']\n","        else:\n","            token_text = token.text\n","            token_start, token_stop = token.start, token.stop\n","        \n","        if not prev_end is None:\n","            sent += (token_start - prev_end) * ' '\n","\n","        sent += token_text\n","        prev_end = token_stop\n"," \n","    return sent\n","\n","\n","# get inputs for model\n","def get_inputs(text):\n","    inputs = bert.tokenizer(\n","        text, padding=True, truncation=True, \n","        add_special_tokens=True, return_tensors='pt'\n","    )\n","\n","    ids = inputs['input_ids'].type(torch.long).to(device)\n","    mask = inputs['attention_mask'].type(torch.long).to(device)\n","    token_type_ids = inputs[\"token_type_ids\"].type(torch.long).to(device)\n","\n","    # return input for model\n","    return ids, mask, token_type_ids\n","\n","\n","def predict_text(text):\n","    \"\"\"\n","    for Lime: return probability distribution of text\n","    \"\"\"\n","    # get model outputs\n","    ids, mask, token_type_ids = get_inputs(text)\n","    with torch.no_grad():\n","        outputs = bert(ids, mask, token_type_ids)\n","    \n","    # get probs\n","    probs = torch.nn.functional.softmax(outputs, dim=1).cpu().detach().numpy()\n","\n","    return probs\n","\n","\n","def RazdelSplit(text):\n","        \n","    return [raz_tok.text for raz_tok in list(tokenize(text))]\n","\n","def NatashaSplit(text):\n","    \n","    segmenter = Segmenter()\n","    text_doc = Doc(text.lower())\n","    text_doc.segment(segmenter)\n","    \n","    return [nat_tok['text'] for nat_tok in text_doc]\n","\n","\n","# get words score to final output\n","def extract_essential_words(\n","    tokens: List[str], target: int, tok_imoprtance: str, \n","    tokens_type: str, num_samples: int=850, num_featyres: int=150\n",") -> List[Tuple[str, int]]:\n","    \"\"\"\n","    возвращает список слов по убыванию важности\n","    причем если на вход поданы токены natasha\n","    то вернет токены natasha\n","    а если на вход - токены razdel\n","    то вернет токены razdel\n","    \"\"\"\n","\n","    assert mode in ['loss', 'lime', 'shap']\n","    assert tokens_type in ['razdel', 'natasha']\n","\n","    # список для наиболее важных слов\n","    essential_words = list()\n","    \n","    # восстанавливаем текст из слов\n","    text_to_explain = gather_back_tokens(tokens,tokens_type)\n","\n","    if mode == 'lime':\n","        \n","        if tokens_type == 'razdel':\n","            Spliter = RazdelSplit\n","        elif token_type == 'natasha':\n","            Spliter = NatashaSplit\n","        # создаем Explainer\n","        explainer = LimeTextExplainer(\n","            class_names=['Neg', 'Neu', 'Pos'],\n","            split_expression=Spliter\n","        )\n","\n","        # \"объясняем\" текст\n","        explanation = explainer.explain_instance(\n","            text_to_explain, predict_text, \n","            num_features=num_features, num_samples=num_samples\n","        )\n","\n","        # создаем mapping из токена в его вес LogReg\n","        explanation_list = explanation.as_list()\n","        tok2weight = {token:weight for token, weight in explanation_list}\n","        \n","        # создаем список из токенов, их важности и позиции в тексте\n","        for token in tokens:\n","            if tokens_type == 'razdel':\n","                token_text = token.text.lower()\n","            else:\n","                token_text = token['text'].lower()\n","            \n","            essential_words.append((\n","                token, tok2weight[token_text]\n","            ))\n","        \n","        # создаем функцию сравнения важности\n","        sort_func = lambda x: np.abs(x[1])\n","    \n","    elif mode == 'shap':\n","\n","        def f(x):\n","            print(x)\n","            import time\n","            time.sleep(1)\n","            tv = torch.tensor(\n","                [\n","                    tokenizer.encode(v, padding=\"max_length\", max_length=128, truncation=True)\n","                    for v in x\n","                ]\n","            ).cuda()\n","            attention_mask = (tv != 0).type(torch.int64).cuda()\n","            outputs = model(tv, attention_mask=attention_mask)[0].detach().cpu().numpy()\n","            scores = (np.exp(outputs).T / np.exp(outputs).sum(-1)).T\n","            val = sp.special.logit(scores)\n","            return val\n","\n","        def custom_tokenizer(s, return_offsets_mapping=True):\n","            \"\"\"Custom tokenizers conform to a subset of the transformers API.\"\"\"\n","            pos = 0\n","            offset_ranges = []\n","            input_ids = []\n","            for m in re.finditer(r\"\\W\", s):\n","                start, end = m.span(0)\n","                offset_ranges.append((pos, start))\n","                input_ids.append(s[pos:start])\n","                pos = end\n","            if pos != len(s):\n","                offset_ranges.append((pos, len(s)))\n","                input_ids.append(s[pos:])\n","            out = {}\n","            out[\"input_ids\"] = input_ids\n","            if return_offsets_mapping:\n","                out[\"offset_mapping\"] = offset_ranges\n","            return out\n","\n","        masker = shap.maskers.Text(custom_tokenizer)\n","        explainer = shap.Explainer(f, masker, output_names=labels)\n","    \n","    elif mode == 'alti':\n","        \n","        pass\n","        \n","    elif mode == 'loss':\n","        \n","        loss = torch.nn.CrossEntropyLoss()\n","        \n","        # get inputs and outputs from model\n","        ids, mask, token_type_ids = get_inputs(text_to_explain)\n","        outputs = bert(ids, mask, token_type_ids)\n","\n","        # calculate loss for original text\n","        loss_score_integral = loss(outputs.cpu(), torch.tensor([target], dtype=torch.long))\n","\n","        for idx, token in enumerate(tokens):\n","            # get text without one token\n","            tokens_copy = tokens.copy()\n","            tokens_copy.pop(idx)\n","            text_to_explain = gather_back_tokens(tokens_copy, tokens_type=tokens_type)\n","\n","            # calculate loss without current word\n","            ids, mask, token_type_ids = get_inputs(text_to_explain)\n","            with torch.no_grad():\n","                outputs = bert(ids, mask, token_type_ids)\n","            loss_score_part = loss(outputs.cpu(), torch.tensor([target], dtype=torch.long))\n","            # add our score of change\n","            essential_words.append((\n","                token, (loss_score_part - loss_score_integral).cpu().detach().numpy()\n","            ))\n","            # создаем функцию сравнения важности\n","            sort_func = lambda x: x[1]\n","    \n","    # сортируем токены по важности\n","    essential_words = sorted(essential_words, key=sort_func, reverse=True)\n","\n","    # возвращаем только слова и их позиции в тексте\n","    essential_words = [(word, pos) for word, score, pos in essential_words]\n","\n","    return essential_words\n","\n","\n","def extract_random_words(\n","    tokens: List[str]\n",") -> List[Tuple[str, int]]:\n","    \"\"\"\n","    возвращает список слов в случайном порядке\n","    \"\"\"\n","    permutation = np.random.permutation(len(tokens))\n","\n","    return [tokens[idx] for idx in permutation]"]},{"cell_type":"markdown","metadata":{"_cell_guid":"32811848-cf03-4d3e-a485-607a1e879aa1","_uuid":"ae956c6a-8df7-420b-8a63-ccaef04b1ac6","trusted":true},"source":["## char-level attacks"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e083c436-9f81-4544-9205-387e75d2a8b1","_uuid":"d71ca614-f090-4867-9dd3-9a831b34223c","collapsed":false,"execution":{"iopub.execute_input":"2023-12-04T23:18:16.536717Z","iopub.status.busy":"2023-12-04T23:18:16.536449Z","iopub.status.idle":"2023-12-04T23:18:16.561665Z","shell.execute_reply":"2023-12-04T23:18:16.560878Z","shell.execute_reply.started":"2023-12-04T23:18:16.536691Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# функция для генерации состязательных примеров на уровне символов\n","def extract_spoiled_text_char_level(\n","        dataframe, words2spoil=2, \n","        sub_percent=0.15, sub_amount=1,\n","        mode2spoil='mixed', mode2amount='percent',\n","        tok_importance='loss'\n","    ):\n","\n","\n","    def get_indexes2change(\n","        sub_letter: int, token_len: int\n","    ) -> List[int]:\n","        \"\"\"\n","        функция для получения индексов букв на замену (кроме 0)\n","        \"\"\"\n","        lst_to_random = list(range(1, token_len))\n","        np_ids = np.random.choice(lst_to_random, size=sub_letter, replace=False)\n","\n","        return np_ids.tolist()\n","    \n","    def make_token_change(\n","        indexes: List[int], token: str, mode='mixed'\n","    ):\n","        \"\"\"\n","        фукнция для замены букв по индексам с использованием 4 типов замены:\n","        del: только удаление\n","        ins: только вставка\n","        sub: только замена\n","        mixed: все вместе сразу\n","        \"\"\"\n","        if mode == 'sub':\n","            # заменяем букву на позиции\n","            word = list(token.text)\n","            for idx in indexes:\n","                symbol = word[idx]\n","                try:\n","                    word[idx] = key_errors[symbol][random.randint(0, len(key_errors[symbol])-1)]\n","                except:\n","                    pass\n","            return (token.start, token.stop, ''.join(word), 0)\n","        elif mode == 'ins':\n","            # вставляем букву на позиция и увеличиваем длину токена на 1\n","            ins_count = 0\n","            word = list(token.text)\n","            indexes = sorted(indexes)\n","            for idx in indexes:\n","                symbol = word[idx+ins_count]\n","                try:\n","                    word.insert(idx+ins_count, key_errors[symbol][random.randint(0, len(key_errors[symbol]) - 1)])\n","                    ins_count += 1\n","                except:\n","                    pass\n","            return (token.start, token.stop, ''.join(word), ins_count)\n","        elif mode == 'del':\n","            # удаляем букву на позиция и уменьшаем длину токена на 1\n","            del_count = 0\n","            word = list(token.text)\n","            indexes = sorted(indexes)\n","            for idx in indexes:\n","                if len(word) == 1:\n","                    break\n","                try:\n","                    word.pop(idx-del_count)\n","                    del_count += 1\n","                except:\n","                    pass\n","            return (token.start, token.stop, ''.join(word), -del_count)\n","        elif mode == 'mixed':\n","            ins_count = 0\n","            del_count = 0\n","            word = list(token.text)\n","            # генерируем самое первое действие в слове\n","            idx2action = random.randint(0, 2)\n","            indexes = sorted(indexes)\n","            for idx in indexes:\n","                # вставляем букву на позиция и увеличиваем длину токена на 1, если ins\n","                # удаляем букву на позиция и уменьшаем длину токена на 1, если del\n","                # заменяем букву на позиции, если sub\n","                new_idx = idx+ins_count-del_count\n","                try:\n","                    if idx2action == 0:\n","                        symbol = word[new_idx]\n","                        word[new_idx] = key_errors[symbol][random.randint(0, len(key_errors[symbol]) - 1)]\n","                        idx2action += 1\n","                    elif idx2action == 1:\n","                        word.insert(new_idx, key_errors[symbol][random.randint(0, len(key_errors[symbol]) - 1)])\n","                        ins_count += 1\n","                        idx2action += 1\n","                    elif idx2action == 2:\n","                        word.pop(new_idx)\n","                        del_count += 1\n","                        idx2action = 0 \n","                except:\n","                    pass\n","            return (token.start, token.stop, ''.join(word), ins_count-del_count)\n","\n","    # spoiled texts\n","    spoiled_text = list()\n","    pbar = tqdm(dataframe['text'], leave=True, position=0)\n","    for idx, sent, target1 in zip(\n","        range(len(dataframe['text'])), \n","        dataframe['text'], \n","        dataframe['0class']\n","    ):\n","        # get tokens of our text\n","        tokens = [data for data in list(tokenize(sent.lower()))]\n","        # just one word\n","        if len(tokens) == 1:\n","            spoiled_text.append(sent)\n","            continue\n","        \n","        # выбираем определенным методом наиболее важные слова\n","        if tok_importance in ['loss', 'lime', 'shap']:\n","            word2spoil_order = extract_essential_words(\n","                tokens, target1, tok_importance, tokens_type='razdel'\n","            )\n","        # выбираем случайным образом наиболее важные слова\n","        else:\n","            word2spoil_order = extract_random_words(tokens)\n","\n","        sub_count = 0\n","        spoiled_tokens = list()\n","        for token in word2spoil_order:\n","            # get token and token's position\n","            token_len = token.stop - token.start\n","            # no way to change\n","            if token_len != 1 and sub_count < words2spoil: \n","                # count our changes\n","                if mode2amount == 'percent':\n","                    sub_letter = max(1, int(token_len * sub_percent))\n","                elif mode2amount == 'amount':\n","                    sub_letter = max(1, sub_amount)\n","                # get indexes to change\n","                indexes = get_indexes2change(sub_letter, token_len)\n","                # go through indexes\n","                spoiled_word = make_token_change(indexes, token, mode2spoil)\n","                # increase our subs\n","                sub_count += 1\n","                spoiled_tokens.append(spoiled_word)\n","            # сделали нужное количество порч\n","            if sub_count >= words2spoil:\n","                break\n","        \n","        # заменяем исходные слов в тексте испорченными\n","        shift_in_sent = 0\n","        spoiled_sent = list(sent.lower())\n","        spoiled_tokens = sorted(spoiled_tokens, key=lambda x:x[0])\n","        for spoiled in spoiled_tokens:\n","            spoiled_start, spoiled_stop, spoiled_word, word_shift = spoiled\n","            spoiled_sent[spoiled_start + shift_in_sent:spoiled_stop + shift_in_sent] = spoiled_word\n","            shift_in_sent += word_shift\n","        spoiled_sent = ''.join(spoiled_sent)\n","        spoiled_text.append(spoiled_sent)\n","        \n","        pbar.update(1)\n","        pbar.set_description(f'Total processed: {idx + 1}')\n","        \n","    return spoiled_text"]},{"cell_type":"markdown","metadata":{"_cell_guid":"8d5f880d-9df6-4595-96c2-5e31988b8d76","_uuid":"6ea8af42-a20a-45aa-8c3e-f7f9f58d8804","trusted":true},"source":["### Портим текст, вычисляем показатель use_score и accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"efa44e96-a095-433f-91c6-fe487359e517","_uuid":"1d64853f-94d1-4a6c-ac78-9bde88701737","collapsed":false,"execution":{"iopub.execute_input":"2023-12-04T23:18:16.563042Z","iopub.status.busy":"2023-12-04T23:18:16.562720Z","iopub.status.idle":"2023-12-04T23:18:16.578110Z","shell.execute_reply":"2023-12-04T23:18:16.577397Z","shell.execute_reply.started":"2023-12-04T23:18:16.563010Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["def get_scores_char_spoiled_text(\n","        model: torch.nn.Module, dataframe: pd.DataFrame, \n","        words2spoil: List[int], mode2amount: str, \n","        sub_amount: List[int], sub_percent: List[float],\n","        spoil_modes: List[str], word_importances: List[str]\n","    ) -> Tuple[Dict[str, float], Dict[str, float], Dict[str, float], pd.DataFrame]:\n","    \"\"\"\n","    dataframe: (pandas данные с текстом и метками)\n","    mode2amount: 'percent', 'amount' (по процентам или по количеству букв)\n","    words2spoil_amount: (количество слов для порчи)\n","    subs: (сколько букв испортить)\n","    subs_percent: (сколько букв в процентах от слова испортить)\n","    spoil_modes: 'mixed', 'ins', 'del', 'sub' (способы порчи текста)\n","    spoil_init: 'random', 'loss', 'lime' (тип выбора слов для порчи)\n","    \"\"\"\n","    \n","    assert mode2amount in ['percent', 'amount']\n","    assert set(word_importances) <= {'loss', 'shap', 'lime', 'random', 'alti'}\n","    assert set(spoil_modes) <= {'mixed', 'ins', 'del', 'sub'}\n","    assert all(amount > 0 for amount in words2spoil)\n","    assert all(sub >= 1 for sub in sub_amount)\n","    assert all(sub >= 0.05 for sub in sub_percent)\n","    \n","    dan_scores = dict()\n","    bert_scores = dict()\n","    acc_scores = dict()\n","    \n","    subs = sub_percent if mode2amount == 'percent' else sub_amount\n","\n","    for word_importance in word_importances:\n","        for sub in subs:\n","            for mode2spoil in spoil_modes:\n","                for words_amount in words2spoil:\n","\n","                    col_name = f'{word_importance}_{words_amount}_{mode2spoil}_{mode2amount}_{sub}_CharSpoiledText'\n","                    \n","                    # генерируем состязательные примеры\n","                    spoiled_text = extract_spoiled_text_char_level(\n","                        dataframe, words2spoil=words_amount,\n","                        sub_amount=sub, sub_percent=sub, mode2amount=mode2amount, \n","                        mode2spoil=mode2spoil, spoil_init=word_importance\n","                    )\n","                    \n","                    # сохраняем колонку со состязательными примерами\n","                    dataframe[col_name] = spoiled_text\n","                    \n","                    # считаем use score на основе представлений bert\n","                    _, use_result_char_bert = use_score(\n","                        dataframe['text'],\n","                        dataframe[col_name],\n","                        use_bert_encoder=True,\n","                        model=model\n","                    )\n","                    # считаем use score на основе dan кодировщика\n","                    _, use_result_char = use_score(\n","                        dataframe['text'],\n","                        dataframe[col_name]\n","                    )\n","\n","                    sentidata = SentimentData(\n","                        dataframe=dataframe,\n","                        mode='test',\n","                        col_name=col_name\n","                    )\n","\n","                    # замеряем качество состязательных примеров\n","                    spoiled_accuracy_char = calculate_accuracy(model, sentidata)\n","                    \n","                    # сохраняем результаты\n","                    dan_scores[col_name] = use_result_char\n","                    bert_scores[col_name] = use_result_char_bert\n","                    acc_scores[col_name] = spoiled_accuracy_char\n","                \n","    return dan_scores, bert_scores, acc_scores, dataframe"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"99b6d519-3d2f-4d94-919a-15807f99a32d","_uuid":"ad6d6ffe-47cf-46ae-958d-effdfa8438ee","collapsed":false,"execution":{"iopub.execute_input":"2023-10-25T22:23:02.265859Z","iopub.status.busy":"2023-10-25T22:23:02.265536Z","iopub.status.idle":"2023-10-25T22:23:08.423188Z","shell.execute_reply":"2023-10-25T22:23:08.421976Z","shell.execute_reply.started":"2023-10-25T22:23:02.265834Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["mode2amount = 'amount'\n","sub_amount, sub_percent = [1], [0.05]\n","spoil_modes = ['ins', 'del', 'sub']\n","spoil_inits = ['shap']\n","words2spoil_amount = [1, 2]\n","\n","dan_scores_char, bert_scores_char, acc_scores_char, adversial_examples_char = get_scores_char_spoiled_text(\n","    bert, adversial_examples_char,\n","    words2spoil_amount=words2spoil_amount,\n","    mode2amount=mode2amount,\n","    sub_amount=sub_amount, sub_percent=sub_percent,\n","    spoil_modes=spoil_modes, spoil_inits=spoil_inits\n",")"]},{"cell_type":"markdown","metadata":{"_cell_guid":"c6b59874-65d5-467a-af95-3163f18e52c9","_uuid":"416b233b-03d4-482f-999b-0f486ac4df96","trusted":true},"source":["### Сохраняем все результаты"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5e6d79b8-7e9f-4ac7-95c4-23939007d726","_uuid":"329e80fe-92d5-44ba-8ede-646087b2dbe5","collapsed":false,"execution":{"iopub.execute_input":"2023-12-05T00:03:25.641255Z","iopub.status.busy":"2023-12-05T00:03:25.640654Z","iopub.status.idle":"2023-12-05T00:03:25.670652Z","shell.execute_reply":"2023-12-05T00:03:25.669927Z","shell.execute_reply.started":"2023-12-05T00:03:25.641220Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# сохраняем состязательные примеры\n","adversial_examples_char.to_csv('adversial_examples_char.csv')\n","\n","# создание pd.DataFrame с бъединенными данными\n","scores = [dan_scores_char, bert_scores_char, acc_scores_char]\n","names = ['dan_score', 'bert_score', 'acc_score']\n","dataframes = list()\n","\n","# создаем список отдельных dataframe\n","for name, score in zip(names, scores):\n","    score_dct = {\n","        'modification': list(),\n","        name: list()\n","    }\n","    for key, val in score.items():\n","        score_dct['modification'].append(key)\n","        score_dct[name].append(val)\n","    dataframes.append(pd.DataFrame(score_dct))\n","\n","# merge всех dataframe\n","init_dataframe = dataframes[0]\n","for i in range(1, len(dataframes)):\n","    init_dataframe = init_dataframe.merge(dataframes[i], how='left', on='modification')\n","\n","init_dataframe['importance'] = init_dataframe['modification'].apply(lambda x: x.split('_')[0])\n","init_dataframe['modification'] = init_dataframe['modification'].apply(lambda x: '_'.join(x.split('_')[1:]))\n","\n","# init_dataframe = init_dataframe.set_index('modification')"]},{"cell_type":"markdown","metadata":{"_cell_guid":"17e1e040-5c3a-4c1b-ab06-d5c0de7dd007","_uuid":"005cac6e-0f3b-4114-9b53-9648f2328ab3","trusted":true},"source":["### Графики зависимостей символов"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f5e88d2f-b7ad-41c7-92ad-317ce989ff3d","_uuid":"6eb78e58-b305-48cd-af80-f373d8bb74bf","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["orig_acc = 0.762\n","orig_dan = 1\n","orig_use = 1\n","\n","def plot_char_results(method: str, modification2value: Dict[str, float]) -> None:\n","    \n","    # сохраняем mappings от (способ порчи, кол-во испорченных слов)\n","    # к полученному результату\n","    bert_method_to_res = dict()\n","    acc_method_to_res = dict()\n","    dan_method_to_res = dict()\n","    # есть ли в результатах данный метод оценки важности слова\n","    used_method = False\n","    # получаем имена всех модификаций\n","    modifications = modification2value.keys()\n","    for modification in modifications:\n","        modification_parts = modification.split('_')\n","        # получаем характеристики модификации\n","        importance = modification_parts[0]\n","        spoil_method = modification_parts[2]\n","        words_amount = modification_parts[1]\n","        sub = modification_parts[4]\n","        # если хотим визуализировать другой метод\n","        if importance != method:\n","            continue\n","        used_method = True\n","        # получаем и сохраняем результаты\n","        bert_score = bert_scores_char[modification]\n","        dan_score = dan_scores_char[modification]\n","        acc_score = acc_scores_char[modification]\n","        \n","        if bert_method_to_res.get((spoil_method, words_amount), None) is None:\n","            bert_method_to_res[(spoil_method, words_amount)] = [(orig_acc, 0)]\n","        if dan_method_to_res.get((spoil_method, words_amount), None) is None:\n","            dan_method_to_res[(spoil_method, words_amount)] = [(orig_dan, 0)]\n","        if acc_method_to_res.get((spoil_method, words_amount), None) is None:\n","            acc_method_to_res[(spoil_method, words_amount)] = [(orig_use, 0)]\n","\n","        bert_method_to_res[(spoil_method, words_amount)].append((bert_score, sub))\n","        acc_method_to_res[(spoil_method, words_amount)].append((acc_score, sub))\n","        dan_method_to_res[(spoil_method, words_amount)].append((dan_score, sub))\n","    \n","    assert used_method\n","    \n","    names = ['accuracy', 'dan_sim', 'bert_sim']\n","    scores = [acc_method_to_res, dan_method_to_res, bert_method_to_res]\n","    _, axes = plt.subplots(3, 1, figsize=(20, 10))\n","    \n","    for idx, (name, mapping) in enumerate(zip(names, scores)):\n","        subs = None\n","        for key, value in mapping.items():\n","            if subs is None:\n","                subs = [sub for _, sub in value]\n","            results = [result for result, _ in value]\n","\n","            axes[idx].plot(results, label=''.join(key))\n","        \n","        axes[idx].set_xlable('spoil chars amount')\n","        axes[idx].set_ylabel(name)\n","        axes[idx].set_title(f'{name} with {method} depending on spoil chars amount')\n","        axes[idx].set_xticklabel(subs)\n","        axes[idx].legend()\n","        axes[idx].grid(True)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":3505809,"sourceId":6116952,"sourceType":"datasetVersion"},{"datasetId":3511519,"sourceId":6125391,"sourceType":"datasetVersion"},{"datasetId":4109850,"sourceId":7124626,"sourceType":"datasetVersion"}],"dockerImageVersionId":30528,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
