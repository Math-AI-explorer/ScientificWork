{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":6116952,"sourceType":"datasetVersion","datasetId":3505809},{"sourceId":6125391,"sourceType":"datasetVersion","datasetId":3511519},{"sourceId":7160860,"sourceType":"datasetVersion","datasetId":4135938},{"sourceId":7179402,"sourceType":"datasetVersion","datasetId":4149394}],"dockerImageVersionId":30528,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Установка и импорт всех необходимых зависимостей","metadata":{"_cell_guid":"07105a38-167d-4080-86f1-e28bbc172b85","_uuid":"b7f3779a-a199-4bcc-b86c-6eba030528bd","trusted":true}},{"cell_type":"code","source":"!pip install -q razdel\n!pip install -q pymorphy2\n!pip install -q git+https://github.com/ahmados/rusynonyms.git\n!pip install -q natasha\n!pip install -q pyaml-env\n!pip install -q captum","metadata":{"_cell_guid":"16d1f1be-6c5b-4cf0-af02-3f589e64f949","_uuid":"4b94f1c3-c161-4222-abfa-e3fb66e4880d","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-12T00:10:49.704809Z","iopub.execute_input":"2023-12-12T00:10:49.705198Z","iopub.status.idle":"2023-12-12T00:12:09.080200Z","shell.execute_reply.started":"2023-12-12T00:10:49.705166Z","shell.execute_reply":"2023-12-12T00:12:09.078807Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nmomepy 0.6.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nscikit-image 0.21.0 requires networkx>=2.8, but you have networkx 2.6.3 which is incompatible.\nydata-profiling 4.3.1 requires scipy<1.11,>=1.4.1, but you have scipy 1.11.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport sys\n\npath_to_alti = '/kaggle/input/transformer-contributions1/transformer-contributions'\nif not path_to_alti in sys.path:\n    sys.path.append(path_to_alti)\n\nfrom src.utils_contributions import *\nfrom src.contributions import ModelWrapper, ClassificationModelWrapperCaptum, interpret_sentence, occlusion","metadata":{"execution":{"iopub.status.busy":"2023-12-12T00:12:28.846558Z","iopub.execute_input":"2023-12-12T00:12:28.847536Z","iopub.status.idle":"2023-12-12T00:12:28.852582Z","shell.execute_reply.started":"2023-12-12T00:12:28.847500Z","shell.execute_reply":"2023-12-12T00:12:28.851682Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import xml.etree.ElementTree as ET\nimport pandas as pd\npd.set_option('display.max_columns', None)\n\nimport nltk\nfrom nltk.corpus import stopwords\nimport re\nimport pymorphy2\nimport razdel\nimport string\nfrom natasha import (\n    MorphVocab,\n    NewsMorphTagger,\n    NewsEmbedding,\n    Segmenter,\n    NewsSyntaxParser,\n    Doc\n)\n\nimport torch\nimport tensorflow_hub as hub\nfrom torch.utils.data import Dataset, DataLoader\nimport transformers\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModel, AutoConfig\nimport numpy as np\nfrom sklearn.metrics import f1_score\n\nfrom tqdm import tqdm\nfrom typing import *\nfrom collections import defaultdict\nfrom functools import partial\nfrom scipy.special import logit\n\nfrom lime.lime_text import LimeTextExplainer\nimport shap\n\nnltk.download('stopwords')\nnltk.download('punkt')\nrus_stopwords = stopwords.words('russian')\npunctuation = list(string.punctuation)","metadata":{"_cell_guid":"e8af1228-dfb5-4d7f-bbf6-d1cb72552c66","_uuid":"11ecb4af-671c-4f65-8913-9c95f7f796bd","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-12T00:12:22.775910Z","iopub.execute_input":"2023-12-12T00:12:22.776266Z","iopub.status.idle":"2023-12-12T00:12:25.680920Z","shell.execute_reply.started":"2023-12-12T00:12:22.776232Z","shell.execute_reply":"2023-12-12T00:12:25.679989Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Работа с данными (kaggle)","metadata":{"_cell_guid":"d7cacad4-4f63-43c2-93b1-c9c180bf748d","_uuid":"4351a3ac-2754-4156-96ae-b1866e7f6e82","trusted":true}},{"cell_type":"code","source":"datasets_folder = '/kaggle/input/sw-datasets/Russian-Sentiment-Analysis-Evaluation-Datasets'\ndatasets = ['SentiRuEval-2015-telecoms', 'SentiRuEval-2015-banks', 'SentiRuEval-2016-banks', 'SentiRuEval-2016-telecoms']\nsamples = ['test.xml', 'train.xml', 'test_etalon.xml']","metadata":{"_cell_guid":"19eb260a-7fe2-42e3-ac1e-9657a713481c","_uuid":"5b40ffa6-7032-4f5c-9c17-3cdf66f4a633","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-12T00:12:31.600500Z","iopub.execute_input":"2023-12-12T00:12:31.600865Z","iopub.status.idle":"2023-12-12T00:12:31.605805Z","shell.execute_reply.started":"2023-12-12T00:12:31.600834Z","shell.execute_reply":"2023-12-12T00:12:31.604789Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def extract_data(path: str) -> pd.DataFrame:\n    \"\"\"\n    функция для извлечения данных из xml\n    \"\"\"\n    tree = ET.parse(path)\n    root = tree.getroot()\n    DataFrame = dict()\n    database = root.findall('database')[0]\n    DataFrame_columns = list()\n\n    for idx, table in enumerate(database.findall('table')):\n        for column in table.findall('column'):\n            DataFrame[column.attrib['name']] = list()\n            DataFrame_columns.append(column.attrib['name'])\n        if idx == 0:\n            break\n\n    for table in database.findall('table'):\n        for column in table.findall('column'):\n            DataFrame[column.attrib['name']].append(column.text)\n\n    data = pd.DataFrame(DataFrame, columns=DataFrame_columns)\n    return data\n\n# инициализация всех путей (kaggle)\nbanks_dataset = datasets[2]\npath2samples = os.path.join(datasets_folder, banks_dataset)\nbanks = ['sberbank', 'vtb', 'gazprom', 'alfabank', 'bankmoskvy', 'raiffeisen', 'uralsib', 'rshb']\n\npath2test = os.path.join(path2samples, samples[2])\ndata_test = extract_data(path2test)\n\npath2train = os.path.join(path2samples, samples[1])\ndata_train = extract_data(path2train)","metadata":{"_cell_guid":"10ad2541-7c34-4b11-a6d4-8ef30594e6e0","_uuid":"77e7298c-cf2e-4be8-a178-de925ce239f3","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-12T00:14:36.211409Z","iopub.execute_input":"2023-12-12T00:14:36.211787Z","iopub.status.idle":"2023-12-12T00:14:37.579893Z","shell.execute_reply.started":"2023-12-12T00:14:36.211755Z","shell.execute_reply":"2023-12-12T00:14:37.579087Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def extract_text_features(data: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    функция для первичной обработки текста от лишних символов\n    \"\"\"\n    extracted_data = dict()\n    extracted_data['text'] = list()\n    extracted_data['0class'] = list()\n    extracted_data['1class'] = list()\n\n    for idx in range(len(data)):\n        row = data.iloc[idx, :]\n        banks_review = row[banks]\n        unique_labels = set(banks_review)\n        unique_labels.remove('NULL')\n\n        # убираем все ненужные знаки\n        filtered_text = re.sub('http[A-z|:|.|/|0-9]*', '', row['text'])\n        filtered_text = re.sub('@\\S*', '', filtered_text)\n        filtered_text = re.sub('#|:|»|«|-|xD|;D|\\\"|_|/', '', filtered_text)\n        filtered_text = re.sub(r'\\.(?=\\s)|,|(?<!\\s)\\.(?!\\s)', ' ', filtered_text)\n        filtered_text = re.sub(r'[A-Z]|[a-z]', '', filtered_text)\n        filtered_text = re.sub(r'\\d+', 'число', filtered_text)\n        # надо немного подправить\n        filtered_text = re.sub(r'\\b(\\w+)\\s+\\1\\b', r'\\1', filtered_text)\n        filtered_text = re.sub(r'\\s+', ' ', filtered_text).strip()\n        new_text = filtered_text\n\n        # сохраняем только уникальные токены (без придатка xml NULL)\n        unique_labels = list(unique_labels)\n        while len(unique_labels) < 2:\n            unique_labels.append(unique_labels[-1])\n        extracted_data['text'].append(new_text)\n        for idx, label in enumerate(unique_labels):\n            text_label = int(label) + 1\n            extracted_data[f'{idx}' + 'class'].append(text_label)\n\n    extracted_data = pd.DataFrame(extracted_data)\n    \n    # возвращаем dataframe\n    return extracted_data\n\nextracted_val = extract_text_features(data_test)\nextracted_train = extract_text_features(data_train)","metadata":{"_cell_guid":"e465b3aa-afb3-4491-ad48-3a8f07c88a7b","_uuid":"7c270cb9-6e6d-44e5-8aa9-5b9a81759385","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-12T00:16:22.011495Z","iopub.execute_input":"2023-12-12T00:16:22.011860Z","iopub.status.idle":"2023-12-12T00:16:30.025896Z","shell.execute_reply.started":"2023-12-12T00:16:22.011828Z","shell.execute_reply":"2023-12-12T00:16:30.025116Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# пример твита из датасета\nextracted_val.iloc[3308].text","metadata":{"_cell_guid":"553036a6-e0cb-4859-9f43-fc8dd418894b","_uuid":"d874a823-0b6d-443d-b0cf-fee72b75b727","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-12T00:16:30.027330Z","iopub.execute_input":"2023-12-12T00:16:30.027627Z","iopub.status.idle":"2023-12-12T00:16:30.033781Z","shell.execute_reply.started":"2023-12-12T00:16:30.027600Z","shell.execute_reply":"2023-12-12T00:16:30.032908Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"'Цены на нефть попрежнему остаются на очень низких уровнях РБК НПЗ стремятся отложить импортные…'"},"metadata":{}}]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\n# анализ распределения таргетов на твитах\nfig, axes = plt.subplots(1, 2, figsize=(8, 5))\nplt.subplots_adjust(hspace=0.3, wspace=0.5)\nfontsize=15\n\nsns.countplot(data=extracted_train, x='0class', ax=axes[0])\naxes[0].set_xlabel('class 0', fontsize=fontsize)\naxes[0].set_ylabel('count', fontsize=fontsize)\naxes[0].set_xticks([0, 1, 2], ['Neg', 'Neu', 'Pos'], fontsize=fontsize)\naxes[0].grid(True)\n\nsns.countplot(data=extracted_train, x='1class', ax=axes[1])\naxes[1].set_xlabel('class 1', fontsize=fontsize)\naxes[1].set_ylabel('count', fontsize=fontsize)\naxes[1].set_xticks([0, 1, 2], ['Neg', 'Neu', 'Pos'], fontsize=fontsize)\naxes[1].grid(True)\n\nfig.suptitle('target distribution', fontsize=fontsize)\n\nNone","metadata":{"_cell_guid":"d6af3578-6851-46ac-81ed-247202502f82","_uuid":"90899eeb-ca26-4010-9d08-888db203f310","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-12T00:16:34.723405Z","iopub.execute_input":"2023-12-12T00:16:34.723775Z","iopub.status.idle":"2023-12-12T00:16:35.092013Z","shell.execute_reply.started":"2023-12-12T00:16:34.723745Z","shell.execute_reply":"2023-12-12T00:16:35.091113Z"},"trusted":true},"execution_count":24,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 800x500 with 2 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"markdown","source":"### Инициализируем модель (fine-tune) для решения нашей задачи классификации","metadata":{"_cell_guid":"84c3b206-9192-44d8-b0c3-99030962fbf1","_uuid":"27a39401-ac18-4df4-b723-57d39d511fb7","trusted":true}},{"cell_type":"code","source":"fn_model_name = \"DeepPavlov/distilrubert-base-cased-conversational\"\n\nclass BERTmy(torch.nn.Module):\n    def __init__(\n        self, model_name: str, n_classes: int, \n        use_tok_type_ids: bool, p: float=0.05\n    ) -> None:\n        super(BERTmy, self).__init__()\n        self.rubert = transformers.AutoModel.from_pretrained(\n            model_name\n        )\n        self.tokenizer = transformers.AutoTokenizer.from_pretrained(\n            model_name, \n            do_lower_case=True,\n            add_additional_tokens=True\n        )\n        self.use_tok_type_ids = use_tok_type_ids\n        \n        hidden_size_output = self.rubert.config.hidden_size\n        self.pre_classifier = torch.nn.Sequential(\n            torch.nn.Linear(hidden_size_output, hidden_size_output, bias=True),\n            torch.nn.Dropout(p),\n            torch.nn.ReLU()\n        )\n        self.classifier = torch.nn.Sequential(\n            torch.nn.Linear(hidden_size_output, hidden_size_output, bias=True),\n            torch.nn.Dropout(p),\n            torch.nn.ReLU(),\n            torch.nn.Linear(hidden_size_output, n_classes),\n        )\n\n    def forward(\n        self, input_ids: torch.Tensor, attention_mask: torch.Tensor, \n        token_type_ids: torch.Tensor=None, output_attentions: bool=False,\n        output_hidden_states: bool=False, return_dict: bool=True\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n        \n        input_dict = {\n            'input_ids': input_ids,\n            'attention_mask': attention_mask,\n            'return_dict': True,\n            'output_attentions': True,\n            'output_hidden_states': True\n        }\n        if self.use_tok_type_ids and not token_type_ids is None:\n            input_dict['token_type_ids'] = token_type_ids\n        \n        rubert_output = self.rubert(**input_dict)\n\n        pooled = rubert_output['last_hidden_state']\n        attentions = rubert_output['attentions']\n        hid_states = rubert_output['hidden_states']\n\n        output_pre_cls = self.pre_classifier(pooled[:, 0, :])\n        logits = self.classifier(output_pre_cls)\n\n        return {\n            'logits': logits,\n            'attentions': attentions,\n            'hidden_states': hid_states\n        }","metadata":{"execution":{"iopub.status.busy":"2023-12-12T00:16:35.348042Z","iopub.execute_input":"2023-12-12T00:16:35.348876Z","iopub.status.idle":"2023-12-12T00:16:35.360582Z","shell.execute_reply.started":"2023-12-12T00:16:35.348842Z","shell.execute_reply":"2023-12-12T00:16:35.359689Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"def load_model_hf_cls(\n    model_load: str, model_type: str, \n    load_model_weights: bool=False\n) -> torch.nn.Module:\n\n    assert model_type in ['distilbert', 'bert']\n\n    tokenizer = AutoTokenizer.from_pretrained(\n        model_load, do_lower_case=True,\n        add_additional_tokens=True\n    )\n\n    if load_model_weights:\n        model = AutoModel.from_pretrained(model_load)\n        model_config = model.config\n    else:\n        model_config = AutoConfig.from_pretrained(model_load)\n\n    model_cls = AutoModelForSequenceClassification.from_config(model_config)\n    \n    if load_model_weights:\n        if model_type == 'distilbert':\n            model_cls.distilbert = model\n        elif model_type == 'bert':\n            model_cls.bert = model\n        del model\n\n    return model_cls, tokenizer","metadata":{"execution":{"iopub.status.busy":"2023-12-12T00:16:35.541437Z","iopub.execute_input":"2023-12-12T00:16:35.542123Z","iopub.status.idle":"2023-12-12T00:16:35.548785Z","shell.execute_reply.started":"2023-12-12T00:16:35.542091Z","shell.execute_reply":"2023-12-12T00:16:35.547810Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"distilbert_name = \"DeepPavlov/distilrubert-base-cased-conversational\"\nbert_base_name = \"DeepPavlov/rubert-base-cased\"\n\ndevice = 'cuda:0' if torch.cuda.is_available() else 'cpu'\nnum_cls = len(pd.unique(extracted_train['0class']))\nload_tf = True\n\nif load_tf:\n    model_cls, tokenizer = load_model_hf_cls(\n        distilbert_name, model_type='distilbert', \n        load_model_weights=True\n    )\n    seq_max_len = model_cls.config.max_position_embeddings\n    hid_dim = model_cls.config.dim\n    model_cls.dropout = torch.nn.Identity()\n    model_cls.pre_classifier = torch.nn.Sequential(\n        torch.nn.Linear(hid_dim, hid_dim, bias=False),\n        torch.nn.Dropout(0.15),\n        torch.nn.ReLU()\n    )\n    model_cls.classifier = torch.nn.Sequential(\n        torch.nn.Linear(hid_dim, hid_dim, bias=False),\n        torch.nn.Dropout(0.15),\n        torch.nn.ReLU(),\n        torch.nn.Linear(hid_dim, num_cls, bias=False),\n    )\nelse:\n    model_cls = BERTmy(model_name=distilbert_name, n_classes=num_cls, use_tok_type_ids=False)\n    tokenizer = model_cls.tokenizer\n    seq_max_len = model_cls.rubert.config.max_position_embeddings","metadata":{"execution":{"iopub.status.busy":"2023-12-12T00:16:35.721099Z","iopub.execute_input":"2023-12-12T00:16:35.721758Z","iopub.status.idle":"2023-12-12T00:16:40.083829Z","shell.execute_reply.started":"2023-12-12T00:16:35.721731Z","shell.execute_reply":"2023-12-12T00:16:40.082989Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at DeepPavlov/distilrubert-base-cased-conversational were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Инициализируем class для нашего датасета","metadata":{"_cell_guid":"a021a9f9-44ed-42a5-84c6-848e064b38a7","_uuid":"46a83749-640d-4bd6-9be0-7623bc16c690","trusted":true}},{"cell_type":"code","source":"train_batch_size = 24\nval_batch_size = 24\n\nclass SentimentDataTransformer(Dataset):\n    # инициализация датасета\n    def __init__(\n        self, texts: List[str], \n        labels: List[Tuple[int, ...]]=None\n    ) -> None:\n        \n        self.texts = texts\n        self.labels = labels\n\n    # для получения размера датасета\n    def __len__(self) -> int:\n        return len(self.texts)\n\n    # для получения элемента по индексу\n    def __getitem__(\n        self, index: int\n    ) -> Tuple[Union[str, int]]:\n\n        if self.labels is None:\n            return self.texts[index]\n\n        text = self.texts[index]\n        labels = self.labels[index]\n        \n        target1, target2 = labels\n\n        return text, target1, target2","metadata":{"_cell_guid":"4fe85b1d-d8b2-4be9-8b4e-dde97c2a4d42","_uuid":"9ba9fdee-22c8-4555-a26c-0bcec67ca8a9","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-12T00:16:40.085409Z","iopub.execute_input":"2023-12-12T00:16:40.085718Z","iopub.status.idle":"2023-12-12T00:16:40.093084Z","shell.execute_reply.started":"2023-12-12T00:16:40.085691Z","shell.execute_reply":"2023-12-12T00:16:40.092201Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"class collate_fn_transformers():\n    \n    def __init__(\n        self, tokenizer: AutoTokenizer, \n        use_labels:bool, use_tok_type_ids: bool\n    ) -> None:\n        \n        self.tokenizer = tokenizer\n        self.use_tok_type_ids = use_tok_type_ids\n        self.use_labels = use_labels\n        \n    def __call__(self, batch):\n        \n        if not self.use_labels:\n\n            texts = batch\n\n            return self.tokenizer(\n                texts, #truncation=True,\n                padding=True, add_special_tokens=True,\n                return_token_type_ids=self.use_tok_type_ids,\n                return_tensors='pt'\n            )\n        \n        texts, target1, target2 = zip(*batch)\n        \n        input_ids = self.tokenizer(\n            texts, #truncation=True,\n            padding=True, add_special_tokens=True,\n            return_token_type_ids=self.use_tok_type_ids,\n            return_tensors='pt'\n        )\n        target1 = torch.tensor(target1)\n        target2 = torch.tensor(target2)\n        \n        return input_ids, target1, target2","metadata":{"execution":{"iopub.status.busy":"2023-12-12T00:16:40.094133Z","iopub.execute_input":"2023-12-12T00:16:40.094400Z","iopub.status.idle":"2023-12-12T00:16:40.106922Z","shell.execute_reply.started":"2023-12-12T00:16:40.094376Z","shell.execute_reply":"2023-12-12T00:16:40.106100Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"### Инициализируем наши DataLoaders","metadata":{"_cell_guid":"ec214cce-6c71-4987-b673-f4f59b6e29f2","_uuid":"d72254d0-e70d-443a-9506-34554ec96efe","trusted":true}},{"cell_type":"code","source":"train = SentimentDataTransformer(\n    texts=extracted_train['text'].tolist(),\n    labels=list(zip(extracted_train['0class'], extracted_train['1class']))\n)\n\nval = SentimentDataTransformer(\n    texts=extracted_val['text'].tolist(),\n    labels=list(zip(extracted_val['0class'], extracted_val['1class']))\n)\n\ntrain_loader = DataLoader(\n    train, batch_size=train_batch_size, shuffle=True,\n    collate_fn=collate_fn_transformers(tokenizer=tokenizer, use_tok_type_ids=False, use_labels=True)\n)\nval_loader = DataLoader(\n    val, batch_size=val_batch_size, shuffle=False,\n    collate_fn=collate_fn_transformers(tokenizer=tokenizer, use_tok_type_ids=False, use_labels=True)\n)\nloaders = {\n    'train': train_loader,\n    'val': val_loader\n}","metadata":{"_cell_guid":"b5110fba-0208-4967-90fb-04bc71a68957","_uuid":"be40ab48-96e2-4c10-af39-36066c75dfc4","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-12T00:16:40.108604Z","iopub.execute_input":"2023-12-12T00:16:40.108878Z","iopub.status.idle":"2023-12-12T00:16:40.128734Z","shell.execute_reply.started":"2023-12-12T00:16:40.108852Z","shell.execute_reply":"2023-12-12T00:16:40.127849Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"### Дообучение модели","metadata":{"_cell_guid":"f4164662-55b5-43d0-ad49-7c57ea8eb62c","_uuid":"6c111c3b-95c7-4dc8-ba88-8f8bbce229dd","trusted":true}},{"cell_type":"code","source":"def train_model(\n    epochs: int, model: torch.nn.Module, loaders: Dict[str, DataLoader], \n    optimizer: torch.optim, scheduler: torch.optim.lr_scheduler, \n    weights_vector: torch.tensor=None, device: str='cpu'\n) -> None:\n    # cross entropy loss\n    model = model.to(device)\n    if weights_vector is None:\n        weights_vector = torch.ones(size=(num_cls,), device=device)\n    loss_function1 = torch.nn.CrossEntropyLoss(reduction='mean', weight=weights_vector)\n    loss_function2 = torch.nn.CrossEntropyLoss(reduction='mean', weight=weights_vector)\n    \n    # извлечение DataLoaders\n    if len(loaders) > 1:\n        train_loader = loaders['train']\n        val_loader = loaders['val']\n        steps_per_epoch = [('train', train_loader), ('val', val_loader)]\n    else:\n        train_loader = loaders['train']\n        steps_per_epoch = [('train', train_loader)]\n\n    # обучение по эпохам\n    for epoch in range(epochs):\n        for mode, loader in steps_per_epoch:\n            # сохранение статистик\n            train_loss = 0\n            n_correct = 0\n            processed_data = 0\n            \n            # train/val \n            if mode == 'train':\n                model.train()\n                requires_grad_mode = True\n            else:\n                model.eval()\n                requires_grad_mode = False\n            \n            # проход по батчам\n            for inputs, trg1, trg2 in tqdm(loader):\n                # обнуляем градиенты\n                optimizer.zero_grad()\n\n                # извлечение входных данных для модели\n                for key, value in inputs.items():\n                    inputs[key] = value.to(device)\n                trg1, trg2 = trg1.to(device), trg2.to(device)\n                inputs['return_dict'] = True\n                \n                # устанавливаем необходимость вычислять/не_вычислять градиенты\n                with torch.set_grad_enabled(requires_grad_mode):\n                    outputs = model(**inputs)\n                    preds = torch.argmax(outputs['logits'], dim=1)\n\n                    # настраиваем модели на конкретный target\n                    if all(trg1 == trg2):\n                        loss1 = loss_function1(outputs['logits'], trg1)\n                        train_loss += loss1.item()\n                        n_correct += torch.sum(preds == trg1).cpu().detach().numpy()\n                        if mode == 'train':\n                            # вычисляем градиенты и обновляем веса\n                            loss1.backward()\n                            optimizer.step()\n                    # если у твита более чем 1 метка, то настраиваем на обе\n                    else:\n                        loss1 = loss_function1(outputs['logits'], trg1) * 0.5\n                        loss2 = loss_function2(outputs['logits'], trg2) * 0.5\n                        loss_all = loss1 + loss2\n                        train_loss += loss_all.item()\n\n                        mask_singular = trg1 == trg2\n                        mask_multiple = trg1 != trg2\n                        singular = preds[mask_singular]\n                        n_correct += torch.sum(\n                            singular == trg1[mask_singular]\n                        ).cpu().detach().numpy()\n                        multiple = preds[mask_multiple]\n                        n_correct += torch.sum(\n                            (multiple == trg1[mask_multiple]) | (multiple == trg2[mask_multiple])\n                        ).cpu().detach().numpy()\n                        if mode == 'train':\n                            # вычисляем градиенты и обновляем веса\n                            loss_all.backward()\n                            optimizer.step()\n\n                    processed_data += len(preds)\n\n            # вычисляем ошибку и точность прогноза на эпохе\n            loader_loss = train_loss / processed_data\n            loader_acc = n_correct / processed_data\n            print(f'{epoch + 1} epoch with {mode} mode has: {loader_loss} loss, {loader_acc} acc')\n        \n        # делаем шаг для sheduler оптимайзера\n        scheduler.step()","metadata":{"_cell_guid":"8ab49c8f-c3fa-4786-a63b-fe6708927100","_uuid":"c349e8e4-a562-4927-aaa2-ac14f5dcf8df","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-12T00:16:40.129933Z","iopub.execute_input":"2023-12-12T00:16:40.130413Z","iopub.status.idle":"2023-12-12T00:16:40.148568Z","shell.execute_reply.started":"2023-12-12T00:16:40.130387Z","shell.execute_reply":"2023-12-12T00:16:40.147656Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"# weigths for classes\nweights_vector = torch.zeros(size=(num_cls,), device=device)\nunique_labels, counts = np.unique(\n    extracted_train['0class'], return_counts=True\n)\nsm_count = np.sum(counts)\nfor label, count in zip(unique_labels, counts):\n    weights_vector[label] = 1 - count/sm_count\n\n# train model\nepochs = 15\noptimizer = torch.optim.Adam([\n    {'params': model_cls.pre_classifier.parameters(), 'lr': 8e-4},\n    {'params': model_cls.classifier.parameters(), 'lr': 8e-4}\n], lr=1e-6)\nscheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_model(1, model_cls, loaders, optimizer, scheduler, weights_vector, device)","metadata":{"execution":{"iopub.status.busy":"2023-12-12T00:22:03.474867Z","iopub.execute_input":"2023-12-12T00:22:03.475561Z","iopub.status.idle":"2023-12-12T00:22:21.731392Z","shell.execute_reply.started":"2023-12-12T00:22:03.475486Z","shell.execute_reply":"2023-12-12T00:22:21.730451Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stderr","text":"100%|██████████| 392/392 [00:16<00:00, 24.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"1 epoch with train mode has: 0.005373856139373672 loss, 0.947721465076661 acc\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 139/139 [00:02<00:00, 67.27it/s]","output_type":"stream"},{"name":"stdout","text":"1 epoch with val mode has: 0.04252899262078767 loss, 0.7555086024750981 acc\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"model_name = 'distilbert_cls.pth'\n\nmode_process = input('Load weights? (y/n)')\nif mode_process == 'n':\n    torch.save(model_cls.state_dict(), model_name)\nelif mode_process == 'y':\n    model_cls.load_state_dict(torch.load('/kaggle/input/distilbert-w3/distilbert_cls.pth'))\nelse:\n    assert mode_process in ['n', 'y']\nmodel_cls.eval()\nNone","metadata":{"_cell_guid":"2fa670b4-8330-4a79-ac62-d867928544f9","_uuid":"19503e86-4d81-4fd8-8a6e-f0f6f9891405","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-12T00:23:25.426036Z","iopub.execute_input":"2023-12-12T00:23:25.426780Z","iopub.status.idle":"2023-12-12T00:23:27.666371Z","shell.execute_reply.started":"2023-12-12T00:23:25.426743Z","shell.execute_reply":"2023-12-12T00:23:27.665343Z"},"trusted":true},"execution_count":44,"outputs":[{"output_type":"stream","name":"stdin","text":"Load weights? (y/n) n\n"}]},{"cell_type":"markdown","source":"### Вычисление итоговых показателей","metadata":{"_cell_guid":"114d3f5e-6292-4964-8e88-7ed63b57100a","_uuid":"ed44b641-9164-4946-ac60-777b1b4afb6c","trusted":true}},{"cell_type":"code","source":"def calculate_accuracy(\n    model: torch.nn.Module, loader: DataLoader,\n    device: str='cpu'\n) -> float:\n    model.eval()\n    model = model.to(device)\n    n_correct = 0\n    processed_data = 0\n    \n    # проход по батчам\n    for inputs, trg1, trg2 in tqdm(loader):\n\n        # извлечение входных данных для модели\n        for key, value in inputs.items():\n            inputs[key] = value.to(device)\n        trg1, trg2 = trg1.to(device), trg2.to(device)\n        inputs['return_dict'] = True\n        \n        with torch.no_grad():\n            outputs = model(**inputs)\n            preds = torch.argmax(outputs['logits'], dim=1)\n            mask_singular = trg1 == trg2\n            mask_multiple = trg1 != trg2\n            singular = preds[mask_singular]\n            n_correct += torch.sum(\n                singular == trg1[mask_singular]\n            ).cpu().detach().numpy()\n            multiple = preds[mask_multiple]\n            if len(multiple) > 0:\n                n_correct += torch.sum(\n                    (multiple == trg1[mask_multiple]) | (multiple == trg2[mask_multiple])\n                ).cpu().detach().numpy()\n\n            processed_data += len(preds)\n        \n    loader_acc = n_correct / processed_data\n    \n    return loader_acc\n\ndef calculate_f1_class(\n    model: torch.nn.Module, loader: DataLoader,\n    class_num: int, device: str='cpu'\n) -> float:\n    model.eval()\n    model = model.to(device)\n    all_preds = list()\n    groud_truth = list()\n    \n    # проход по батчам\n    for inputs, trg1, trg2 in tqdm(loader):\n\n        # извлечение входных данных для модели\n        for key, value in inputs.items():\n            inputs[key] = value.to(device)\n        inputs['return_dict'] = True\n        \n        with torch.no_grad():\n            outputs = model(**inputs)\n            \n            preds = torch.argmax(\n                outputs['logits'], dim=1\n            ).cpu().numpy()\n            all_preds.append(preds)\n            groud_truth.append(trg1.cpu().detach().numpy())\n\n    all_preds = np.hstack(all_preds)\n    groud_truth = np.hstack(groud_truth)\n    mask = all_preds == class_num\n    all_preds[mask] = 1\n    all_preds[~mask] = 0\n    mask = groud_truth == class_num\n    groud_truth[mask] = 1\n    groud_truth[~mask] = 0\n    \n    return f1_score(groud_truth, all_preds)","metadata":{"_cell_guid":"26fee109-eaec-4b0f-a90d-a4d72ea6a9e1","_uuid":"ca969426-e888-4d62-ad59-39920e55edec","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-12T00:22:25.330050Z","iopub.execute_input":"2023-12-12T00:22:25.330921Z","iopub.status.idle":"2023-12-12T00:22:25.345002Z","shell.execute_reply.started":"2023-12-12T00:22:25.330886Z","shell.execute_reply":"2023-12-12T00:22:25.344082Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"test_acc = calculate_accuracy(model_cls, val_loader, device)\nclass_neg_f1 = calculate_f1_class(model_cls, val_loader, 0, device)\nclass_neu_f1 = calculate_f1_class(model_cls, val_loader, 1, device)\nclass_pos_f1 = calculate_f1_class(model_cls, val_loader, 2, device)","metadata":{"execution":{"iopub.status.busy":"2023-12-12T00:22:25.673629Z","iopub.execute_input":"2023-12-12T00:22:25.674586Z","iopub.status.idle":"2023-12-12T00:22:33.853682Z","shell.execute_reply.started":"2023-12-12T00:22:25.674550Z","shell.execute_reply":"2023-12-12T00:22:33.852673Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stderr","text":"100%|██████████| 139/139 [00:02<00:00, 67.94it/s]\n100%|██████████| 139/139 [00:02<00:00, 66.90it/s]\n100%|██████████| 139/139 [00:02<00:00, 69.18it/s]\n100%|██████████| 139/139 [00:02<00:00, 69.30it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"# общая accuracy и f1 по классам\ntest_acc, class_neg_f1, class_neu_f1, class_pos_f1","metadata":{"_cell_guid":"82265fd8-c0bd-44d7-b9d8-aca69d404cb0","_uuid":"9b2b22cf-e842-40f4-b450-2dd7bf72d097","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-12T00:22:33.855078Z","iopub.execute_input":"2023-12-12T00:22:33.855369Z","iopub.status.idle":"2023-12-12T00:22:33.861567Z","shell.execute_reply.started":"2023-12-12T00:22:33.855342Z","shell.execute_reply":"2023-12-12T00:22:33.860676Z"},"trusted":true},"execution_count":40,"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"(0.7555086024750981, 0.6678240740740741, 0.825029515938607, 0.5188536953242835)"},"metadata":{}}]},{"cell_type":"markdown","source":"## Backdoor attacks on neural network(adversial examples)","metadata":{"_cell_guid":"8882bdef-e71a-4445-8823-c6cd3ac91816","_uuid":"cac0e4f4-82dd-4140-9443-c4539d51a316","trusted":true}},{"cell_type":"markdown","source":"### USE metric for similarity between original sentence and spoiled sentence","metadata":{"_cell_guid":"c7a5f905-6e00-430b-a6b1-a9cb1ea55c89","_uuid":"2def7913-246a-4e98-8733-7579e3d96e4c","trusted":true}},{"cell_type":"code","source":"def use_score(original, adversial, use_bert_encoder=False, model=None):\n    from scipy.spatial.distance import cosine\n    # Load pre-trained universal sentence encoder model\n    if not use_bert_encoder:\n        # using DAN from tensorflow\n        use_encoder = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n\n        sentences_orig = list()\n        sentences_adv = list()\n        for pair in zip(original, adversial):\n            orig, adv = pair\n            sentences_orig.append(orig)\n            sentences_adv.append(adv)\n\n        # get embs of texts\n        sentences_orig_emb = use_encoder(sentences_orig)\n        sentences_adv_emb = use_encoder(sentences_adv)\n\n        # calculate use_score with DAN\n        use_scores = list()\n        for pair in zip(sentences_orig_emb, sentences_adv_emb):\n            orig_emb, adv_emb = pair[0], pair[1]\n            use_score_one = 1 - cosine(orig_emb, adv_emb)\n            use_scores.append(use_score_one)\n    else:\n        # using BERT itself\n        def get_inputs(text): # get inputs for model\n            inputs = model.tokenizer(\n                text, padding=True, \n                add_special_tokens=True, \n                return_tensors='pt'\n            )\n            ids = inputs['input_ids'].type(torch.long).to(device)\n            mask = inputs['attention_mask'].type(torch.long).to(device)\n            token_type_ids = inputs[\"token_type_ids\"].type(torch.long).to(device)\n            \n            return ids, mask, token_type_ids\n\n        # calculate use_score with BERT\n        use_scores = list()\n        for pair in zip(original, adversial):\n            orig, adv = pair[0], pair[1]\n            orig_inputs = get_inputs(orig)\n            adv_inputs = get_inputs(adv)\n            orig_outputs = model.rubert(*orig_inputs)\n            adv_outputs = model.rubert(*adv_inputs)\n            orig_pooled, adv_pooled = orig_outputs[1], adv_outputs[1]\n            orig_pooled = orig_pooled.cpu().detach().numpy()\n            adv_pooled = adv_pooled.cpu().detach().numpy()\n            use_score_one = 1 - cosine(orig_pooled, adv_pooled)\n            use_scores.append(use_score_one)\n    \n    return use_scores, np.mean(use_scores)","metadata":{"_cell_guid":"a2237584-a021-4633-a333-8413f2555a5f","_uuid":"e5c67eb2-846d-4e5e-aaf4-57493cfc825b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-11T20:02:49.141305Z","iopub.execute_input":"2023-12-11T20:02:49.142291Z","iopub.status.idle":"2023-12-11T20:02:49.154257Z","shell.execute_reply.started":"2023-12-11T20:02:49.142253Z","shell.execute_reply":"2023-12-11T20:02:49.153319Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"markdown","source":"### Attention visualization","metadata":{"_cell_guid":"2fd87276-d681-4aa6-8cc4-d5824c8f3642","_uuid":"fdf759b2-374a-4203-892d-13259da64f14","trusted":true}},{"cell_type":"code","source":"def visualize_attention_one_head(tokens, attention_weights, num_layer, num_head):\n    # works only with batch_size=1\n    num_layer -= 1\n    num_head -= 1\n    assert num_head >= 0 and num_head < len(attention_weights[0][0])\n    assert num_layer < len(attention_weights) and num_layer >= 0\n    \n    attention_layer = attention_weights[num_layer][0].cpu().detach().numpy()\n    \n    fig, ax = plt.subplots(1, 1, figsize=(9, 6))\n\n    g = sns.heatmap(attention_layer[num_head], annot=True, linewidth=0.1, fmt='.1g')\n    # xlabel='weight_for_embed', ylabel='num_embed'\n    g.set(title=f'layer: {num_layer + 1}; head: {num_head + 1} attention map')\n    tickvalues = range(0,len(tokens) + 2)\n    tokens = ['CLS'] + tokens + ['SEP']\n    g.set_yticks(ticks=tickvalues ,labels=tokens, rotation='horizontal')\n    g.set_xticks(ticks=tickvalues ,labels=tokens, rotation='vertical')\n    ax = g\n    plt.show()","metadata":{"_cell_guid":"e3c4968e-a083-4284-a5c4-0b88a9592f00","_uuid":"733e2079-83b7-4298-9626-9e7575f915a9","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-10T13:00:33.492386Z","iopub.execute_input":"2023-12-10T13:00:33.492761Z","iopub.status.idle":"2023-12-10T13:00:33.501011Z","shell.execute_reply.started":"2023-12-10T13:00:33.492731Z","shell.execute_reply":"2023-12-10T13:00:33.499949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"adversial_examples_char = pd.read_csv('/kaggle/input/result-data/adversial_examples_char.csv')\ntext_example = extracted_val.iloc[10].text\ntext_example_ins = adversial_examples_char['1_ins_amount_1_SpoiledText'].iloc[10]\ntext_example_del = adversial_examples_char['1_del_amount_1_SpoiledText'].iloc[10]\ntext_example_sub = adversial_examples_char['1_sub_amount_1_SpoiledText'].iloc[10]","metadata":{"_cell_guid":"63054f35-5904-4e39-8871-37b2cfaf13ac","_uuid":"12f060c2-107c-4121-b231-a4079561b10c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-10T13:00:33.854689Z","iopub.execute_input":"2023-12-10T13:00:33.855379Z","iopub.status.idle":"2023-12-10T13:00:33.916219Z","shell.execute_reply.started":"2023-12-10T13:00:33.855345Z","shell.execute_reply":"2023-12-10T13:00:33.915211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def visualize_attention_for_text(text, num_layer, num_head):\n    text_seq = bert.tokenizer(\n        text,\n        padding=True,\n        add_special_tokens=True,\n        return_tensors='pt'\n    ).to(device)\n    logits, attention = bert(**text_seq, output_attentions=True)\n    tokens = bert.tokenizer.tokenize(text)\n    visualize_attention_one_head(tokens, attention, num_layer, num_head)","metadata":{"_cell_guid":"ede9ac5a-c196-4cc1-b7c5-0ed241060268","_uuid":"52147ee1-2a73-4137-bd24-a043e4911b09","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-09T17:56:55.894597Z","iopub.execute_input":"2023-12-09T17:56:55.894998Z","iopub.status.idle":"2023-12-09T17:56:55.900762Z","shell.execute_reply.started":"2023-12-09T17:56:55.894967Z","shell.execute_reply":"2023-12-09T17:56:55.899813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visualize_attention_for_text(text_example_sub, 12, 1)","metadata":{"_cell_guid":"3c3b0df9-45ac-49e2-84d7-f4241dd80267","_uuid":"8c67df3a-86ce-4bf6-b915-49ec30ea8c11","collapsed":false,"execution":{"iopub.execute_input":"2023-12-04T23:18:15.793690Z","iopub.status.busy":"2023-12-04T23:18:15.793315Z","iopub.status.idle":"2023-12-04T23:18:16.475574Z","shell.execute_reply":"2023-12-04T23:18:16.474547Z","shell.execute_reply.started":"2023-12-04T23:18:15.793657Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### utils for generating adversarial text","metadata":{"_cell_guid":"c843d9f5-bff2-4a42-a9a0-6c0ee9d461c4","_uuid":"3f317f59-a082-4c84-a625-6a96ed49979c","trusted":true}},{"cell_type":"code","source":"key_errors = {\n    'й': ['ц', 'ы', 'ф'],\n    'ц': ['й', 'ы', 'у'],\n    'у': ['ц', 'в', 'к'],\n    'к': ['у', 'а', 'е'],\n    'е': ['к', 'п', 'н'],\n    'н': ['е', 'р', 'г'],\n    'г': ['н', 'о', 'ш'],\n    'ш': ['г', 'л', 'щ'],\n    'щ': ['ш', 'д', 'з'],\n    'з': ['щ', 'ж'],\n    'х': ['ъ', 'э', 'з'],\n    'ъ': ['э', 'х'],\n    'ф': ['й', 'ы', 'я'],\n    'ы': ['ц', 'в', 'ч', 'ф'],\n    'в': ['у', 'а', 'с', 'ы'],\n    'а': ['к', 'п', 'м', 'в'],\n    'п': ['е', 'р', 'и', 'а'],\n    'р': ['н', 'о', 'т', 'п'],\n    'о': ['г', 'л', 'ь', 'р'],\n    'л': ['ш', 'д', 'б', 'о'],\n    'д': ['щ', 'ж', 'ю', 'л', 'б'],\n    'ж': ['з', 'э', 'ю', 'д'],\n    'э': ['х', 'ъ', 'ж'],\n    'я': ['ф', 'ы', 'ч'],\n    'ч': ['ы', 'в', 'с', 'я'],\n    'с': ['в', 'а', 'м', 'ч'],\n    'м': ['а', 'п', 'и', 'с'],\n    'и': ['п', 'р', 'т', 'м'],\n    'т': ['р', 'о', 'ь', 'и'],\n    'ь': ['о', 'л', 'б', 'т'],\n    'б': ['ь', 'л', 'д', 'ю'],\n    'ю': ['д', 'ж', 'б'],\n    'r': ['t', 'f', 'e'],\n    't': ['y', 'f', 'e'],\n    '0': ['9', '-'],\n    '1': ['`', '2'],\n    '2': ['1', '3'],\n    '3': ['2', '4'],\n    '4': ['3', '5'],\n    '5': ['4', '6'],\n    '6': ['5', '7'],\n    '7': ['6', '8'],\n    '8': ['7', '9'],\n    '9': ['8', '0'],\n    '-': ['0', '+'],\n    'k': ['i', 'j', 'l', 'm'],\n    '.': [',', '/', 'l', ';']\n}\n# получаем словарь формата: буква -> ближайшие буквы на клавиатуре","metadata":{"_cell_guid":"273f1cd4-10e9-4c45-b3b7-e8da3516c7e5","_uuid":"8ea101b3-c750-46ed-8dfe-10aee7384980","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-11T20:02:52.852357Z","iopub.execute_input":"2023-12-11T20:02:52.852719Z","iopub.status.idle":"2023-12-11T20:02:52.866156Z","shell.execute_reply.started":"2023-12-11T20:02:52.852690Z","shell.execute_reply":"2023-12-11T20:02:52.865171Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"markdown","source":"### Prepare data for adversarial generating","metadata":{}},{"cell_type":"code","source":"# выбираем текст для генерации состязательных примеров с сохранением исходной пропорции\nlimit_neu = 1300\nlimit_pos = 270\nlimit_neg = 550\nadversial_examples_pos = extracted_val[extracted_val['0class'] == 2]\nadversial_examples_neu = extracted_val[extracted_val['0class'] == 1]\nadversial_examples_neg = extracted_val[extracted_val['0class'] == 0]\n\nadversial_examples_pos = adversial_examples_pos.head(limit_pos)\nadversial_examples_neu = adversial_examples_neu.head(limit_neu)\nadversial_examples_neg = adversial_examples_neg.head(limit_neg)\n\nadversial_examples = pd.concat([adversial_examples_pos, adversial_examples_neu, adversial_examples_neg])\nadversial_examples_char = adversial_examples.sample(frac=1)\n\nprint('Размер текста для генерации: ', len(adversial_examples_char))\nprint('Баланс классов: ')\nprint(np.unique(adversial_examples_char['0class'], return_counts=True))","metadata":{"_cell_guid":"0dd827a1-3317-4285-9dab-79b4e96d78c8","_uuid":"da7eac17-3985-45b6-b1c5-8c0ff4a8950c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-11T20:02:53.279448Z","iopub.execute_input":"2023-12-11T20:02:53.279849Z","iopub.status.idle":"2023-12-11T20:02:53.293571Z","shell.execute_reply.started":"2023-12-11T20:02:53.279816Z","shell.execute_reply":"2023-12-11T20:02:53.292565Z"},"trusted":true},"execution_count":63,"outputs":[{"name":"stdout","text":"Размер текста для генерации:  2120\nБаланс классов: \n(array([0, 1, 2]), array([ 550, 1300,  270]))\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Work with word importance","metadata":{}},{"cell_type":"code","source":"def gather_back_tokens(tokens: List[str], tokens_type: str) -> str:\n    \"\"\"\n    для превращения токенов в предложение\n    tokens: список токенов\n    tokens_type: natasha или razdel\n    \"\"\"\n    assert tokens_type in ['razdel', 'natasha']\n\n    sent = ''\n    prev_end = None\n    for token in tokens:\n\n        if tokens_type == 'natasha':\n            token_text = token['text']\n            token_start, token_stop = token['start'], token['stop']\n        else:\n            token_text = token.text\n            token_start, token_stop = token.start, token.stop\n        \n        if not prev_end is None:\n            sent += (token_start - prev_end) * ' '\n\n        sent += token_text\n        prev_end = token_stop\n \n    return sent","metadata":{"execution":{"iopub.status.busy":"2023-12-11T20:03:31.195907Z","iopub.execute_input":"2023-12-11T20:03:31.196312Z","iopub.status.idle":"2023-12-11T20:03:31.203564Z","shell.execute_reply.started":"2023-12-11T20:03:31.196278Z","shell.execute_reply":"2023-12-11T20:03:31.202538Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"model_cls.eval()\nmodel_cls = model_cls.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-12-11T22:54:43.726172Z","iopub.execute_input":"2023-12-11T22:54:43.726578Z","iopub.status.idle":"2023-12-11T22:54:43.734400Z","shell.execute_reply.started":"2023-12-11T22:54:43.726547Z","shell.execute_reply":"2023-12-11T22:54:43.733532Z"},"trusted":true},"execution_count":193,"outputs":[]},{"cell_type":"code","source":"def predict_texts(texts: List[str], func_type: str):\n    \"\"\"\n    for Lime: return probability distribution of text\n    \"\"\"\n    assert func_type in ['shap', 'lime']\n    \n    if func_type == 'shap':\n        texts = list(map(lambda x: re.sub(r'\\.{3}', '[MASK]', x), texts))\n\n    # get model outputs\n    dataset = SentimentDataTransformer(texts=texts)\n    dataloader = DataLoader(\n        dataset, batch_size=30, shuffle=False,\n        collate_fn=collate_fn_transformers(\n            tokenizer=tokenizer, use_labels=False,\n            use_tok_type_ids=False\n        )\n    )\n    all_probs = list()\n\n    for batch in dataloader:\n        for key, value in batch.items():\n            batch[key] = value.to(device)\n        batch['return_dict'] = True\n\n        with torch.no_grad():\n            logits = model_cls(**batch)['logits']\n\n        # get probs\n        probs = torch.nn.functional.softmax(\n            logits, dim=1\n        ).cpu().detach().numpy()\n        all_probs.append(probs)\n    \n    return np.vstack(all_probs)","metadata":{"execution":{"iopub.status.busy":"2023-12-11T23:30:51.391340Z","iopub.execute_input":"2023-12-11T23:30:51.391796Z","iopub.status.idle":"2023-12-11T23:30:51.400950Z","shell.execute_reply.started":"2023-12-11T23:30:51.391764Z","shell.execute_reply":"2023-12-11T23:30:51.399908Z"},"trusted":true},"execution_count":262,"outputs":[]},{"cell_type":"markdown","source":"## lime importance ","metadata":{}},{"cell_type":"code","source":"def lime_importance(\n    tokens: List[Tuple[str, int, int]], tokens_type: str, \n    num_features:int=300, num_samples:int=700, device: str='cpu'\n) -> List[str]:\n\n    assert tokens_type in ['razdel', 'natasha']\n\n    # список для наиболее важных слов\n    essential_words = list()\n    \n    def RazdelSplit(text):\n\n        return [raz_tok.text for raz_tok in list(tokenize(text))]\n\n    def NatashaSplit(text):\n\n        segmenter = Segmenter()\n        text_doc = Doc(text.lower())\n        text_doc.segment(segmenter)\n\n        return [nat_tok['text'] for nat_tok in text_doc]\n\n    text_to_explain = gather_back_tokens(tokens, tokens_type)\n\n    if tokens_type == 'razdel':\n        Spliter = RazdelSplit\n    elif token_type == 'natasha':\n        Spliter = NatashaSplit\n    # создаем Explainer\n    explainer = LimeTextExplainer(\n        class_names=['Neg', 'Neu', 'Pos'],\n        split_expression=Spliter\n    )\n\n    # \"объясняем\" текст\n    explanation = explainer.explain_instance(\n        text_to_explain, partial(predict_texts,func_type='lime'), \n        num_features=num_features, num_samples=num_samples\n    )\n\n    # создаем mapping из токена в его вес LogReg\n    explanation_list = explanation.as_list()\n    tok2weight = {token:weight for token, weight in explanation_list}\n\n    # создаем список из токенов, их важности и позиции в тексте\n    for token in tokens:\n        if tokens_type == 'razdel':\n            token_text = token.text.lower()\n        else:\n            token_text = token['text'].lower()\n\n        essential_words.append((\n            token, tok2weight[token_text]\n        ))\n\n    # создаем функцию сравнения важности\n    sort_func = lambda x: np.abs(x[1])\n    \n    # сортируем токены по важности\n    essential_words = sorted(essential_words, key=sort_func, reverse=True)\n    print(essential_words)\n\n    # возвращаем только слова и их позиции в тексте\n    essential_words = [word for word, _ in essential_words]\n\n    return essential_words","metadata":{"execution":{"iopub.status.busy":"2023-12-11T23:50:15.162596Z","iopub.execute_input":"2023-12-11T23:50:15.163583Z","iopub.status.idle":"2023-12-11T23:50:15.175410Z","shell.execute_reply.started":"2023-12-11T23:50:15.163545Z","shell.execute_reply":"2023-12-11T23:50:15.174480Z"},"trusted":true},"execution_count":308,"outputs":[]},{"cell_type":"markdown","source":"## shap importance","metadata":{}},{"cell_type":"code","source":"def shap_importance(\n    tokens: List[str], tokens_type: str,\n    target: int\n) -> List[str]:\n\n    assert tokens_type in ['razdel', 'natasha']\n\n    # восстанавливаем текст из слов\n    text_to_explain = gather_back_tokens(tokens,tokens_type)\n\n    def custom_tokenizer(\n        text: str, return_offsets_mapping=True\n    ) -> Dict[str, List[Union[str, Tuple[int, ...]]]]:\n        \"\"\"Custom tokenizers conform to a subset of the transformers API.\"\"\"\n        tokens = list(razdel.tokenize(text))\n        \n        words = list()\n        offsets = list()\n        for token in tokens:\n            words.append(token.text)\n            offsets.append((token.start, token.stop))\n\n        return {\n            'input_ids': words,\n            'offset_mapping': offsets\n        }\n\n    masker = shap.maskers.Text(custom_tokenizer)\n    explainer = shap.Explainer(\n        partial(predict_texts,func_type='shap'), masker, \n        output_names=['Neg', 'Neu', 'Pos']\n    )\n    # get shap values for the onliest text\n    shap_values = explainer([text_to_explain])\n\n    tokens_order = shap_values.data[0]\n    base_values = shap_values.base_values\n    contributions = np.abs(shap_values.values[0]).sum(axis=1)\n    essential_words = list(zip(tokens, contributions))\n    \n    sort_func = lambda x: x[1]\n    \n    essential_words = sorted(\n        essential_words, key=sort_func, \n        reverse=True\n    )\n    \n    essential_words = [word for word, _ in essential_words]\n    \n    return essential_words","metadata":{"_cell_guid":"ff82fe94-6beb-4d7b-af9c-702319834adc","_uuid":"be5751b8-0b73-423e-9ae0-4a90461d1648","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-11T23:50:15.655865Z","iopub.execute_input":"2023-12-11T23:50:15.656255Z","iopub.status.idle":"2023-12-11T23:50:15.666862Z","shell.execute_reply.started":"2023-12-11T23:50:15.656207Z","shell.execute_reply":"2023-12-11T23:50:15.665650Z"},"trusted":true},"execution_count":309,"outputs":[]},{"cell_type":"markdown","source":"## alti importance","metadata":{}},{"cell_type":"code","source":"model_cls_wrapper = ModelWrapper(model_cls)\n\ndef alti_importance(\n    tokens: List[Tuple[str, int, int]], tokens_type: str,\n    measure_tokens_contributions: str\n) -> List[str]:\n\n    assert measure_tokens_contributions in ['cls', 'all_tokens']\n    assert tokens_type in ['razdel', 'natasha']\n    \n    text_to_explain = gather_back_tokens(tokens, tokens_type)\n        \n    text_tokens = tokenizer.tokenize(text_to_explain)\n    text_input = tokenizer(\n        text_to_explain, return_tensors=\"pt\", \n        return_token_type_ids=False,\n        return_offsets_mapping=True\n    ).to(device)\n    offset_mapping = text_input['offset_mapping'][0,1:-1,:].cpu().detach()\n    text_input['return_dict'] = True\n    del text_input['offset_mapping']\n\n    # get words of text\n    words = list(razdel.tokenize(text_to_explain))\n    pos_words = [(word.start, word.stop) for word in words]\n\n    # create mapping from token to word\n    cur_index = 0\n    token_pos_to_word = dict()\n    for idx, (offset, token) in enumerate(zip(offset_mapping, text_tokens)):\n        start, _ = offset\n        while start >= pos_words[cur_index][1]:\n            cur_index += 1\n        token_pos_to_word[idx] = words[cur_index]\n\n    _, _, _, contributions_data = model_cls_wrapper(text_input)\n\n    # get Yi from alti\n    resultant_norm = torch.norm(\n        torch.squeeze(contributions_data['resultants']),\n        p=1, dim=-1\n    )\n    # get Cij from alti method\n    # 'contributions' means Tij\n    # alti requires scaling = min_sum\n    normalized_contributions = normalize_contributions(\n        contributions_data['contributions'], scaling='min_sum',\n        resultant_norm=resultant_norm\n    )\n\n    # apply attention rollout and get seq of Ci\n    contributions_mix = compute_joint_attention(normalized_contributions)\n    # extract Ci after last self-attention layer\n    joint_attention_layer = -1\n    contributions_mix_last_hid = contributions_mix[joint_attention_layer]\n\n    if measure_tokens_contributions == 'cls':\n        # contribution to token cls\n        positions=np.array([0])\n    else:\n        positions=np.arange(len(contributions_mix_cls) - 2) + 1\n\n    word_to_contribution = defaultdict(float)\n    for pos in positions:\n        # get tokens contrubitons\n        contributions_mix_cur = contributions_mix_last_hid[pos][1:-1]\n        for idx, contribution in enumerate(contributions_mix_cur):\n            word_to_contribution[token_pos_to_word[idx]] += contribution\n    \n    # функция для сортировки\n    sort_func = lambda x: x[1]\n    \n    essential_words = sorted(\n        [(word, cont) for word, cont in word_to_contribution.items()],\n        key=sort_func, reverse=True\n    )\n    \n    essential_words = [word for word, _ in essential_words]\n    \n    return essential_words","metadata":{"execution":{"iopub.status.busy":"2023-12-11T23:50:15.983563Z","iopub.execute_input":"2023-12-11T23:50:15.983849Z","iopub.status.idle":"2023-12-11T23:50:15.998937Z","shell.execute_reply.started":"2023-12-11T23:50:15.983825Z","shell.execute_reply":"2023-12-11T23:50:15.998147Z"},"trusted":true},"execution_count":310,"outputs":[]},{"cell_type":"markdown","source":"## Heuristic loss","metadata":{}},{"cell_type":"code","source":"def loss_importance(\n    tokens: List[Tuple[str, int, int]], \n    target: Union[int, str], tokens_type: str\n) -> List[str]:\n    \n    assert tokens_type in ['razdel', 'natasha']\n    \n    text_to_explain = gather_back_tokens(tokens, tokens_type)\n    \n    # список для наиболее важных слов\n    essential_words = list()\n\n    loss = torch.nn.CrossEntropyLoss()\n    get_inputs = lambda x: tokenizer(\n        x, padding=True,\n        add_special_tokens=True,\n        return_token_type_ids=False,\n        return_tensors='pt'\n    ).to(device)\n\n    # get inputs and outputs from model\n    inputs = get_inputs(text_to_explain)\n    inputs['return_dict'] = True\n\n    outputs = model_cls(**inputs)['logits']\n    target_pt = torch.tensor([target], dtype=torch.long)\n\n    # calculate loss for original text\n    loss_score_integral = loss(\n        outputs.cpu(), target_pt\n    )\n\n    for idx, token in enumerate(tokens):\n        # get text without one token\n        tokens_copy = tokens.copy()\n        tokens_copy.pop(idx)\n        text_to_explain = gather_back_tokens(tokens_copy, tokens_type)\n\n        # calculate loss without current word\n        inputs = get_inputs(text_to_explain)\n        inputs['return_dict'] = True\n\n        with torch.no_grad():\n            outputs = model_cls(**inputs)['logits']\n        loss_score_part = loss(outputs.cpu(), target_pt)\n        # add our score of change\n        essential_words.append((\n            token, (loss_score_part-loss_score_integral).cpu().detach().numpy()\n        ))\n\n    # создаем функцию сравнения важности\n    sort_func = lambda x: x[1]\n\n    # сортируем токены по важности\n    essential_words = sorted(essential_words, key=sort_func, reverse=True)\n\n    # возвращаем только слова и их позиции в тексте\n    essential_words = [word for word, _ in essential_words]\n\n    return essential_words","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-12-11T23:50:16.276000Z","iopub.execute_input":"2023-12-11T23:50:16.276335Z","iopub.status.idle":"2023-12-11T23:50:16.287839Z","shell.execute_reply.started":"2023-12-11T23:50:16.276308Z","shell.execute_reply":"2023-12-11T23:50:16.286925Z"},"trusted":true},"execution_count":311,"outputs":[]},{"cell_type":"markdown","source":"## Random important words","metadata":{}},{"cell_type":"code","source":"def extract_random_words(\n    tokens: List[str]\n) -> List[Tuple[str, int]]:\n    \"\"\"\n    возвращает список слов в случайном порядке\n    \"\"\"\n    permutation = np.random.permutation(len(tokens))\n\n    return [tokens[idx] for idx in permutation]","metadata":{"execution":{"iopub.status.busy":"2023-12-11T23:50:16.585029Z","iopub.execute_input":"2023-12-11T23:50:16.585414Z","iopub.status.idle":"2023-12-11T23:50:16.590805Z","shell.execute_reply.started":"2023-12-11T23:50:16.585385Z","shell.execute_reply":"2023-12-11T23:50:16.589804Z"},"trusted":true},"execution_count":312,"outputs":[]},{"cell_type":"markdown","source":"## char-level attacks","metadata":{"_cell_guid":"32811848-cf03-4d3e-a485-607a1e879aa1","_uuid":"ae956c6a-8df7-420b-8a63-ccaef04b1ac6","trusted":true}},{"cell_type":"code","source":"# функция для генерации состязательных примеров на уровне символов\ndef extract_spoiled_text_char_level(\n        dataframe, words2spoil=2, \n        sub_percent=0.15, sub_amount=1,\n        mode2spoil='mixed', mode2amount='percent',\n        importance='loss'\n    ):\n\n\n    def get_indexes2change(\n        sub_letter: int, token_len: int\n    ) -> List[int]:\n        \"\"\"\n        функция для получения индексов букв на замену (кроме 0)\n        \"\"\"\n        lst_to_random = list(range(1, token_len))\n        np_ids = np.random.choice(lst_to_random, size=sub_letter, replace=False)\n\n        return np_ids.tolist()\n    \n    def make_token_change(\n        indexes: List[int], token: str, mode='mixed'\n    ):\n        \"\"\"\n        фукнция для замены букв по индексам с использованием 4 типов замены:\n        del: только удаление\n        ins: только вставка\n        sub: только замена\n        mixed: все вместе сразу\n        \"\"\"\n        if mode == 'sub':\n            # заменяем букву на позиции\n            word = list(token.text)\n            for idx in indexes:\n                symbol = word[idx]\n                try:\n                    word[idx] = key_errors[symbol][random.randint(0, len(key_errors[symbol])-1)]\n                except:\n                    pass\n            return (token.start, token.stop, ''.join(word), 0)\n        elif mode == 'ins':\n            # вставляем букву на позиция и увеличиваем длину токена на 1\n            ins_count = 0\n            word = list(token.text)\n            indexes = sorted(indexes)\n            for idx in indexes:\n                symbol = word[idx+ins_count]\n                try:\n                    word.insert(idx+ins_count, key_errors[symbol][random.randint(0, len(key_errors[symbol]) - 1)])\n                    ins_count += 1\n                except:\n                    pass\n            return (token.start, token.stop, ''.join(word), ins_count)\n        elif mode == 'del':\n            # удаляем букву на позиция и уменьшаем длину токена на 1\n            del_count = 0\n            word = list(token.text)\n            indexes = sorted(indexes)\n            for idx in indexes:\n                if len(word) == 1:\n                    break\n                try:\n                    word.pop(idx-del_count)\n                    del_count += 1\n                except:\n                    pass\n            return (token.start, token.stop, ''.join(word), -del_count)\n        elif mode == 'mixed':\n            ins_count = 0\n            del_count = 0\n            word = list(token.text)\n            # генерируем самое первое действие в слове\n            idx2action = random.randint(0, 2)\n            indexes = sorted(indexes)\n            for idx in indexes:\n                # вставляем букву на позиция и увеличиваем длину токена на 1, если ins\n                # удаляем букву на позиция и уменьшаем длину токена на 1, если del\n                # заменяем букву на позиции, если sub\n                new_idx = idx+ins_count-del_count\n                try:\n                    if idx2action == 0:\n                        symbol = word[new_idx]\n                        word[new_idx] = key_errors[symbol][random.randint(0, len(key_errors[symbol]) - 1)]\n                        idx2action += 1\n                    elif idx2action == 1:\n                        word.insert(new_idx, key_errors[symbol][random.randint(0, len(key_errors[symbol]) - 1)])\n                        ins_count += 1\n                        idx2action += 1\n                    elif idx2action == 2:\n                        word.pop(new_idx)\n                        del_count += 1\n                        idx2action = 0 \n                except:\n                    pass\n            return (token.start, token.stop, ''.join(word), ins_count-del_count)\n\n    # spoiled texts\n    spoiled_text = list()\n    pbar = tqdm(dataframe['text'], leave=True, position=0)\n    for idx, sent, target1 in zip(\n        range(len(dataframe['text'])), \n        dataframe['text'], \n        dataframe['0class']\n    ):\n        # get tokens of our text\n        tokens = [data for data in list(tokenize(sent.lower()))]\n        # just one word\n        if len(tokens) == 1:\n            spoiled_text.append(sent)\n            continue\n\n        # выбираем определенным методом наиболее важные слова\n        if importance == 'lime':\n            word2spoil_order = lime_importance(\n                tokens=tokens, tokens_type='razdel', \n                device=device\n            )\n        elif importance == 'alti':\n            word2spoil_order = alti_importance(\n                tokens=tokens, tokens_type='razdel',\n                measure_tokens_contributions='cls'\n            )\n        elif importance == 'shap':\n            word2spoil_order = shap_importance(\n                tokens=tokens, tokens_type='razdel',\n                target=target1\n            )\n        elif importance == 'loss':\n            word2spoil_order = loss_importance(\n                tokens=tokens, tokens_type='razdel', \n                target=target1\n            )\n        elif importance == 'random':\n            word2spoil_order = extract_random_words(tokens)\n\n        sub_count = 0\n        spoiled_tokens = list()\n        for token in word2spoil_order:\n            # get token and token's position\n            token_len = token.stop - token.start\n            # no way to change\n            if token_len != 1 and sub_count < words2spoil: \n                # count our changes\n                if mode2amount == 'percent':\n                    sub_letter = max(1, int(token_len * sub_percent))\n                elif mode2amount == 'amount':\n                    sub_letter = max(1, sub_amount)\n                # get indexes to change\n                indexes = get_indexes2change(sub_letter, token_len)\n                # go through indexes\n                spoiled_word = make_token_change(indexes, token, mode2spoil)\n                # increase our subs\n                sub_count += 1\n                spoiled_tokens.append(spoiled_word)\n            # сделали нужное количество порч\n            if sub_count >= words2spoil:\n                break\n        \n        # заменяем исходные слов в тексте испорченными\n        shift_in_sent = 0\n        spoiled_sent = list(sent.lower())\n        spoiled_tokens = sorted(spoiled_tokens, key=lambda x:x[0])\n        for spoiled in spoiled_tokens:\n            spoiled_start, spoiled_stop, spoiled_word, word_shift = spoiled\n            spoiled_sent[spoiled_start + shift_in_sent:spoiled_stop + shift_in_sent] = spoiled_word\n            shift_in_sent += word_shift\n        spoiled_sent = ''.join(spoiled_sent)\n        spoiled_text.append(spoiled_sent)\n        \n        pbar.update(1)\n        pbar.set_description(f'Total processed: {idx + 1}')\n        \n    return spoiled_text","metadata":{"_cell_guid":"e083c436-9f81-4544-9205-387e75d2a8b1","_uuid":"d71ca614-f090-4867-9dd3-9a831b34223c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-11T23:50:18.361498Z","iopub.execute_input":"2023-12-11T23:50:18.361876Z","iopub.status.idle":"2023-12-11T23:50:18.389956Z","shell.execute_reply.started":"2023-12-11T23:50:18.361845Z","shell.execute_reply":"2023-12-11T23:50:18.388878Z"},"trusted":true},"execution_count":313,"outputs":[]},{"cell_type":"markdown","source":"### Портим текст, вычисляем показатель use_score и accuracy","metadata":{"_cell_guid":"8d5f880d-9df6-4595-96c2-5e31988b8d76","_uuid":"6ea8af42-a20a-45aa-8c3e-f7f9f58d8804","trusted":true}},{"cell_type":"code","source":"def get_scores_char_spoiled_text(\n        dataframe: pd.DataFrame, \n        words2spoil: List[int], mode2amount: str,\n        sub_amount: List[int], sub_percent: List[float],\n        spoil_modes: List[str], importances: List[str]\n    ) -> Tuple[Dict[str, float], Dict[str, float], Dict[str, float], pd.DataFrame]:\n    \"\"\"\n    dataframe: (pandas данные с текстом и метками)\n    mode2amount: 'percent', 'amount' (по процентам или по количеству букв)\n    words2spoil_amount: (количество слов для порчи)\n    subs: (сколько букв испортить)\n    subs_percent: (сколько букв в процентах от слова испортить)\n    spoil_modes: 'mixed', 'ins', 'del', 'sub' (способы порчи текста)\n    spoil_init: 'random', 'loss', 'lime' (тип выбора слов для порчи)\n    \"\"\"\n    \n    assert mode2amount in ['percent', 'amount']\n    assert set(importances) <= {'loss', 'shap', 'lime', 'random', 'alti'}\n    assert set(spoil_modes) <= {'mixed', 'ins', 'del', 'sub'}\n    assert all(amount > 0 for amount in words2spoil)\n    assert all(sub >= 1 for sub in sub_amount)\n    assert all(sub >= 0.05 for sub in sub_percent)\n    \n    dan_scores = dict()\n    bert_scores = dict()\n    acc_scores = dict()\n    \n    subs = sub_percent if mode2amount == 'percent' else sub_amount\n\n    for importance in importances:\n        for sub in subs:\n            for mode2spoil in spoil_modes:\n                for words_amount in words2spoil:\n\n                    col_name = f'{importance}_{words_amount}_{mode2spoil}_{mode2amount}_{sub}_CharSpoiledText'\n                    # генерируем состязательные примеры\n                    spoiled_text = extract_spoiled_text_char_level(\n                        dataframe, words2spoil=words_amount,\n                        sub_amount=sub, sub_percent=sub, mode2amount=mode2amount, \n                        mode2spoil=mode2spoil, importance=importance\n                    )\n                    \n                    # сохраняем колонку со состязательными примерами\n                    dataframe[col_name] = spoiled_text\n                    \n                    # считаем use score на основе представлений bert\n                    _, use_result_char_bert = use_score(\n                        dataframe['text'],\n                        dataframe[col_name],\n                        use_bert_encoder=True,\n                        model=model\n                    )\n                    # считаем use score на основе dan кодировщика\n                    _, use_result_char = use_score(\n                        dataframe['text'],\n                        dataframe[col_name]\n                    )\n\n                    sentidata = SentimentDataTransformer(\n                        texts=..., labels=...\n                    )\n                    \n                    loader_sentidata = DataLoader(\n                        sentidata, batch_size=16, shuffle=False,\n                        collate_fn=collate_fn_transformers(\n                            tokenizer=tokenizer, use_tok_type_ids=False, max_len=seq_mas_len\n                        )\n                    )\n\n                    # замеряем качество состязательных примеров\n                    spoiled_accuracy_char = calculate_accuracy(model, loader_sentidata, device)\n                    \n                    # сохраняем результаты\n                    dan_scores[col_name] = use_result_char\n                    bert_scores[col_name] = use_result_char_bert\n                    acc_scores[col_name] = spoiled_accuracy_char\n                \n    return dan_scores, bert_scores, acc_scores, dataframe","metadata":{"_cell_guid":"efa44e96-a095-433f-91c6-fe487359e517","_uuid":"1d64853f-94d1-4a6c-ac78-9bde88701737","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-11T23:42:14.033534Z","iopub.execute_input":"2023-12-11T23:42:14.034315Z","iopub.status.idle":"2023-12-11T23:42:14.048225Z","shell.execute_reply.started":"2023-12-11T23:42:14.034283Z","shell.execute_reply":"2023-12-11T23:42:14.047258Z"},"trusted":true},"execution_count":267,"outputs":[]},{"cell_type":"code","source":"mode2amount = 'amount'\nsub_amount, sub_percent = [1], [0.05]\nspoil_modes = ['ins', 'del', 'sub']\nimportances = ['lime']\nwords2spoil_amount = [1, 2]\n\ndan_scores_char, bert_scores_char, acc_scores_char, adversial_examples_char = get_scores_char_spoiled_text(\n    adversial_examples_char,\n    words2spoil=words2spoil_amount,\n    mode2amount=mode2amount,\n    sub_amount=sub_amount, sub_percent=sub_percent,\n    spoil_modes=spoil_modes, importances=importances\n)","metadata":{"_cell_guid":"99b6d519-3d2f-4d94-919a-15807f99a32d","_uuid":"ad6d6ffe-47cf-46ae-958d-effdfa8438ee","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-11T23:52:41.770880Z","iopub.execute_input":"2023-12-11T23:52:41.771286Z","iopub.status.idle":"2023-12-11T23:52:45.764161Z","shell.execute_reply.started":"2023-12-11T23:52:41.771254Z","shell.execute_reply":"2023-12-11T23:52:45.762955Z"},"trusted":true},"execution_count":325,"outputs":[{"name":"stderr","text":"Total processed: 1:   0%|          | 1/2120 [00:00<10:32,  3.35it/s]","output_type":"stream"},{"name":"stdout","text":"[(Substring(59, 65, 'русала'), 0.17499618625666669), (Substring(35, 52, 'реструктурировать'), 0.1473447059169203), (Substring(21, 34, 'необходимости'), 0.1217962788837209), (Substring(15, 20, 'нашли'), -0.11842125627977523), (Substring(12, 14, 'не'), -0.07403943424715557), (Substring(2, 11, 'сбербанке'), -0.06740369093477944), (Substring(58, 59, '\"'), 0.05371174669144409), (Substring(65, 66, '\"'), 0.05371174669144409), (Substring(53, 57, 'долг'), -0.0401951530899513), (Substring(0, 1, 'в'), -0.02678929812681088)]\n","output_type":"stream"},{"name":"stderr","text":"Total processed: 2:   0%|          | 2/2120 [00:00<09:12,  3.83it/s]","output_type":"stream"},{"name":"stdout","text":"[(Substring(7, 11, 'дали'), -0.5541643074842184), (Substring(4, 6, 'не'), -0.41613853543188084), (Substring(19, 20, 'в'), 0.11181120371189801), (Substring(21, 30, 'сбербанке'), -0.0966923379447318), (Substring(12, 18, 'кредит'), 0.06361010985638435), (Substring(0, 2, 'rt'), -0.021856783650271657)]\n","output_type":"stream"},{"name":"stderr","text":"Total processed: 3:   0%|          | 3/2120 [00:00<09:43,  3.63it/s]","output_type":"stream"},{"name":"stdout","text":"[(Substring(37, 38, '»'), 0.17124000619788585), (Substring(0, 12, 'рекомендации'), -0.07171309152996208), (Substring(25, 33, 'сбербанк'), -0.06905196645147835), (Substring(13, 15, 'по'), -0.057367067840748534), (Substring(39, 49, 'переоценил'), -0.052652366925402795), (Substring(22, 23, ':'), -0.03992488364409207), (Substring(24, 25, '«'), 0.022119279488510103), (Substring(50, 63, 'медиакомпании'), 0.009829513046289406), (Substring(34, 37, 'cib'), -0.0060725623278805), (Substring(16, 22, 'акциям'), 0.005370893042210175)]\n","output_type":"stream"},{"name":"stderr","text":"Total processed: 4:   0%|          | 4/2120 [00:01<12:04,  2.92it/s]","output_type":"stream"},{"name":"stdout","text":"[(Substring(55, 59, 'было'), -0.06025674972661085), (Substring(106, 110, 'андр'), 0.05582484274403715), (Substring(95, 104, 'сбербанка'), 0.04734432906382417), (Substring(28, 39, 'приватбанка'), 0.043108108201767796), (Substring(77, 93, 'государственного'), 0.04097168772994914), (Substring(52, 54, 'не'), -0.04096473788771755), (Substring(59, 60, ','), -0.03512733882626919), (Substring(61, 62, '—'), -0.031504476561571106), (Substring(104, 105, '»'), 0.022465039064638544), (Substring(63, 69, 'пышный'), -0.01857470473305516), (Substring(110, 113, '...'), -0.015554345436045868), (Substring(71, 76, 'глава'), 0.014535830059086971), (Substring(0, 11, 'переговоров'), -0.012273706448780054), (Substring(69, 70, ':'), 0.011315711629357805), (Substring(42, 51, 'сбербанку'), -0.007818251289837132), (Substring(12, 13, 'о'), -0.007667650240984489), (Substring(14, 27, 'присоединении'), 0.006614244470092058), (Substring(94, 95, '«'), -0.005954351758178778), (Substring(40, 41, 'к'), -0.004727156836019839)]\n","output_type":"stream"},{"name":"stderr","text":"Total processed: 5:   0%|          | 5/2120 [00:01<12:07,  2.91it/s]","output_type":"stream"},{"name":"stdout","text":"[(Substring(47, 56, 'сбербанке'), 0.4268225685231348), (Substring(19, 28, 'четвертую'), -0.15991438034827762), (Substring(4, 9, 'можно'), -0.1494012269845187), (Substring(29, 34, 'карту'), 0.14772617215077646), (Substring(63, 64, '.'), -0.1413518079539435), (Substring(10, 18, 'потерять'), -0.12779336371390632), (Substring(45, 46, 'в'), 0.07618708441308057), (Substring(41, 44, 'уже'), -0.06538750659622858), (Substring(57, 63, 'узнают'), 0.05565195855966747), (Substring(0, 3, 'как'), -0.0438922769440838), (Substring(34, 36, '?!'), -0.024974218154429153), (Substring(36, 40, 'меня'), -0.019997137742118937)]\n","output_type":"stream"},{"name":"stderr","text":"Total processed: 6:   0%|          | 6/2120 [00:01<11:02,  3.19it/s]","output_type":"stream"},{"name":"stdout","text":"[(Substring(9, 16, 'понизил'), -0.6411809229960604), (Substring(17, 23, 'ставки'), -0.3630537931705997), (Substring(36, 43, 'вкладам'), -0.14739360726502865), (Substring(0, 8, 'сбербанк'), -0.13543994507918253), (Substring(27, 35, 'рублевым'), -0.09694433987076884), (Substring(24, 26, 'по'), 0.007695040640761638)]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m7\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 4 \u001b[0mimportances = [\u001b[33m'\u001b[0m\u001b[33mlime\u001b[0m\u001b[33m'\u001b[0m]                                                                      \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 5 \u001b[0mwords2spoil_amount = [\u001b[94m1\u001b[0m, \u001b[94m2\u001b[0m]                                                                 \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 6 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 7 dan_scores_char, bert_scores_char, acc_scores_char, adversial_examples_char = get_scores    \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 8 \u001b[0m\u001b[2m│   \u001b[0madversial_examples_char,                                                                \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 9 \u001b[0m\u001b[2m│   \u001b[0mwords2spoil=words2spoil_amount,                                                         \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m10 \u001b[0m\u001b[2m│   \u001b[0mmode2amount=mode2amount,                                                                \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m in \u001b[92mget_scores_char_spoiled_text\u001b[0m:\u001b[94m37\u001b[0m                                                               \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m34 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m                                                                        \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m35 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mcol_name = \u001b[33mf\u001b[0m\u001b[33m'\u001b[0m\u001b[33m{\u001b[0mimportance\u001b[33m}\u001b[0m\u001b[33m_\u001b[0m\u001b[33m{\u001b[0mwords_amount\u001b[33m}\u001b[0m\u001b[33m_\u001b[0m\u001b[33m{\u001b[0mmode2spoil\u001b[33m}\u001b[0m\u001b[33m_\u001b[0m\u001b[33m{\u001b[0mmode2amount\u001b[33m}\u001b[0m\u001b[33m_\u001b[0m    \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m36 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[2m# генерируем состязательные примеры\u001b[0m                                     \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m37 \u001b[2m│   │   │   │   │   \u001b[0mspoiled_text = extract_spoiled_text_char_level(                         \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m38 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0mdataframe, words2spoil=words_amount,                                \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m39 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0msub_amount=sub, sub_percent=sub, mode2amount=mode2amount,           \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m40 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0mmode2spoil=mode2spoil, importance=importance                        \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m in \u001b[92mextract_spoiled_text_char_level\u001b[0m:\u001b[94m114\u001b[0m                                                           \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m111 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m112 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# выбираем определенным методом наиболее важные слова\u001b[0m                              \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m113 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m importance == \u001b[33m'\u001b[0m\u001b[33mlime\u001b[0m\u001b[33m'\u001b[0m:                                                           \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m114 \u001b[2m│   │   │   \u001b[0mword2spoil_order = lime_importance(                                            \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m115 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mtokens=tokens, tokens_type=\u001b[33m'\u001b[0m\u001b[33mrazdel\u001b[0m\u001b[33m'\u001b[0m,                                       \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m116 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mdevice=device                                                              \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m117 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                              \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m in \u001b[92mlime_importance\u001b[0m:\u001b[94m36\u001b[0m                                                                            \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m33 \u001b[0m\u001b[2m│   \u001b[0m)                                                                                       \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m34 \u001b[0m\u001b[2m│   \u001b[0m                                                                                        \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m35 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# \"объясняем\" текст\u001b[0m                                                                     \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m36 \u001b[2m│   \u001b[0mexplanation = explainer.explain_instance(                                               \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m37 \u001b[0m\u001b[2m│   │   \u001b[0mtext_to_explain, partial(predict_texts,func_type=\u001b[33m'\u001b[0m\u001b[33mlime\u001b[0m\u001b[33m'\u001b[0m),                           \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m38 \u001b[0m\u001b[2m│   │   \u001b[0mnum_features=num_features, num_samples=num_samples                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m39 \u001b[0m\u001b[2m│   \u001b[0m)                                                                                       \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/lime/\u001b[0m\u001b[1;33mlime_text.py\u001b[0m:\u001b[94m413\u001b[0m in \u001b[92mexplain_instance\u001b[0m                \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m410 \u001b[0m\u001b[2m│   │   │   │   │   │   │   │   │   │   \u001b[0msplit_expression=\u001b[96mself\u001b[0m.split_expression,            \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m411 \u001b[0m\u001b[2m│   │   │   │   │   │   │   │   │   │   \u001b[0mmask_string=\u001b[96mself\u001b[0m.mask_string))                     \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m412 \u001b[0m\u001b[2m│   │   \u001b[0mdomain_mapper = TextDomainMapper(indexed_string)                                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m413 \u001b[2m│   │   \u001b[0mdata, yss, distances = \u001b[96mself\u001b[0m.__data_labels_distances(                               \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m414 \u001b[0m\u001b[2m│   │   │   \u001b[0mindexed_string, classifier_fn, num_samples,                                    \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m415 \u001b[0m\u001b[2m│   │   │   \u001b[0mdistance_metric=distance_metric)                                               \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m416 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m.class_names \u001b[95mis\u001b[0m \u001b[94mNone\u001b[0m:                                                       \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/lime/\u001b[0m\u001b[1;33mlime_text.py\u001b[0m:\u001b[94m482\u001b[0m in \u001b[92m__data_labels_distances\u001b[0m         \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m479 \u001b[0m\u001b[2m│   │   │   │   │   │   │   │   │   │   │   │   \u001b[0mreplace=\u001b[94mFalse\u001b[0m)                             \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m480 \u001b[0m\u001b[2m│   │   │   \u001b[0mdata[i, inactive] = \u001b[94m0\u001b[0m                                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m481 \u001b[0m\u001b[2m│   │   │   \u001b[0minverse_data.append(indexed_string.inverse_removing(inactive))                 \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m482 \u001b[2m│   │   \u001b[0mlabels = classifier_fn(inverse_data)                                               \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m483 \u001b[0m\u001b[2m│   │   \u001b[0mdistances = distance_fn(sp.sparse.csr_matrix(data))                                \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m484 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m data, labels, distances                                                     \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m485 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m in \u001b[92mpredict_texts\u001b[0m:\u001b[94m27\u001b[0m                                                                              \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m24 \u001b[0m\u001b[2m│   │   \u001b[0mbatch[\u001b[33m'\u001b[0m\u001b[33mreturn_dict\u001b[0m\u001b[33m'\u001b[0m] = \u001b[94mTrue\u001b[0m                                                         \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m25 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                    \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m26 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mwith\u001b[0m torch.no_grad():                                                               \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m27 \u001b[2m│   │   │   \u001b[0mlogits = model_cls(**batch)[\u001b[33m'\u001b[0m\u001b[33mlogits\u001b[0m\u001b[33m'\u001b[0m]                                           \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m28 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                    \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m29 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# get probs\u001b[0m                                                                         \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m30 \u001b[0m\u001b[2m│   │   \u001b[0mprobs = torch.nn.functional.softmax(                                                \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1501\u001b[0m in \u001b[92m_call_impl\u001b[0m            \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1501 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m│   │   \u001b[0mbackward_pre_hooks = []                                                           \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/transformers/models/distilbert/\u001b[0m\u001b[1;33mmodeling_distilbert.py\u001b[0m:\u001b[94m76\u001b[0m \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[94m3\u001b[0m in \u001b[92mforward\u001b[0m                                                                                     \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 760 \u001b[0m\u001b[2;33m│   │   \u001b[0m\u001b[33m\"\"\"\u001b[0m                                                                               \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 761 \u001b[0m\u001b[2m│   │   \u001b[0mreturn_dict = return_dict \u001b[94mif\u001b[0m return_dict \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m \u001b[94melse\u001b[0m \u001b[96mself\u001b[0m.config.use_return  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 762 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 763 \u001b[2m│   │   \u001b[0mdistilbert_output = \u001b[96mself\u001b[0m.distilbert(                                              \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 764 \u001b[0m\u001b[2m│   │   │   \u001b[0minput_ids=input_ids,                                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 765 \u001b[0m\u001b[2m│   │   │   \u001b[0mattention_mask=attention_mask,                                                \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 766 \u001b[0m\u001b[2m│   │   │   \u001b[0mhead_mask=head_mask,                                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1501\u001b[0m in \u001b[92m_call_impl\u001b[0m            \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1501 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m│   │   \u001b[0mbackward_pre_hooks = []                                                           \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/transformers/models/distilbert/\u001b[0m\u001b[1;33mmodeling_distilbert.py\u001b[0m:\u001b[94m58\u001b[0m \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[94m3\u001b[0m in \u001b[92mforward\u001b[0m                                                                                     \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 580 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 581 \u001b[0m\u001b[2m│   │   \u001b[0membeddings = \u001b[96mself\u001b[0m.embeddings(input_ids, inputs_embeds)  \u001b[2m# (bs, seq_length, dim)\u001b[0m   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 582 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 583 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m.transformer(                                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 584 \u001b[0m\u001b[2m│   │   │   \u001b[0mx=embeddings,                                                                 \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 585 \u001b[0m\u001b[2m│   │   │   \u001b[0mattn_mask=attention_mask,                                                     \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 586 \u001b[0m\u001b[2m│   │   │   \u001b[0mhead_mask=head_mask,                                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1501\u001b[0m in \u001b[92m_call_impl\u001b[0m            \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1501 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m│   │   \u001b[0mbackward_pre_hooks = []                                                           \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/transformers/models/distilbert/\u001b[0m\u001b[1;33mmodeling_distilbert.py\u001b[0m:\u001b[94m35\u001b[0m \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[94m9\u001b[0m in \u001b[92mforward\u001b[0m                                                                                     \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 356 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m output_hidden_states:                                                      \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 357 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mall_hidden_states = all_hidden_states + (hidden_state,)                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 358 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                                              \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 359 \u001b[2m│   │   │   \u001b[0mlayer_outputs = layer_module(                                                 \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 360 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mx=hidden_state, attn_mask=attn_mask, head_mask=head_mask[i], output_atte  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 361 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                             \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 362 \u001b[0m\u001b[2m│   │   │   \u001b[0mhidden_state = layer_outputs[-\u001b[94m1\u001b[0m]                                              \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1501\u001b[0m in \u001b[92m_call_impl\u001b[0m            \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1501 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m│   │   \u001b[0mbackward_pre_hooks = []                                                           \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/transformers/models/distilbert/\u001b[0m\u001b[1;33mmodeling_distilbert.py\u001b[0m:\u001b[94m31\u001b[0m \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[94m3\u001b[0m in \u001b[92mforward\u001b[0m                                                                                     \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 310 \u001b[0m\u001b[2m│   │   \u001b[0msa_output = \u001b[96mself\u001b[0m.sa_layer_norm(sa_output + x)  \u001b[2m# (bs, seq_length, dim)\u001b[0m            \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 311 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 312 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Feed Forward Network\u001b[0m                                                            \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 313 \u001b[2m│   │   \u001b[0mffn_output = \u001b[96mself\u001b[0m.ffn(sa_output)  \u001b[2m# (bs, seq_length, dim)\u001b[0m                         \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 314 \u001b[0m\u001b[2m│   │   \u001b[0mffn_output: torch.Tensor = \u001b[96mself\u001b[0m.output_layer_norm(ffn_output + sa_output)  \u001b[2m# (bs\u001b[0m  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 315 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 316 \u001b[0m\u001b[2m│   │   \u001b[0moutput = (ffn_output,)                                                            \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1501\u001b[0m in \u001b[92m_call_impl\u001b[0m            \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1501 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m│   │   \u001b[0mbackward_pre_hooks = []                                                           \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/transformers/models/distilbert/\u001b[0m\u001b[1;33mmodeling_distilbert.py\u001b[0m:\u001b[94m25\u001b[0m \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[94m4\u001b[0m in \u001b[92mforward\u001b[0m                                                                                     \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 251 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.activation = get_activation(config.activation)                               \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 252 \u001b[0m\u001b[2m│   \u001b[0m                                                                                      \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 253 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mforward\u001b[0m(\u001b[96mself\u001b[0m, \u001b[96minput\u001b[0m: torch.Tensor) -> torch.Tensor:                               \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 254 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m apply_chunking_to_forward(\u001b[96mself\u001b[0m.ff_chunk, \u001b[96mself\u001b[0m.chunk_size_feed_forward, \u001b[96mse\u001b[0m  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 255 \u001b[0m\u001b[2m│   \u001b[0m                                                                                      \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 256 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mff_chunk\u001b[0m(\u001b[96mself\u001b[0m, \u001b[96minput\u001b[0m: torch.Tensor) -> torch.Tensor:                              \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 257 \u001b[0m\u001b[2m│   │   \u001b[0mx = \u001b[96mself\u001b[0m.lin1(\u001b[96minput\u001b[0m)                                                              \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/transformers/\u001b[0m\u001b[1;33mpytorch_utils.py\u001b[0m:\u001b[94m237\u001b[0m in                     \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[92mapply_chunking_to_forward\u001b[0m                                                                        \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m234 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# concatenate output at same dimension\u001b[0m                                             \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m235 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m torch.cat(output_chunks, dim=chunk_dim)                                     \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m236 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m237 \u001b[2m│   \u001b[0m\u001b[94mreturn\u001b[0m forward_fn(*input_tensors)                                                      \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m238 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m239 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m240 \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mfind_pruneable_heads_and_indices\u001b[0m(                                                      \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/transformers/models/distilbert/\u001b[0m\u001b[1;33mmodeling_distilbert.py\u001b[0m:\u001b[94m25\u001b[0m \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[94m7\u001b[0m in \u001b[92mff_chunk\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 254 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m apply_chunking_to_forward(\u001b[96mself\u001b[0m.ff_chunk, \u001b[96mself\u001b[0m.chunk_size_feed_forward, \u001b[96mse\u001b[0m  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 255 \u001b[0m\u001b[2m│   \u001b[0m                                                                                      \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 256 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mff_chunk\u001b[0m(\u001b[96mself\u001b[0m, \u001b[96minput\u001b[0m: torch.Tensor) -> torch.Tensor:                              \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 257 \u001b[2m│   │   \u001b[0mx = \u001b[96mself\u001b[0m.lin1(\u001b[96minput\u001b[0m)                                                              \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 258 \u001b[0m\u001b[2m│   │   \u001b[0mx = \u001b[96mself\u001b[0m.activation(x)                                                            \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 259 \u001b[0m\u001b[2m│   │   \u001b[0mx = \u001b[96mself\u001b[0m.lin2(x)                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 260 \u001b[0m\u001b[2m│   │   \u001b[0mx = \u001b[96mself\u001b[0m.dropout(x)                                                               \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1501\u001b[0m in \u001b[92m_call_impl\u001b[0m            \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1501 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m│   │   \u001b[0mbackward_pre_hooks = []                                                           \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mlinear.py\u001b[0m:\u001b[94m114\u001b[0m in \u001b[92mforward\u001b[0m                \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m111 \u001b[0m\u001b[2m│   │   │   \u001b[0minit.uniform_(\u001b[96mself\u001b[0m.bias, -bound, bound)                                        \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m112 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m113 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mforward\u001b[0m(\u001b[96mself\u001b[0m, \u001b[96minput\u001b[0m: Tensor) -> Tensor:                                            \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m114 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m F.linear(\u001b[96minput\u001b[0m, \u001b[96mself\u001b[0m.weight, \u001b[96mself\u001b[0m.bias)                                     \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m115 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m116 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mextra_repr\u001b[0m(\u001b[96mself\u001b[0m) -> \u001b[96mstr\u001b[0m:                                                           \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m117 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[33m'\u001b[0m\u001b[33min_features=\u001b[0m\u001b[33m{}\u001b[0m\u001b[33m, out_features=\u001b[0m\u001b[33m{}\u001b[0m\u001b[33m, bias=\u001b[0m\u001b[33m{}\u001b[0m\u001b[33m'\u001b[0m.format(                          \u001b[31m│\u001b[0m\n\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n\u001b[1;91mKeyboardInterrupt\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">7</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4 </span>importances = [<span style=\"color: #808000; text-decoration-color: #808000\">'lime'</span>]                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 5 </span>words2spoil_amount = [<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>, <span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span>]                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 6 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 7 dan_scores_char, bert_scores_char, acc_scores_char, adversial_examples_char = get_scores    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 8 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>adversial_examples_char,                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 9 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>words2spoil=words2spoil_amount,                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">10 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>mode2amount=mode2amount,                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">get_scores_char_spoiled_text</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">37</span>                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">34 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span>                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">35 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span>col_name = <span style=\"color: #808000; text-decoration-color: #808000\">f'{</span>importance<span style=\"color: #808000; text-decoration-color: #808000\">}_{</span>words_amount<span style=\"color: #808000; text-decoration-color: #808000\">}_{</span>mode2spoil<span style=\"color: #808000; text-decoration-color: #808000\">}_{</span>mode2amount<span style=\"color: #808000; text-decoration-color: #808000\">}_</span>    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">36 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># генерируем состязательные примеры</span>                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>37 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span>spoiled_text = extract_spoiled_text_char_level(                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">38 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   │   </span>dataframe, words2spoil=words_amount,                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">39 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   │   </span>sub_amount=sub, sub_percent=sub, mode2amount=mode2amount,           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">40 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   │   </span>mode2spoil=mode2spoil, importance=importance                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">extract_spoiled_text_char_level</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">114</span>                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">111 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">112 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># выбираем определенным методом наиболее важные слова</span>                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">113 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> importance == <span style=\"color: #808000; text-decoration-color: #808000\">'lime'</span>:                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>114 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>word2spoil_order = lime_importance(                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">115 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>tokens=tokens, tokens_type=<span style=\"color: #808000; text-decoration-color: #808000\">'razdel'</span>,                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">116 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>device=device                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">117 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>)                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">lime_importance</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">36</span>                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">33 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>)                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">34 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">35 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># \"объясняем\" текст</span>                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>36 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>explanation = explainer.explain_instance(                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">37 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>text_to_explain, partial(predict_texts,func_type=<span style=\"color: #808000; text-decoration-color: #808000\">'lime'</span>),                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">38 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>num_features=num_features, num_samples=num_samples                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">39 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>)                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/lime/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">lime_text.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">413</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">explain_instance</span>                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">410 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   │   │   │   │   │   </span>split_expression=<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.split_expression,            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">411 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   │   │   │   │   │   </span>mask_string=<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.mask_string))                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">412 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>domain_mapper = TextDomainMapper(indexed_string)                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>413 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>data, yss, distances = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.__data_labels_distances(                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">414 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>indexed_string, classifier_fn, num_samples,                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">415 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>distance_metric=distance_metric)                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">416 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.class_names <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/lime/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">lime_text.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">482</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__data_labels_distances</span>         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">479 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   │   │   │   │   │   │   │   </span>replace=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">False</span>)                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">480 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>data[i, inactive] = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">481 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>inverse_data.append(indexed_string.inverse_removing(inactive))                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>482 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>labels = classifier_fn(inverse_data)                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">483 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>distances = distance_fn(sp.sparse.csr_matrix(data))                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">484 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> data, labels, distances                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">485 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">predict_texts</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">27</span>                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">24 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>batch[<span style=\"color: #808000; text-decoration-color: #808000\">'return_dict'</span>] = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">25 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">26 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> torch.no_grad():                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>27 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>logits = model_cls(**batch)[<span style=\"color: #808000; text-decoration-color: #808000\">'logits'</span>]                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">28 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">29 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># get probs</span>                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">30 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>probs = torch.nn.functional.softmax(                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1501</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1498 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1499 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1500 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1501 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1502 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1503 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1504 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/transformers/models/distilbert/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_distilbert.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">76</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">3</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 760 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"\"\"</span>                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 761 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>return_dict = return_dict <span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> return_dict <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.config.use_return  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 762 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 763 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>distilbert_output = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.distilbert(                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 764 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>input_ids=input_ids,                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 765 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>attention_mask=attention_mask,                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 766 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>head_mask=head_mask,                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1501</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1498 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1499 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1500 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1501 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1502 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1503 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1504 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/transformers/models/distilbert/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_distilbert.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">58</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">3</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 580 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 581 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>embeddings = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.embeddings(input_ids, inputs_embeds)  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># (bs, seq_length, dim)</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 582 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 583 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.transformer(                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 584 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>x=embeddings,                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 585 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>attn_mask=attention_mask,                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 586 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>head_mask=head_mask,                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1501</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1498 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1499 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1500 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1501 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1502 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1503 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1504 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/transformers/models/distilbert/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_distilbert.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">35</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">9</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 356 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> output_hidden_states:                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 357 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>all_hidden_states = all_hidden_states + (hidden_state,)                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 358 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 359 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>layer_outputs = layer_module(                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 360 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>x=hidden_state, attn_mask=attn_mask, head_mask=head_mask[i], output_atte  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 361 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>)                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 362 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>hidden_state = layer_outputs[-<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>]                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1501</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1498 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1499 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1500 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1501 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1502 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1503 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1504 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/transformers/models/distilbert/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_distilbert.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">31</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">3</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 310 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>sa_output = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.sa_layer_norm(sa_output + x)  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># (bs, seq_length, dim)</span>            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 311 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 312 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Feed Forward Network</span>                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 313 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>ffn_output = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.ffn(sa_output)  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># (bs, seq_length, dim)</span>                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 314 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>ffn_output: torch.Tensor = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.output_layer_norm(ffn_output + sa_output)  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># (bs</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 315 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 316 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>output = (ffn_output,)                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1501</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1498 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1499 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1500 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1501 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1502 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1503 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1504 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/transformers/models/distilbert/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_distilbert.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">25</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">4</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 251 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.activation = get_activation(config.activation)                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 252 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 253 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>: torch.Tensor) -&gt; torch.Tensor:                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 254 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> apply_chunking_to_forward(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.ff_chunk, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.chunk_size_feed_forward, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">se</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 255 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 256 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">ff_chunk</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>: torch.Tensor) -&gt; torch.Tensor:                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 257 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>x = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.lin1(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>)                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">pytorch_utils.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">237</span> in                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">apply_chunking_to_forward</span>                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">234 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># concatenate output at same dimension</span>                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">235 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> torch.cat(output_chunks, dim=chunk_dim)                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">236 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>237 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_fn(*input_tensors)                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">238 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">239 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">240 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">find_pruneable_heads_and_indices</span>(                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/transformers/models/distilbert/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_distilbert.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">25</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">7</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">ff_chunk</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 254 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> apply_chunking_to_forward(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.ff_chunk, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.chunk_size_feed_forward, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">se</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 255 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 256 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">ff_chunk</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>: torch.Tensor) -&gt; torch.Tensor:                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 257 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>x = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.lin1(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>)                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 258 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>x = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.activation(x)                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 259 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>x = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.lin2(x)                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 260 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>x = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.dropout(x)                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1501</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1498 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1499 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1500 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1501 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1502 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1503 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1504 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">linear.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">114</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">111 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>init.uniform_(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.bias, -bound, bound)                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">112 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">113 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>: Tensor) -&gt; Tensor:                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>114 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> F.linear(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.weight, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.bias)                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">115 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">116 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">extra_repr</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>) -&gt; <span style=\"color: #00ffff; text-decoration-color: #00ffff\">str</span>:                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">117 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #808000; text-decoration-color: #808000\">'in_features={}, out_features={}, bias={}'</span>.format(                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">KeyboardInterrupt</span>\n</pre>\n"},"metadata":{}}]},{"cell_type":"markdown","source":"### Сохраняем все результаты","metadata":{"_cell_guid":"c6b59874-65d5-467a-af95-3163f18e52c9","_uuid":"416b233b-03d4-482f-999b-0f486ac4df96","trusted":true}},{"cell_type":"code","source":"# сохраняем состязательные примеры\nadversial_examples_char.to_csv('adversial_examples_char.csv')\n\n# создание pd.DataFrame с бъединенными данными\nscores = [dan_scores_char, bert_scores_char, acc_scores_char]\nnames = ['dan_score', 'bert_score', 'acc_score']\ndataframes = list()\n\n# создаем список отдельных dataframe\nfor name, score in zip(names, scores):\n    score_dct = {\n        'modification': list(),\n        name: list()\n    }\n    for key, val in score.items():\n        score_dct['modification'].append(key)\n        score_dct[name].append(val)\n    dataframes.append(pd.DataFrame(score_dct))\n\n# merge всех dataframe\ninit_dataframe = dataframes[0]\nfor i in range(1, len(dataframes)):\n    init_dataframe = init_dataframe.merge(dataframes[i], how='left', on='modification')\n\ninit_dataframe['importance'] = init_dataframe['modification'].apply(lambda x: x.split('_')[0])\ninit_dataframe['modification'] = init_dataframe['modification'].apply(lambda x: '_'.join(x.split('_')[1:]))\n\n# init_dataframe = init_dataframe.set_index('modification')","metadata":{"_cell_guid":"5e6d79b8-7e9f-4ac7-95c4-23939007d726","_uuid":"329e80fe-92d5-44ba-8ede-646087b2dbe5","collapsed":false,"execution":{"iopub.execute_input":"2023-12-05T00:03:25.641255Z","iopub.status.busy":"2023-12-05T00:03:25.640654Z","iopub.status.idle":"2023-12-05T00:03:25.670652Z","shell.execute_reply":"2023-12-05T00:03:25.669927Z","shell.execute_reply.started":"2023-12-05T00:03:25.641220Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Графики зависимостей символов","metadata":{"_cell_guid":"17e1e040-5c3a-4c1b-ab06-d5c0de7dd007","_uuid":"005cac6e-0f3b-4114-9b53-9648f2328ab3","trusted":true}},{"cell_type":"code","source":"orig_acc = 0.762\norig_dan = 1\norig_use = 1\n\ndef plot_char_results(method: str, modification2value: Dict[str, float]) -> None:\n    \n    # сохраняем mappings от (способ порчи, кол-во испорченных слов)\n    # к полученному результату\n    bert_method_to_res = dict()\n    acc_method_to_res = dict()\n    dan_method_to_res = dict()\n    # есть ли в результатах данный метод оценки важности слова\n    used_method = False\n    # получаем имена всех модификаций\n    modifications = modification2value.keys()\n    for modification in modifications:\n        modification_parts = modification.split('_')\n        # получаем характеристики модификации\n        importance = modification_parts[0]\n        spoil_method = modification_parts[2]\n        words_amount = modification_parts[1]\n        sub = modification_parts[4]\n        # если хотим визуализировать другой метод\n        if importance != method:\n            continue\n        used_method = True\n        # получаем и сохраняем результаты\n        bert_score = bert_scores_char[modification]\n        dan_score = dan_scores_char[modification]\n        acc_score = acc_scores_char[modification]\n        \n        if bert_method_to_res.get((spoil_method, words_amount), None) is None:\n            bert_method_to_res[(spoil_method, words_amount)] = [(orig_acc, 0)]\n        if dan_method_to_res.get((spoil_method, words_amount), None) is None:\n            dan_method_to_res[(spoil_method, words_amount)] = [(orig_dan, 0)]\n        if acc_method_to_res.get((spoil_method, words_amount), None) is None:\n            acc_method_to_res[(spoil_method, words_amount)] = [(orig_use, 0)]\n\n        bert_method_to_res[(spoil_method, words_amount)].append((bert_score, sub))\n        acc_method_to_res[(spoil_method, words_amount)].append((acc_score, sub))\n        dan_method_to_res[(spoil_method, words_amount)].append((dan_score, sub))\n    \n    assert used_method\n    \n    names = ['accuracy', 'dan_sim', 'bert_sim']\n    scores = [acc_method_to_res, dan_method_to_res, bert_method_to_res]\n    _, axes = plt.subplots(3, 1, figsize=(20, 10))\n    \n    for idx, (name, mapping) in enumerate(zip(names, scores)):\n        subs = None\n        for key, value in mapping.items():\n            if subs is None:\n                subs = [sub for _, sub in value]\n            results = [result for result, _ in value]\n\n            axes[idx].plot(results, label=''.join(key))\n        \n        axes[idx].set_xlable('spoil chars amount')\n        axes[idx].set_ylabel(name)\n        axes[idx].set_title(f'{name} with {method} depending on spoil chars amount')\n        axes[idx].set_xticklabel(subs)\n        axes[idx].legend()\n        axes[idx].grid(True)","metadata":{"_cell_guid":"f5e88d2f-b7ad-41c7-92ad-317ce989ff3d","_uuid":"6eb78e58-b305-48cd-af80-f373d8bb74bf","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}