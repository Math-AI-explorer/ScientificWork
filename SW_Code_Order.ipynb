{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Установка и импорт всех необходимых зависимостей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q razdel\n",
    "!pip install -q pymorphy2\n",
    "!pip install -q git+https://github.com/ahmados/rusynonyms.git\n",
    "!pip install -q natasha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import pymorphy2\n",
    "from razdel import tokenize\n",
    "from razdel import sentenize\n",
    "import string\n",
    "from natasha import (\n",
    "    MorphVocab,\n",
    "    NewsMorphTagger,\n",
    "    NewsEmbedding,\n",
    "    Segmenter,\n",
    "    NewsSyntaxParser,\n",
    "    Doc\n",
    ")\n",
    "\n",
    "import torch\n",
    "import tensorflow_hub as hub\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import transformers\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import sys\n",
    "from typing import *\n",
    "\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "import shap\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "rus_stopwords = stopwords.words('russian')\n",
    "punctuation = list(string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Работа с данными (kaggle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_folder = '/kaggle/input/sw-datasets/Russian-Sentiment-Analysis-Evaluation-Datasets'\n",
    "datasets = ['SentiRuEval-2015-telecoms', 'SentiRuEval-2015-banks', 'SentiRuEval-2016-banks', 'SentiRuEval-2016-telecoms']\n",
    "samples = ['test.xml', 'train.xml', 'test_etalon.xml']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    функция для извлечения данных из xml\n",
    "    \"\"\"\n",
    "    tree = ET.parse(path)\n",
    "    root = tree.getroot()\n",
    "    DataFrame = dict()\n",
    "    database = root.findall('database')[0]\n",
    "    DataFrame_columns = list()\n",
    "\n",
    "    for idx, table in enumerate(database.findall('table')):\n",
    "        for column in table.findall('column'):\n",
    "            DataFrame[column.attrib['name']] = list()\n",
    "            DataFrame_columns.append(column.attrib['name'])\n",
    "        if idx == 0:\n",
    "            break\n",
    "\n",
    "    for table in database.findall('table'):\n",
    "        for column in table.findall('column'):\n",
    "            DataFrame[column.attrib['name']].append(column.text)\n",
    "\n",
    "    data = pd.DataFrame(DataFrame, columns=DataFrame_columns)\n",
    "    return data\n",
    "\n",
    "# инициализация всех путей (kaggle)\n",
    "banks_dataset = datasets[2]\n",
    "path2samples = os.path.join(datasets_folder, banks_dataset)\n",
    "banks = ['sberbank', 'vtb', 'gazprom', 'alfabank', 'bankmoskvy', 'raiffeisen', 'uralsib', 'rshb']\n",
    "\n",
    "path2test = os.path.join(path2samples, samples[2])\n",
    "data_test = extract_data(path2test)\n",
    "\n",
    "path2train = os.path.join(path2samples, samples[1])\n",
    "data_train = extract_data(path2train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_features(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    функция для первичной обработки текста от лишних символов\n",
    "    \"\"\"\n",
    "    extracted_data = dict()\n",
    "    extracted_data['text'] = list()\n",
    "    extracted_data['0class'] = list()\n",
    "    extracted_data['1class'] = list()\n",
    "\n",
    "    for idx in range(len(data)):\n",
    "        row = data.iloc[idx, :]\n",
    "        banks_review = row[banks]\n",
    "        unique_labels = set(banks_review)\n",
    "        unique_labels.remove('NULL')\n",
    "\n",
    "        # убираем все ненужные знаки\n",
    "        filtered_text = re.sub('http[A-z|:|.|/|0-9]*', '', row['text']).strip()\n",
    "        filtered_text = re.sub('@\\S*', '', filtered_text).strip()\n",
    "        filtered_text = re.sub('#', '', filtered_text).strip()\n",
    "        new_text = filtered_text\n",
    "\n",
    "        # сохраняем только уникальные токены (без придатка xml NULL)\n",
    "        unique_labels = list(unique_labels)\n",
    "        while len(unique_labels) < 2:\n",
    "            unique_labels.append(unique_labels[-1])\n",
    "        extracted_data['text'].append(new_text)\n",
    "        for idx, label in enumerate(unique_labels):\n",
    "            text_label = int(label) + 1\n",
    "            extracted_data[f'{idx}' + 'class'].append(text_label)\n",
    "\n",
    "    extracted_data = pd.DataFrame(extracted_data)\n",
    "    \n",
    "    # возвращаем dataframe\n",
    "    return extracted_data\n",
    "\n",
    "extracted_test = extract_text_features(data_test)\n",
    "extracted_train = extract_text_features(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# пример твита из датасета\n",
    "extracted_test.iloc[3308].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# анализ распределения таргетов на твитах\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 5))\n",
    "plt.subplots_adjust(hspace=0.15, wspace=0.3)\n",
    "\n",
    "graph1 = sns.countplot(data=extracted_train, x='0class', ax=axes[0])\n",
    "graph1.set(xlabel='class_num', ylabel='amount of class', title='Amount of classes according 1 label')\n",
    "graph1.grid(True)\n",
    "\n",
    "graph2 = sns.countplot(data=extracted_train, x='1class', ax=axes[1])\n",
    "graph2.set(xlabel='class_num', ylabel='amount of class', title='Amount of classes according 2 label')\n",
    "graph2.grid(True)\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Инициализируем модель (fine-tune) для решения нашей задачи классификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-05\n",
    "\n",
    "\n",
    "class BERTmy(torch.nn.Module):\n",
    "    def __init__(self, n_classes: int) -> None:\n",
    "        super(BERTmy, self).__init__()\n",
    "        self.rubert = transformers.AutoModel.from_pretrained(\n",
    "            \"DeepPavlov/rubert-base-cased-sentence\"\n",
    "        )\n",
    "        self.tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "            \"DeepPavlov/rubert-base-cased-sentence\", \n",
    "            do_lower_case=True,\n",
    "            add_additional_tokens=True\n",
    "        )\n",
    "        \n",
    "        hidden_size_output = self.rubert.config.hidden_size\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_size_output, hidden_size_output, bias=True),\n",
    "            torch.nn.Dropout(0.05),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_size_output, n_classes),\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self, input_ids: torch.Tensor, attention_mask: torch.Tensor, \n",
    "        token_type_ids: torch.Tensor, output_attentions: bool=False\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
    "        rubert_output = self.rubert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            return_dict=True,\n",
    "            output_attentions=output_attentions\n",
    "        )\n",
    "        if not output_attentions:\n",
    "            pooled = rubert_output['pooler_output']\n",
    "        else:\n",
    "            pooled, attentions = rubert_output['pooler_output'], rubert_output['attentions']\n",
    "\n",
    "        output = self.classifier(pooled)\n",
    "\n",
    "        if not output_attentions:\n",
    "            return output\n",
    "        else:\n",
    "            return output, attentions\n",
    "    \n",
    "    def configure_optimizer(\n",
    "        self, use_scheduler: bool=False\n",
    "    ) -> torch.optim:\n",
    "        # freeze part of params\n",
    "        encoder_size = 0\n",
    "        for param in self.rubert._modules['encoder'].parameters():\n",
    "            encoder_size += 1\n",
    "        encoder_size_half = encoder_size // 2\n",
    "        for idx, param in enumerate(self.rubert._modules['encoder'].parameters()):\n",
    "            param.requires_grad = False\n",
    "            if idx >= encoder_size_half:\n",
    "                break\n",
    "        \n",
    "        # Adam\n",
    "        optimizer = torch.optim.Adam(\n",
    "            params=[\n",
    "                {'params':self.rubert._modules['embeddings'].parameters(), 'lr':4e-6},\n",
    "                {'params':self.rubert._modules['encoder'].parameters(), 'lr':4e-6},\n",
    "                {'params':self.rubert._modules['pooler'].parameters(), 'lr':4e-6},\n",
    "                {'params':self.classifier.parameters(), 'lr':9e-5}\n",
    "            ],\n",
    "            lr=learning_rate\n",
    "        )\n",
    "        if use_scheduler:\n",
    "            # scheduler\n",
    "            scheduler = torch.optim.lr_scheduler.ExponentialLR(\n",
    "                optimizer, gamma=0.96\n",
    "            )\n",
    "        \n",
    "            return optimizer, scheduler\n",
    "        \n",
    "        else:\n",
    "            return optimizer\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "num_cls = len(pd.unique(extracted_train['0class']))\n",
    "bert = BERTmy(num_cls)\n",
    "if torch.cuda.is_available():\n",
    "    bert = bert.cuda()\n",
    "optimizer, scheduler = bert.configure_optimizer(use_scheduler=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Инициализируем class для нашего датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 32\n",
    "val_batch_size = 16\n",
    "\n",
    "class SentimentData(Dataset):\n",
    "    # инициализация датасета\n",
    "    def __init__(\n",
    "        self, dataframe: pd.DataFrame, mode: str, \n",
    "        col_name: str, split_param: float=0.9\n",
    "    ) -> None:\n",
    "        self.mode = mode # train/test\n",
    "        self.data = dataframe # data\n",
    "        self.col_name = col_name # column for analyzing\n",
    "        \n",
    "        data_size = self.data.shape[0]\n",
    "        if self.mode in ['val', 'train']:\n",
    "            if self.mode == 'train':\n",
    "                self.data = self.data.iloc[:int(data_size * split_param)]\n",
    "            else:\n",
    "                self.data = self.data.iloc[int(data_size * split_param):]\n",
    "        \n",
    "        assert self.mode in ['val', 'train', 'test']\n",
    "\n",
    "    # для получения размера датасета\n",
    "    def __len__(self) -> int:\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    # для получения элемента по индексу\n",
    "    def __getitem__(\n",
    "        self, index: int\n",
    "    ) -> Dict[str, Union[str, torch.Tensor]]:\n",
    "        text = self.data.iloc[index][self.col_name]\n",
    "        target1 = self.data.iloc[index]['0class']\n",
    "        target2 = self.data.iloc[index]['1class']\n",
    "\n",
    "        return {\n",
    "            'text': text,\n",
    "            'target1': torch.tensor(target1, dtype=torch.long),\n",
    "            'target2': torch.tensor(target2, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Инициализируем наши DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = SentimentData(\n",
    "    dataframe=extracted_train,\n",
    "    split_param=1.0,\n",
    "    mode='train',\n",
    "    col_name='text'\n",
    ")\n",
    "\n",
    "val = SentimentData(\n",
    "    dataframe=extracted_train,\n",
    "    mode='val',\n",
    "    col_name='text'\n",
    ")\n",
    "\n",
    "test = SentimentData(\n",
    "    dataframe=extracted_test,\n",
    "    mode='test',\n",
    "    col_name='text'\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=train_batch_size, shuffle=True)\n",
    "# val_loader = DataLoader(val, batch_size=val_batch_size, shuffle=False)\n",
    "loaders = {\n",
    "    'train': train_loader,\n",
    "    # 'val': val_loader\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Дообучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rubert_tokenizer = bert.tokenizer\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    epochs: int, model: torch.nn.Module, loaders: List[DataLoader], \n",
    "    optimizer: torch.optim, scheduler: torch.optim.lr_scheduler\n",
    ") -> torch.nn.Module:\n",
    "    # cross entropy loss\n",
    "    loss_function1 = torch.nn.CrossEntropyLoss()\n",
    "    loss_function2 = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    # извлечение DataLoaders\n",
    "    if len(loaders) > 1:\n",
    "        train_loader = loaders['train']\n",
    "        val_loader = loaders['val']\n",
    "        steps_per_epoch = [('train', train_loader), ('val', val_loader)]\n",
    "    else:\n",
    "        train_loader = loaders['train']\n",
    "        steps_per_epoch = [('train', train_loader)]\n",
    "\n",
    "    # обучение по эпохам\n",
    "    for epoch in range(epochs):\n",
    "        for mode, loader in steps_per_epoch:\n",
    "            # сохранение статистик\n",
    "            train_loss = 0\n",
    "            n_correct = 0\n",
    "            processed_data = 0\n",
    "            \n",
    "            # train/val \n",
    "            if mode == 'train':\n",
    "                model.train()\n",
    "                requires_grad_mode = True\n",
    "            else:\n",
    "                model.eval()\n",
    "                requires_grad_mode = False\n",
    "            \n",
    "            # проход по батчам\n",
    "            for data in tqdm(loader):\n",
    "                # обнуляем градиенты\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # извлечение входных данных для модели\n",
    "                inputs = rubert_tokenizer(\n",
    "                    data['text'], padding=True, truncation=True, \n",
    "                    add_special_tokens=True, return_tensors='pt'\n",
    "                )\n",
    "                ids = inputs['input_ids'].to(device)\n",
    "                mask = inputs['attention_mask'].to(device)\n",
    "                token_type_ids = inputs[\"token_type_ids\"].to(device)\n",
    "                target1 = data['target1'].to(device)\n",
    "                target2 = data['target2'].to(device)\n",
    "                \n",
    "                # устанавливаем необходимость вычислять/не_вычислять градиенты\n",
    "                with torch.set_grad_enabled(requires_grad_mode):\n",
    "                    outputs = model(ids, mask, token_type_ids)\n",
    "                    preds = torch.argmax(outputs.data, dim=1)\n",
    "\n",
    "                    # настраиваем модели на конкретный target\n",
    "                    if all(target1 == target2):\n",
    "                        loss1 = loss_function1(outputs, target1)\n",
    "                        train_loss += loss1.item() * outputs.size(0)\n",
    "                        n_correct += torch.sum(preds == target1)\n",
    "                        if mode == 'train':\n",
    "                            # вычисляем градиенты и обновляем веса\n",
    "                            loss1.backward()\n",
    "                            optimizer.step()\n",
    "                    # если у твита более чем 1 метка, то настраиваем на обе\n",
    "                    else:\n",
    "                        loss1 = loss_function1(outputs, target1) * 0.5\n",
    "                        loss2 = loss_function2(outputs, target2) * 0.5\n",
    "                        loss_all = loss1 + loss2\n",
    "                        train_loss += loss_all.item() * outputs.size(0)\n",
    "\n",
    "                        mask_singular = target1 == target2\n",
    "                        mask_multiple = target1 != target2\n",
    "                        singular = preds[mask_singular]\n",
    "                        n_correct += torch.sum(singular == target1[mask_singular])\n",
    "                        multiple = preds[mask_multiple]\n",
    "                        n_correct += torch.sum((multiple == target1[mask_multiple]) & (multiple == target2[mask_multiple]))\n",
    "                        if mode == 'train':\n",
    "                            # вычисляем градиенты и обновляем веса\n",
    "                            loss_all.backward()\n",
    "                            optimizer.step()     \n",
    "                    processed_data += outputs.size(0)\n",
    "\n",
    "            # вычисляем ошибку и точность прогноза на эпохе\n",
    "            loader_loss = train_loss / processed_data\n",
    "            loader_acc = n_correct.cpu().numpy() / processed_data\n",
    "            print(f'{epoch + 1} epoch with {mode} mode has: {loader_loss} loss, {loader_acc} acc')\n",
    "        \n",
    "        # делаем шаг для sheduler оптимайзера\n",
    "        scheduler.step()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 12\n",
    "bert = train_model(epochs, bert, loaders, optimizer, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_process = input('Load weights? (y/n)')\n",
    "if mode_process == 'n':\n",
    "    torch.save(bert.state_dict(), 'bert_weights_pooled.pth')\n",
    "elif mode_process == 'y':\n",
    "    bert.load_state_dict(torch.load('/kaggle/input/bert-weights-better/bert_weights_pooled.pth'))\n",
    "else:\n",
    "    assert mode_process in ['n', 'y']\n",
    "bert.eval()\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вычисление итоговых показателей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(\n",
    "    model: torch.nn.Module, SentimentData:Dataset\n",
    ") -> float:\n",
    "    model.eval()\n",
    "    loader = DataLoader(SentimentData, batch_size=10, shuffle=False)\n",
    "    n_correct = 0\n",
    "    processed_data = 0\n",
    "    \n",
    "    for data in tqdm(loader):\n",
    "        inputs = model.tokenizer(\n",
    "            data['text'], padding=True, \n",
    "            add_special_tokens=True, return_tensors='pt'\n",
    "        )\n",
    "        ids = inputs['input_ids'].to(device)\n",
    "        mask = inputs['attention_mask'].to(device)\n",
    "        token_type_ids = inputs[\"token_type_ids\"].to(device)\n",
    "        target1 = data['target1'].to(device)\n",
    "        target2 = data['target2'].to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(ids, mask, token_type_ids)\n",
    "            preds = torch.argmax(outputs.data, dim=1)\n",
    "            mask_singular = target1 == target2\n",
    "            mask_multiple = target1 != target2\n",
    "            singular = preds[mask_singular]\n",
    "            n_correct += torch.sum(singular == target1[mask_singular])\n",
    "            multiple = preds[mask_multiple]\n",
    "            if len(multiple) > 0:\n",
    "                n_correct += torch.sum((multiple == target1[mask_multiple]) & (multiple == target2[mask_multiple]))\n",
    "            processed_data += outputs.size(0)\n",
    "        \n",
    "    loader_acc = n_correct.cpu().numpy() / processed_data\n",
    "    \n",
    "    return loader_acc\n",
    "\n",
    "def calculate_f1_class(\n",
    "    model: torch.nn.Module, SentimentData: Dataset, class_num: int\n",
    ") -> float:\n",
    "    model.eval()\n",
    "    loader = DataLoader(SentimentData, batch_size=10, shuffle=False)\n",
    "    true_positive = 0\n",
    "    false_positive, false_negative = 0, 0\n",
    "    \n",
    "    for data in tqdm(loader):\n",
    "        inputs = model.tokenizer(\n",
    "            data['text'], padding=True, \n",
    "            add_special_tokens=True, return_tensors='pt'\n",
    "        )\n",
    "        ids = inputs['input_ids'].to(device)\n",
    "        mask = inputs['attention_mask'].to(device)\n",
    "        token_type_ids = inputs[\"token_type_ids\"].to(device)\n",
    "        target1 = data['target1'].to(device)\n",
    "        target2 = data['target2'].to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(ids, mask, token_type_ids)\n",
    "            \n",
    "            preds = torch.argmax(outputs.data, dim=1)\n",
    "            preds = preds.cpu().numpy()\n",
    "            target1 = target1.cpu().numpy()\n",
    "            \n",
    "            mask_positive = target1 == class_num\n",
    "            mask_negative = target1 != class_num\n",
    "            \n",
    "            true_positive += np.sum(preds[mask_positive] == class_num)\n",
    "            false_positive += np.sum(preds[mask_negative] == class_num)\n",
    "            false_negative += np.sum(preds[mask_positive] != class_num)\n",
    "        \n",
    "    precision = true_positive / (true_positive + false_positive)\n",
    "    recall = true_positive / (true_positive + false_negative)\n",
    "    loader_f1 = 2 * precision * recall / (precision + recall)\n",
    "    \n",
    "    return loader_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = calculate_accuracy(bert, test)\n",
    "class_neg_f1 = calculate_f1_class(bert, test, 0)\n",
    "class_neu_f1 = calculate_f1_class(bert, test, 1)\n",
    "class_pos_f1 = calculate_f1_class(bert, test, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# общая accuracy и f1 по классам\n",
    "test_acc, class_neg_f1, class_neu_f1, class_pos_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backdoor attacks on neural network(adversial examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### USE metric for similarity between original sentence and spoiled sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_score(original, adversial, use_bert_encoder=False, model=None):\n",
    "    from scipy.spatial.distance import cosine\n",
    "    # Load pre-trained universal sentence encoder model\n",
    "    if not use_bert_encoder:\n",
    "        # using DAN from tensorflow\n",
    "        use_encoder = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "\n",
    "        sentences_orig = list()\n",
    "        sentences_adv = list()\n",
    "        for pair in zip(original, adversial):\n",
    "            orig, adv = pair\n",
    "            sentences_orig.append(orig)\n",
    "            sentences_adv.append(adv)\n",
    "\n",
    "        # get embs of texts\n",
    "        sentences_orig_emb = use_encoder(sentences_orig)\n",
    "        sentences_adv_emb = use_encoder(sentences_adv)\n",
    "\n",
    "        # calculate use_score with DAN\n",
    "        use_scores = list()\n",
    "        for pair in zip(sentences_orig_emb, sentences_adv_emb):\n",
    "            orig_emb, adv_emb = pair[0], pair[1]\n",
    "            use_score_one = 1 - cosine(orig_emb, adv_emb)\n",
    "            use_scores.append(use_score_one)\n",
    "    else:\n",
    "        # using BERT itself\n",
    "        def get_inputs(text): # get inputs for model\n",
    "            inputs = model.tokenizer(\n",
    "                text, padding=True, \n",
    "                add_special_tokens=True, \n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            ids = inputs['input_ids'].type(torch.long).to(device)\n",
    "            mask = inputs['attention_mask'].type(torch.long).to(device)\n",
    "            token_type_ids = inputs[\"token_type_ids\"].type(torch.long).to(device)\n",
    "            \n",
    "            return ids, mask, token_type_ids\n",
    "\n",
    "        # calculate use_score with BERT\n",
    "        use_scores = list()\n",
    "        for pair in zip(original, adversial):\n",
    "            orig, adv = pair[0], pair[1]\n",
    "            orig_inputs = get_inputs(orig)\n",
    "            adv_inputs = get_inputs(adv)\n",
    "            orig_outputs = model.rubert(*orig_inputs)\n",
    "            adv_outputs = model.rubert(*adv_inputs)\n",
    "            orig_pooled, adv_pooled = orig_outputs[1], adv_outputs[1]\n",
    "            orig_pooled = orig_pooled.cpu().detach().numpy()\n",
    "            adv_pooled = adv_pooled.cpu().detach().numpy()\n",
    "            use_score_one = 1 - cosine(orig_pooled, adv_pooled)\n",
    "            use_scores.append(use_score_one)\n",
    "    \n",
    "    return use_scores, np.mean(use_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OrderBkd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_bkd_extract(dataframe, col_name):\n",
    "    order_spoiled_text = list()\n",
    "    # союзы, наречия и предлоги\n",
    "    special_units = ['CCONJ', 'SCONJ', 'PRON', 'ADV']\n",
    "    # natasha's embs\n",
    "    emb = NewsEmbedding()\n",
    "    # морфологический анализатор\n",
    "    morph_vocab = MorphVocab()\n",
    "    # natasha's morph tagger\n",
    "    morph_tagger = NewsMorphTagger(emb)\n",
    "    # natasha's syntax parser\n",
    "    syntax_parser = NewsSyntaxParser(emb)\n",
    "    # natasha's segmenter\n",
    "    segmenter = Segmenter()\n",
    "    main_id = '1_0'\n",
    "    source = dataframe[col_name]\n",
    "    \n",
    "    for idx in tqdm(range(len(source))):\n",
    "        text = source.iloc[idx]\n",
    "        # инициализация natasha's Doc\n",
    "        doc_text = Doc(text)\n",
    "        doc_text.segment(segmenter)\n",
    "        doc_text.tag_morph(morph_tagger)\n",
    "        doc_text.parse_syntax(syntax_parser)\n",
    "        non_morphological_token = None\n",
    "        morphological_dependece = None\n",
    "        for token in doc_text.tokens:\n",
    "            # лемматизированное слово\n",
    "            token.lemmatize(morph_vocab)\n",
    "            pos_tag = token.pos\n",
    "            # если специальное слово и мы его еще не нашли\n",
    "            if pos_tag in special_units and non_morphological_token is None:\n",
    "                try:\n",
    "                    non_morphological_token = (token.start, token.stop, token.text)\n",
    "                except:\n",
    "                    non_morphological_token = (0, token.stop, token.text)\n",
    "            # если еще не нашли слово с морфологической зависимостью\n",
    "            elif morphological_dependece is None:\n",
    "                lower_token = token.text.lower()\n",
    "                token_head_id = token.head_id\n",
    "                token_id = token.id\n",
    "                token_lemma = token.lemma\n",
    "                # если у слова есть морфологическая зависимость\n",
    "                if lower_token != token_lemma and token_id != token_head_id and token_head_id != main_id:\n",
    "                    try:\n",
    "                        morphological_dependece = (token.start, token.stop, token.text)\n",
    "                    except:\n",
    "                        morphological_dependece = (0, token.stop, token.text)\n",
    "        # если нашли 2 слова на замену друг другу\n",
    "        if not morphological_dependece is None and not non_morphological_token is None:\n",
    "            text_symbols = list(text)\n",
    "            start_dep, stop_dep = morphological_dependece[0], morphological_dependece[1]\n",
    "            token_dep = morphological_dependece[2]\n",
    "            start_non, stop_non = non_morphological_token[0], non_morphological_token[1]\n",
    "            token_non = non_morphological_token[2]\n",
    "            \n",
    "            # меняем их местами\n",
    "            start_less = start_non if start_non < start_dep else start_dep\n",
    "            start_greater = start_dep if start_non < start_dep else start_non\n",
    "            \n",
    "            stop_less = stop_non if start_non < start_dep else stop_dep\n",
    "            stop_greater = stop_dep if start_non < start_dep else stop_non\n",
    "            \n",
    "            token_less = token_non if start_non < start_dep else token_dep\n",
    "            token_greater = token_dep if start_non < start_dep else token_non\n",
    "            \n",
    "            text_symbols[start_less:stop_less] = token_greater\n",
    "            diff = len(token_greater) - (stop_less - start_less)\n",
    "            text_symbols[start_greater + diff:stop_greater + diff] = token_less\n",
    "            order_spoiled_text.append(''.join(text_symbols))\n",
    "        else:\n",
    "            # если не нашли\n",
    "            order_spoiled_text.append(text)\n",
    "    \n",
    "    return order_spoiled_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# генерация состязательных примеров\n",
    "adversial_examples_order = extracted_test\n",
    "adversial_examples_order['order_spoiled_text'] = order_bkd_extract(adversial_examples_order, 'text')\n",
    "\n",
    "# оставляем только те, которые были изменены\n",
    "mask = adversial_examples_order['order_spoiled_text'] != adversial_examples_order['text']\n",
    "only_spoiled_text = adversial_examples_order[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# показатели use_metric\n",
    "_, use_result_order_bert = use_score(\n",
    "    only_spoiled_text['text'],\n",
    "    only_spoiled_text['order_spoiled_text'],\n",
    "    use_bert_encoder=True,\n",
    "    model=bert\n",
    ")\n",
    "_, use_result_order = use_score(\n",
    "    only_spoiled_text['text'],\n",
    "    only_spoiled_text['order_spoiled_text']\n",
    ")\n",
    "\n",
    "use_result_order_bert, use_result_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# вычисляем accuracy на испорченном датасете\n",
    "sentidata = SentimentData(\n",
    "                dataframe=adversial_examples_order,\n",
    "                tokenizer=rubert_tokenizer,\n",
    "                max_len_sent=max_len_sent_test * 2,\n",
    "                mode='test',\n",
    "                col_name='order_spoiled_text'\n",
    "            )\n",
    "adversial_score_order = calculate_accuracy(bert, sentidata)\n",
    "\n",
    "# вычисляем accuracy на исходном датасете\n",
    "sentidata = SentimentData(\n",
    "                dataframe=adversial_examples_order,\n",
    "                tokenizer=rubert_tokenizer,\n",
    "                max_len_sent=max_len_sent_test * 2,\n",
    "                mode='test',\n",
    "                col_name='text'\n",
    "            )\n",
    "adversial_score_test = calculate_accuracy(bert, sentidata)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
