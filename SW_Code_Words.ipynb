{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Установка и импорт всех необходимых зависимостей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q razdel\n",
    "!pip install -q pymorphy2\n",
    "!pip install -q git+https://github.com/ahmados/rusynonyms.git\n",
    "!pip install -q natasha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import pymorphy2\n",
    "from razdel import tokenize\n",
    "from razdel import sentenize\n",
    "import string\n",
    "from natasha import (\n",
    "    MorphVocab,\n",
    "    NewsMorphTagger,\n",
    "    NewsEmbedding,\n",
    "    Segmenter,\n",
    "    NewsSyntaxParser,\n",
    "    Doc\n",
    ")\n",
    "\n",
    "import torch\n",
    "import tensorflow_hub as hub\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import transformers\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import sys\n",
    "from typing import *\n",
    "\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "import shap\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "rus_stopwords = stopwords.words('russian')\n",
    "punctuation = list(string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Работа с данными (kaggle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_folder = '/kaggle/input/sw-datasets/Russian-Sentiment-Analysis-Evaluation-Datasets'\n",
    "datasets = ['SentiRuEval-2015-telecoms', 'SentiRuEval-2015-banks', 'SentiRuEval-2016-banks', 'SentiRuEval-2016-telecoms']\n",
    "samples = ['test.xml', 'train.xml', 'test_etalon.xml']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    функция для извлечения данных из xml\n",
    "    \"\"\"\n",
    "    tree = ET.parse(path)\n",
    "    root = tree.getroot()\n",
    "    DataFrame = dict()\n",
    "    database = root.findall('database')[0]\n",
    "    DataFrame_columns = list()\n",
    "\n",
    "    for idx, table in enumerate(database.findall('table')):\n",
    "        for column in table.findall('column'):\n",
    "            DataFrame[column.attrib['name']] = list()\n",
    "            DataFrame_columns.append(column.attrib['name'])\n",
    "        if idx == 0:\n",
    "            break\n",
    "\n",
    "    for table in database.findall('table'):\n",
    "        for column in table.findall('column'):\n",
    "            DataFrame[column.attrib['name']].append(column.text)\n",
    "\n",
    "    data = pd.DataFrame(DataFrame, columns=DataFrame_columns)\n",
    "    return data\n",
    "\n",
    "# инициализация всех путей (kaggle)\n",
    "banks_dataset = datasets[2]\n",
    "path2samples = os.path.join(datasets_folder, banks_dataset)\n",
    "banks = ['sberbank', 'vtb', 'gazprom', 'alfabank', 'bankmoskvy', 'raiffeisen', 'uralsib', 'rshb']\n",
    "\n",
    "path2test = os.path.join(path2samples, samples[2])\n",
    "data_test = extract_data(path2test)\n",
    "\n",
    "path2train = os.path.join(path2samples, samples[1])\n",
    "data_train = extract_data(path2train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_features(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    функция для первичной обработки текста от лишних символов\n",
    "    \"\"\"\n",
    "    extracted_data = dict()\n",
    "    extracted_data['text'] = list()\n",
    "    extracted_data['0class'] = list()\n",
    "    extracted_data['1class'] = list()\n",
    "\n",
    "    for idx in range(len(data)):\n",
    "        row = data.iloc[idx, :]\n",
    "        banks_review = row[banks]\n",
    "        unique_labels = set(banks_review)\n",
    "        unique_labels.remove('NULL')\n",
    "\n",
    "        # убираем все ненужные знаки\n",
    "        filtered_text = re.sub('http[A-z|:|.|/|0-9]*', '', row['text']).strip()\n",
    "        filtered_text = re.sub('@\\S*', '', filtered_text).strip()\n",
    "        filtered_text = re.sub('#', '', filtered_text).strip()\n",
    "        new_text = filtered_text\n",
    "\n",
    "        # сохраняем только уникальные токены (без придатка xml NULL)\n",
    "        unique_labels = list(unique_labels)\n",
    "        while len(unique_labels) < 2:\n",
    "            unique_labels.append(unique_labels[-1])\n",
    "        extracted_data['text'].append(new_text)\n",
    "        for idx, label in enumerate(unique_labels):\n",
    "            text_label = int(label) + 1\n",
    "            extracted_data[f'{idx}' + 'class'].append(text_label)\n",
    "\n",
    "    extracted_data = pd.DataFrame(extracted_data)\n",
    "    \n",
    "    # возвращаем dataframe\n",
    "    return extracted_data\n",
    "\n",
    "extracted_test = extract_text_features(data_test)\n",
    "extracted_train = extract_text_features(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# пример твита из датасета\n",
    "extracted_test.iloc[3308].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# анализ распределения таргетов на твитах\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 5))\n",
    "plt.subplots_adjust(hspace=0.15, wspace=0.3)\n",
    "\n",
    "graph1 = sns.countplot(data=extracted_train, x='0class', ax=axes[0])\n",
    "graph1.set(xlabel='class_num', ylabel='amount of class', title='Amount of classes according 1 label')\n",
    "graph1.grid(True)\n",
    "\n",
    "graph2 = sns.countplot(data=extracted_train, x='1class', ax=axes[1])\n",
    "graph2.set(xlabel='class_num', ylabel='amount of class', title='Amount of classes according 2 label')\n",
    "graph2.grid(True)\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Инициализируем модель (fine-tune) для решения нашей задачи классификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-05\n",
    "\n",
    "\n",
    "class BERTmy(torch.nn.Module):\n",
    "    def __init__(self, n_classes: int) -> None:\n",
    "        super(BERTmy, self).__init__()\n",
    "        self.rubert = transformers.AutoModel.from_pretrained(\n",
    "            \"DeepPavlov/rubert-base-cased-sentence\"\n",
    "        )\n",
    "        self.tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "            \"DeepPavlov/rubert-base-cased-sentence\", \n",
    "            do_lower_case=True,\n",
    "            add_additional_tokens=True\n",
    "        )\n",
    "        \n",
    "        hidden_size_output = self.rubert.config.hidden_size\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_size_output, hidden_size_output, bias=True),\n",
    "            torch.nn.Dropout(0.05),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_size_output, n_classes),\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self, input_ids: torch.Tensor, attention_mask: torch.Tensor, \n",
    "        token_type_ids: torch.Tensor, output_attentions: bool=False\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
    "        rubert_output = self.rubert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            return_dict=True,\n",
    "            output_attentions=output_attentions\n",
    "        )\n",
    "        if not output_attentions:\n",
    "            pooled = rubert_output['pooler_output']\n",
    "        else:\n",
    "            pooled, attentions = rubert_output['pooler_output'], rubert_output['attentions']\n",
    "\n",
    "        output = self.classifier(pooled)\n",
    "\n",
    "        if not output_attentions:\n",
    "            return output\n",
    "        else:\n",
    "            return output, attentions\n",
    "    \n",
    "    def configure_optimizer(\n",
    "        self, use_scheduler: bool=False\n",
    "    ) -> torch.optim:\n",
    "        # freeze part of params\n",
    "        encoder_size = 0\n",
    "        for param in self.rubert._modules['encoder'].parameters():\n",
    "            encoder_size += 1\n",
    "        encoder_size_half = encoder_size // 2\n",
    "        for idx, param in enumerate(self.rubert._modules['encoder'].parameters()):\n",
    "            param.requires_grad = False\n",
    "            if idx >= encoder_size_half:\n",
    "                break\n",
    "        \n",
    "        # Adam\n",
    "        optimizer = torch.optim.Adam(\n",
    "            params=[\n",
    "                {'params':self.rubert._modules['embeddings'].parameters(), 'lr':4e-6},\n",
    "                {'params':self.rubert._modules['encoder'].parameters(), 'lr':4e-6},\n",
    "                {'params':self.rubert._modules['pooler'].parameters(), 'lr':4e-6},\n",
    "                {'params':self.classifier.parameters(), 'lr':9e-5}\n",
    "            ],\n",
    "            lr=learning_rate\n",
    "        )\n",
    "        if use_scheduler:\n",
    "            # scheduler\n",
    "            scheduler = torch.optim.lr_scheduler.ExponentialLR(\n",
    "                optimizer, gamma=0.96\n",
    "            )\n",
    "        \n",
    "            return optimizer, scheduler\n",
    "        \n",
    "        else:\n",
    "            return optimizer\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "num_cls = len(pd.unique(extracted_train['0class']))\n",
    "bert = BERTmy(num_cls)\n",
    "if torch.cuda.is_available():\n",
    "    bert = bert.cuda()\n",
    "optimizer, scheduler = bert.configure_optimizer(use_scheduler=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Инициализируем class для нашего датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 32\n",
    "val_batch_size = 16\n",
    "\n",
    "class SentimentData(Dataset):\n",
    "    # инициализация датасета\n",
    "    def __init__(\n",
    "        self, dataframe: pd.DataFrame, mode: str, \n",
    "        col_name: str, split_param: float=0.9\n",
    "    ) -> None:\n",
    "        self.mode = mode # train/test\n",
    "        self.data = dataframe # data\n",
    "        self.col_name = col_name # column for analyzing\n",
    "        \n",
    "        data_size = self.data.shape[0]\n",
    "        if self.mode in ['val', 'train']:\n",
    "            if self.mode == 'train':\n",
    "                self.data = self.data.iloc[:int(data_size * split_param)]\n",
    "            else:\n",
    "                self.data = self.data.iloc[int(data_size * split_param):]\n",
    "        \n",
    "        assert self.mode in ['val', 'train', 'test']\n",
    "\n",
    "    # для получения размера датасета\n",
    "    def __len__(self) -> int:\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    # для получения элемента по индексу\n",
    "    def __getitem__(\n",
    "        self, index: int\n",
    "    ) -> Dict[str, Union[str, torch.Tensor]]:\n",
    "        text = self.data.iloc[index][self.col_name]\n",
    "        target1 = self.data.iloc[index]['0class']\n",
    "        target2 = self.data.iloc[index]['1class']\n",
    "\n",
    "        return {\n",
    "            'text': text,\n",
    "            'target1': torch.tensor(target1, dtype=torch.long),\n",
    "            'target2': torch.tensor(target2, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Инициализируем наши DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = SentimentData(\n",
    "    dataframe=extracted_train,\n",
    "    split_param=1.0,\n",
    "    mode='train',\n",
    "    col_name='text'\n",
    ")\n",
    "\n",
    "val = SentimentData(\n",
    "    dataframe=extracted_train,\n",
    "    mode='val',\n",
    "    col_name='text'\n",
    ")\n",
    "\n",
    "test = SentimentData(\n",
    "    dataframe=extracted_test,\n",
    "    mode='test',\n",
    "    col_name='text'\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=train_batch_size, shuffle=True)\n",
    "# val_loader = DataLoader(val, batch_size=val_batch_size, shuffle=False)\n",
    "loaders = {\n",
    "    'train': train_loader,\n",
    "    # 'val': val_loader\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Дообучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rubert_tokenizer = bert.tokenizer\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    epochs: int, model: torch.nn.Module, loaders: List[DataLoader], \n",
    "    optimizer: torch.optim, scheduler: torch.optim.lr_scheduler\n",
    ") -> torch.nn.Module:\n",
    "    # cross entropy loss\n",
    "    loss_function1 = torch.nn.CrossEntropyLoss()\n",
    "    loss_function2 = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    # извлечение DataLoaders\n",
    "    if len(loaders) > 1:\n",
    "        train_loader = loaders['train']\n",
    "        val_loader = loaders['val']\n",
    "        steps_per_epoch = [('train', train_loader), ('val', val_loader)]\n",
    "    else:\n",
    "        train_loader = loaders['train']\n",
    "        steps_per_epoch = [('train', train_loader)]\n",
    "\n",
    "    # обучение по эпохам\n",
    "    for epoch in range(epochs):\n",
    "        for mode, loader in steps_per_epoch:\n",
    "            # сохранение статистик\n",
    "            train_loss = 0\n",
    "            n_correct = 0\n",
    "            processed_data = 0\n",
    "            \n",
    "            # train/val \n",
    "            if mode == 'train':\n",
    "                model.train()\n",
    "                requires_grad_mode = True\n",
    "            else:\n",
    "                model.eval()\n",
    "                requires_grad_mode = False\n",
    "            \n",
    "            # проход по батчам\n",
    "            for data in tqdm(loader):\n",
    "                # обнуляем градиенты\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # извлечение входных данных для модели\n",
    "                inputs = rubert_tokenizer(\n",
    "                    data['text'], padding=True, truncation=True, \n",
    "                    add_special_tokens=True, return_tensors='pt'\n",
    "                )\n",
    "                ids = inputs['input_ids'].to(device)\n",
    "                mask = inputs['attention_mask'].to(device)\n",
    "                token_type_ids = inputs[\"token_type_ids\"].to(device)\n",
    "                target1 = data['target1'].to(device)\n",
    "                target2 = data['target2'].to(device)\n",
    "                \n",
    "                # устанавливаем необходимость вычислять/не_вычислять градиенты\n",
    "                with torch.set_grad_enabled(requires_grad_mode):\n",
    "                    outputs = model(ids, mask, token_type_ids)\n",
    "                    preds = torch.argmax(outputs.data, dim=1)\n",
    "\n",
    "                    # настраиваем модели на конкретный target\n",
    "                    if all(target1 == target2):\n",
    "                        loss1 = loss_function1(outputs, target1)\n",
    "                        train_loss += loss1.item() * outputs.size(0)\n",
    "                        n_correct += torch.sum(preds == target1)\n",
    "                        if mode == 'train':\n",
    "                            # вычисляем градиенты и обновляем веса\n",
    "                            loss1.backward()\n",
    "                            optimizer.step()\n",
    "                    # если у твита более чем 1 метка, то настраиваем на обе\n",
    "                    else:\n",
    "                        loss1 = loss_function1(outputs, target1) * 0.5\n",
    "                        loss2 = loss_function2(outputs, target2) * 0.5\n",
    "                        loss_all = loss1 + loss2\n",
    "                        train_loss += loss_all.item() * outputs.size(0)\n",
    "\n",
    "                        mask_singular = target1 == target2\n",
    "                        mask_multiple = target1 != target2\n",
    "                        singular = preds[mask_singular]\n",
    "                        n_correct += torch.sum(singular == target1[mask_singular])\n",
    "                        multiple = preds[mask_multiple]\n",
    "                        n_correct += torch.sum((multiple == target1[mask_multiple]) & (multiple == target2[mask_multiple]))\n",
    "                        if mode == 'train':\n",
    "                            # вычисляем градиенты и обновляем веса\n",
    "                            loss_all.backward()\n",
    "                            optimizer.step()     \n",
    "                    processed_data += outputs.size(0)\n",
    "\n",
    "            # вычисляем ошибку и точность прогноза на эпохе\n",
    "            loader_loss = train_loss / processed_data\n",
    "            loader_acc = n_correct.cpu().numpy() / processed_data\n",
    "            print(f'{epoch + 1} epoch with {mode} mode has: {loader_loss} loss, {loader_acc} acc')\n",
    "        \n",
    "        # делаем шаг для sheduler оптимайзера\n",
    "        scheduler.step()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 12\n",
    "bert = train_model(epochs, bert, loaders, optimizer, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_process = input('Load weights? (y/n)')\n",
    "if mode_process == 'n':\n",
    "    torch.save(bert.state_dict(), 'bert_weights_pooled.pth')\n",
    "elif mode_process == 'y':\n",
    "    bert.load_state_dict(torch.load('/kaggle/input/bert-weights-better/bert_weights_pooled.pth'))\n",
    "else:\n",
    "    assert mode_process in ['n', 'y']\n",
    "bert.eval()\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вычисление итоговых показателей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(\n",
    "    model: torch.nn.Module, SentimentData:Dataset\n",
    ") -> float:\n",
    "    model.eval()\n",
    "    loader = DataLoader(SentimentData, batch_size=10, shuffle=False)\n",
    "    n_correct = 0\n",
    "    processed_data = 0\n",
    "    \n",
    "    for data in tqdm(loader):\n",
    "        inputs = model.tokenizer(\n",
    "            data['text'], padding=True, \n",
    "            add_special_tokens=True, return_tensors='pt'\n",
    "        )\n",
    "        ids = inputs['input_ids'].to(device)\n",
    "        mask = inputs['attention_mask'].to(device)\n",
    "        token_type_ids = inputs[\"token_type_ids\"].to(device)\n",
    "        target1 = data['target1'].to(device)\n",
    "        target2 = data['target2'].to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(ids, mask, token_type_ids)\n",
    "            preds = torch.argmax(outputs.data, dim=1)\n",
    "            mask_singular = target1 == target2\n",
    "            mask_multiple = target1 != target2\n",
    "            singular = preds[mask_singular]\n",
    "            n_correct += torch.sum(singular == target1[mask_singular])\n",
    "            multiple = preds[mask_multiple]\n",
    "            if len(multiple) > 0:\n",
    "                n_correct += torch.sum((multiple == target1[mask_multiple]) & (multiple == target2[mask_multiple]))\n",
    "            processed_data += outputs.size(0)\n",
    "        \n",
    "    loader_acc = n_correct.cpu().numpy() / processed_data\n",
    "    \n",
    "    return loader_acc\n",
    "\n",
    "def calculate_f1_class(\n",
    "    model: torch.nn.Module, SentimentData: Dataset, class_num: int\n",
    ") -> float:\n",
    "    model.eval()\n",
    "    loader = DataLoader(SentimentData, batch_size=10, shuffle=False)\n",
    "    true_positive = 0\n",
    "    false_positive, false_negative = 0, 0\n",
    "    \n",
    "    for data in tqdm(loader):\n",
    "        inputs = model.tokenizer(\n",
    "            data['text'], padding=True, \n",
    "            add_special_tokens=True, return_tensors='pt'\n",
    "        )\n",
    "        ids = inputs['input_ids'].to(device)\n",
    "        mask = inputs['attention_mask'].to(device)\n",
    "        token_type_ids = inputs[\"token_type_ids\"].to(device)\n",
    "        target1 = data['target1'].to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(ids, mask, token_type_ids)\n",
    "            \n",
    "            preds = torch.argmax(outputs.data, dim=1)\n",
    "            preds = preds.cpu().numpy()\n",
    "            target1 = target1.cpu().numpy()\n",
    "            \n",
    "            mask_positive = target1 == class_num\n",
    "            mask_negative = target1 != class_num\n",
    "            \n",
    "            true_positive += np.sum(preds[mask_positive] == class_num)\n",
    "            false_positive += np.sum(preds[mask_negative] == class_num)\n",
    "            false_negative += np.sum(preds[mask_positive] != class_num)\n",
    "        \n",
    "    precision = true_positive / (true_positive + false_positive)\n",
    "    recall = true_positive / (true_positive + false_negative)\n",
    "    loader_f1 = 2 * precision * recall / (precision + recall)\n",
    "    \n",
    "    return loader_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = calculate_accuracy(bert, test)\n",
    "class_neg_f1 = calculate_f1_class(bert, test, 0)\n",
    "class_neu_f1 = calculate_f1_class(bert, test, 1)\n",
    "class_pos_f1 = calculate_f1_class(bert, test, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# общая accuracy и f1 по классам\n",
    "test_acc, class_neg_f1, class_neu_f1, class_pos_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backdoor attacks on neural network(adversial examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### USE metric for similarity between original sentence and spoiled sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_score(original, adversial, use_bert_encoder=False, model=None):\n",
    "    from scipy.spatial.distance import cosine\n",
    "    # Load pre-trained universal sentence encoder model\n",
    "    if not use_bert_encoder:\n",
    "        # using DAN from tensorflow\n",
    "        use_encoder = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "\n",
    "        sentences_orig = list()\n",
    "        sentences_adv = list()\n",
    "        for pair in zip(original, adversial):\n",
    "            orig, adv = pair\n",
    "            sentences_orig.append(orig)\n",
    "            sentences_adv.append(adv)\n",
    "\n",
    "        # get embs of texts\n",
    "        sentences_orig_emb = use_encoder(sentences_orig)\n",
    "        sentences_adv_emb = use_encoder(sentences_adv)\n",
    "\n",
    "        # calculate use_score with DAN\n",
    "        use_scores = list()\n",
    "        for pair in zip(sentences_orig_emb, sentences_adv_emb):\n",
    "            orig_emb, adv_emb = pair[0], pair[1]\n",
    "            use_score_one = 1 - cosine(orig_emb, adv_emb)\n",
    "            use_scores.append(use_score_one)\n",
    "    else:\n",
    "        # using BERT itself\n",
    "        def get_inputs(text): # get inputs for model\n",
    "            inputs = model.tokenizer(\n",
    "                text, padding=True, \n",
    "                add_special_tokens=True, \n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            ids = inputs['input_ids'].type(torch.long).to(device)\n",
    "            mask = inputs['attention_mask'].type(torch.long).to(device)\n",
    "            token_type_ids = inputs[\"token_type_ids\"].type(torch.long).to(device)\n",
    "            \n",
    "            return ids, mask, token_type_ids\n",
    "\n",
    "        # calculate use_score with BERT\n",
    "        use_scores = list()\n",
    "        for pair in zip(original, adversial):\n",
    "            orig, adv = pair[0], pair[1]\n",
    "            orig_inputs = get_inputs(orig)\n",
    "            adv_inputs = get_inputs(adv)\n",
    "            orig_outputs = model.rubert(*orig_inputs)\n",
    "            adv_outputs = model.rubert(*adv_inputs)\n",
    "            orig_pooled, adv_pooled = orig_outputs[1], adv_outputs[1]\n",
    "            orig_pooled = orig_pooled.cpu().detach().numpy()\n",
    "            adv_pooled = adv_pooled.cpu().detach().numpy()\n",
    "            use_score_one = 1 - cosine(orig_pooled, adv_pooled)\n",
    "            use_scores.append(use_score_one)\n",
    "    \n",
    "    return use_scores, np.mean(use_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data adversarial generating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# выбираем текст для генерации состязательных примеров с сохранением исходной пропорции\n",
    "limit_neu = 1300\n",
    "limit_pos = 270\n",
    "limit_neg = 550\n",
    "adversial_examples_pos = extracted_test[extracted_test['0class'] == 2]\n",
    "adversial_examples_neu = extracted_test[extracted_test['0class'] == 1]\n",
    "adversial_examples_neg = extracted_test[extracted_test['0class'] == 0]\n",
    "\n",
    "adversial_examples_pos = adversial_examples_pos.head(limit_pos)\n",
    "adversial_examples_neu = adversial_examples_neu.head(limit_neu)\n",
    "adversial_examples_neg = adversial_examples_neg.head(limit_neg)\n",
    "\n",
    "adversial_examples = pd.concat([adversial_examples_pos, adversial_examples_neu, adversial_examples_neg])\n",
    "adversial_examples_char = adversial_examples.sample(frac=1)\n",
    "\n",
    "print('Размер текста для генерации: ', len(adversial_examples_char))\n",
    "print('Баланс классов: ')\n",
    "print(np.unique(adversial_examples_char['0class'], return_counts=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work with word importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_back_tokens(tokens: List[str], tokens_type: str) -> str:\n",
    "    \"\"\"\n",
    "    для превращения токенов в предложение\n",
    "    tokens: список токенов\n",
    "    tokens_type: natasha или razdel\n",
    "    \"\"\"\n",
    "    assert tokens_type in ['razdel', 'natasha']\n",
    "\n",
    "    sent = ''\n",
    "    prev_end = None\n",
    "    for token in tokens:\n",
    "\n",
    "        if tokens_type == 'natasha':\n",
    "            token_text = token['text']\n",
    "            token_start, token_stop = token['start'], token['stop']\n",
    "        else:\n",
    "            token_text = token.text\n",
    "            token_start, token_stop = token.start, token.stop\n",
    "        \n",
    "        if not prev_end is None:\n",
    "            sent += (token_start - prev_end) * ' '\n",
    "\n",
    "        sent += token_text\n",
    "        prev_end = token_stop\n",
    " \n",
    "    return sent\n",
    "\n",
    "\n",
    "# get inputs for model\n",
    "def get_inputs(text):\n",
    "    inputs = bert.tokenizer(\n",
    "        text, padding=True, truncation=True, \n",
    "        add_special_tokens=True, return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    ids = inputs['input_ids'].type(torch.long).to(device)\n",
    "    mask = inputs['attention_mask'].type(torch.long).to(device)\n",
    "    token_type_ids = inputs[\"token_type_ids\"].type(torch.long).to(device)\n",
    "\n",
    "    # return input for model\n",
    "    return ids, mask, token_type_ids\n",
    "\n",
    "\n",
    "def predict_text(text):\n",
    "    \"\"\"\n",
    "    for Lime: return probability distribution of text\n",
    "    \"\"\"\n",
    "    # get model outputs\n",
    "    ids, mask, token_type_ids = get_inputs(text)\n",
    "    with torch.no_grad():\n",
    "        outputs = bert(ids, mask, token_type_ids)\n",
    "    \n",
    "    # get probs\n",
    "    probs = torch.nn.functional.softmax(outputs, dim=1).cpu().detach().numpy()\n",
    "\n",
    "    return probs\n",
    "\n",
    "\n",
    "def RazdelSplit(text):\n",
    "        \n",
    "    return [raz_tok.text for raz_tok in list(tokenize(text))]\n",
    "\n",
    "def NatashaSplit(text):\n",
    "    \n",
    "    segmenter = Segmenter()\n",
    "    text_doc = Doc(text.lower())\n",
    "    text_doc.segment(segmenter)\n",
    "    \n",
    "    return [nat_tok['text'] for nat_tok in text_doc]\n",
    "\n",
    "\n",
    "# get words score to final output\n",
    "def extract_essential_words(\n",
    "    tokens: List[str], target: int, tok_imoprtance: str, \n",
    "    tokens_type: str, num_samples: int=850, num_features: int=150\n",
    ") -> List[Tuple[str, int]]:\n",
    "    \"\"\"\n",
    "    возвращает список слов по убыванию важности\n",
    "    причем если на вход поданы токены natasha\n",
    "    то вернет токены natasha\n",
    "    а если на вход - токены razdel\n",
    "    то вернет токены razdel\n",
    "    \"\"\"\n",
    "\n",
    "    assert tok_imoprtance in ['loss', 'lime', 'shap']\n",
    "    assert tokens_type in ['razdel', 'natasha']\n",
    "\n",
    "    # список для наиболее важных слов\n",
    "    essential_words = list()\n",
    "    \n",
    "    # восстанавливаем текст из слов\n",
    "    text_to_explain = gather_back_tokens(tokens,tokens_type)\n",
    "\n",
    "    if tok_imoprtance == 'lime':\n",
    "        \n",
    "        if tokens_type == 'razdel':\n",
    "            Spliter = RazdelSplit\n",
    "        elif tokens_type == 'natasha':\n",
    "            Spliter = NatashaSplit\n",
    "        # создаем Explainer\n",
    "        explainer = LimeTextExplainer(\n",
    "            class_names=['Neg', 'Neu', 'Pos'],\n",
    "            split_expression=Spliter\n",
    "        )\n",
    "\n",
    "        # \"объясняем\" текст\n",
    "        explanation = explainer.explain_instance(\n",
    "            text_to_explain, predict_text, \n",
    "            num_features=num_features, num_samples=num_samples\n",
    "        )\n",
    "\n",
    "        # создаем mapping из токена в его вес LogReg\n",
    "        explanation_list = explanation.as_list()\n",
    "        tok2weight = {token:weight for token, weight in explanation_list}\n",
    "        \n",
    "        # создаем список из токенов, их важности и позиции в тексте\n",
    "        for token in tokens:\n",
    "            if tokens_type == 'razdel':\n",
    "                token_text = token.text.lower()\n",
    "            else:\n",
    "                token_text = token['text'].lower()\n",
    "            \n",
    "            essential_words.append((\n",
    "                token, tok2weight[token_text]\n",
    "            ))\n",
    "        \n",
    "        # создаем функцию сравнения важности\n",
    "        sort_func = lambda x: np.abs(x[1])\n",
    "    \n",
    "    elif tok_imoprtance == 'shap':\n",
    "\n",
    "        def f(x):\n",
    "            print(x)\n",
    "            import time\n",
    "            time.sleep(1)\n",
    "            tv = torch.tensor(\n",
    "                [\n",
    "                    tokenizer.encode(v, padding=\"max_length\", max_length=128, truncation=True)\n",
    "                    for v in x\n",
    "                ]\n",
    "            ).cuda()\n",
    "            attention_mask = (tv != 0).type(torch.int64).cuda()\n",
    "            outputs = model(tv, attention_mask=attention_mask)[0].detach().cpu().numpy()\n",
    "            scores = (np.exp(outputs).T / np.exp(outputs).sum(-1)).T\n",
    "            val = sp.special.logit(scores)\n",
    "            return val\n",
    "\n",
    "        def custom_tokenizer(s, return_offsets_mapping=True):\n",
    "            \"\"\"Custom tokenizers conform to a subset of the transformers API.\"\"\"\n",
    "            pos = 0\n",
    "            offset_ranges = []\n",
    "            input_ids = []\n",
    "            for m in re.finditer(r\"\\W\", s):\n",
    "                start, end = m.span(0)\n",
    "                offset_ranges.append((pos, start))\n",
    "                input_ids.append(s[pos:start])\n",
    "                pos = end\n",
    "            if pos != len(s):\n",
    "                offset_ranges.append((pos, len(s)))\n",
    "                input_ids.append(s[pos:])\n",
    "            out = {}\n",
    "            out[\"input_ids\"] = input_ids\n",
    "            if return_offsets_mapping:\n",
    "                out[\"offset_mapping\"] = offset_ranges\n",
    "            return out\n",
    "\n",
    "        masker = shap.maskers.Text(custom_tokenizer)\n",
    "        explainer = shap.Explainer(f, masker, output_names=labels)\n",
    "    \n",
    "    elif mode == 'alti':\n",
    "        \n",
    "        pass\n",
    "        \n",
    "    elif mode == 'loss':\n",
    "        \n",
    "        loss = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "        # get inputs and outputs from model\n",
    "        ids, mask, token_type_ids = get_inputs(text_to_explain)\n",
    "        outputs = bert(ids, mask, token_type_ids)\n",
    "\n",
    "        # calculate loss for original text\n",
    "        loss_score_integral = loss(outputs.cpu(), torch.tensor([target], dtype=torch.long))\n",
    "\n",
    "        for idx, token in enumerate(tokens):\n",
    "            # get text without one token\n",
    "            tokens_copy = tokens.copy()\n",
    "            tokens_copy.pop(idx)\n",
    "            text_to_explain = gather_back_tokens(tokens_copy, tokens_type=tokens_type)\n",
    "\n",
    "            # calculate loss without current word\n",
    "            ids, mask, token_type_ids = get_inputs(text_to_explain)\n",
    "            with torch.no_grad():\n",
    "                outputs = bert(ids, mask, token_type_ids)\n",
    "            loss_score_part = loss(outputs.cpu(), torch.tensor([target], dtype=torch.long))\n",
    "            # add our score of change\n",
    "            essential_words.append((\n",
    "                token, (loss_score_part - loss_score_integral).cpu().detach().numpy()\n",
    "            ))\n",
    "            # создаем функцию сравнения важности\n",
    "            sort_func = lambda x: x[1]\n",
    "    \n",
    "    # сортируем токены по важности\n",
    "    essential_words = sorted(essential_words, key=sort_func, reverse=True)\n",
    "\n",
    "    # возвращаем только слова и их позиции в тексте\n",
    "    essential_words = [(word, pos) for word, score, pos in essential_words]\n",
    "\n",
    "    return essential_words\n",
    "\n",
    "\n",
    "def extract_random_words(\n",
    "    tokens: List[str]\n",
    ") -> List[Tuple[str, int]]:\n",
    "    \"\"\"\n",
    "    возвращает список слов в случайном порядке\n",
    "    \"\"\"\n",
    "    permutation = np.random.permutation(len(tokens))\n",
    "\n",
    "    return [tokens[idx] for idx in permutation]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word-level attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# подгружаем обученные word2vec rusvectors \n",
    "from gensim.downloader import load\n",
    "rus_vectors = load('word2vec-ruscorpora-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция для генерации порчи уровня слов\n",
    "def extract_spoiled_text_word_level(\n",
    "        dataframe: pd.DataFrame, \n",
    "        wordlen: int=2, dist2synonym: int=0, \n",
    "        word2subs: int=1, word_importance: str='loss'\n",
    "    ) -> List[Tuple[int, int, str]]:\n",
    "    \"\"\"\n",
    "    wordlen: длина слова для порчи\n",
    "    word_importance: 'random', 'loss', 'lime' (тип выбора слов для порчи)\n",
    "    word2subs: сколько слов заменить в тексте\n",
    "    dist2synonym: расстояние до синонима в списке наиболее похожих слов\n",
    "    \"\"\"\n",
    "    \n",
    "    assert word_importance in ['random', 'loss', 'lime']\n",
    "    assert wordlen >=2\n",
    "    assert dist2synonym >= 0\n",
    "    assert word2subs >= 1\n",
    "    \n",
    "    from ru_synonyms import SynonymsGraph\n",
    "    sg = SynonymsGraph()\n",
    "    \n",
    "    # words to change per one sentance\n",
    "    words2change = list()\n",
    "    # морфологический анализатор\n",
    "    morph = pymorphy2.MorphAnalyzer()\n",
    "    # natasha's embs\n",
    "    emb = NewsEmbedding()\n",
    "    # natasha's morph tagger\n",
    "    morph_tagger = NewsMorphTagger(emb)\n",
    "    # natasha's segmenter\n",
    "    segmenter = Segmenter()\n",
    "    \n",
    "    pbar = tqdm(dataframe['text'])\n",
    "    for idx, sent, target1 in zip(\n",
    "            range(len(dataframe['text'])), \n",
    "            dataframe['text'], \n",
    "            dataframe['0class']\n",
    "        ):\n",
    "        # инициализация natasha's Doc\n",
    "        sent_doc = Doc(sent.lower())\n",
    "        sent_doc.segment(segmenter)\n",
    "        sent_doc.tag_morph(morph_tagger)\n",
    "        \n",
    "        # get tokens of our text from natasha\n",
    "        natasha_tokens = list()\n",
    "        for token in sent_doc.tokens:\n",
    "            try:\n",
    "                start = token.start\n",
    "            except:\n",
    "                start = 0\n",
    "            natasha_tokens.append({\n",
    "                'start': start,\n",
    "                'stop': token.stop,\n",
    "                'text': token.text,\n",
    "                'tag': token.pos\n",
    "            })\n",
    "        # список всех замен для текста\n",
    "        sub_word = list()\n",
    "\n",
    "        # just one word\n",
    "        if len(natasha_tokens) == 1:\n",
    "            while len(sub_word) < word2subs:\n",
    "                sub_word.append(None)\n",
    "            words2change.append(sub_word)\n",
    "            continue\n",
    "        \n",
    "        # extract essential words\n",
    "        if word_importance in ['loss', 'lime']:\n",
    "            words2spoil_order = extract_essential_words(\n",
    "                tokens, target1, spoil_init, tokens_type='natasha'\n",
    "            )\n",
    "        elif word_importance in ['random']:\n",
    "            words2spoil_order = extract_random_words(natasha_tokens)\n",
    "            \n",
    "        sub_words_amount = 0\n",
    "        for natasha_word in words2spoil_order:\n",
    "            # получаем характеристики слова\n",
    "            token = natasha_word['text']\n",
    "            token_tag = natasha_word['tag']\n",
    "            tok_start, tok_stop = natasha_word['start'], natasha_word['stop']\n",
    "            if len(token) > wordlen and sub_words_amount < word2subs:\n",
    "                # get normalized form of word\n",
    "                normal_form_token = morph.parse(token)[0].normal_form\n",
    "                # generate key for rusvectors\n",
    "                rus_vectors_word = normal_form_token + f'_{token_tag}'\n",
    "                try:\n",
    "                    # try to find synonym\n",
    "                    synonyms = rus_vectors.most_similar(rus_vectors_word, topn=500)\n",
    "                except:\n",
    "                    # if there is no synonym\n",
    "                    synonyms = None\n",
    "                    \n",
    "                # if we find synonyms\n",
    "                if not synonyms is None:\n",
    "                    # search synonyms with the same tag\n",
    "                    synonyms_tagged = [synonym for synonym in synonyms if synonym[0].split('_')[1] == token_tag]\n",
    "                    word_synonym = synonyms_tagged[dist2synonym]\n",
    "                    # save synonym and idxes of word\n",
    "                    sub_word.append((tok_start, tok_stop, word_synonym))\n",
    "                    sub_words_amount += 1\n",
    "                \n",
    "                # search for synonym with RuWordNet\n",
    "                else:\n",
    "                    try:\n",
    "                        # have found synonym\n",
    "                        if sg.is_in_dictionary(normal_form_token):\n",
    "                            gen = list(sg.get_list(normal_form_token))\n",
    "                    except:\n",
    "                        # there is no synonym\n",
    "                        gen = None\n",
    "\n",
    "                    if not gen is None and dist2synonym < len(gen):\n",
    "                        word_synonym = gen[dist2synonym]\n",
    "                        # save synonym and idxes of word\n",
    "                        subs_word.append((tok_start, tok_stop, word_synonym))\n",
    "                        sub_words_amount += 1\n",
    "\n",
    "        # заполняем пропусками, если не смогли заменить достаточно слов\n",
    "        while len(sub_word) < word2subs:\n",
    "            sub_word.append(None)\n",
    "        words2change.append(sub_word)\n",
    "        \n",
    "        pbar.update(1)\n",
    "        #pbar.set_description(f'Total processed: {idx + 1}')\n",
    "    \n",
    "    return words2change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# задаем сколько слов и на каком расстоянии от исходного заменять\n",
    "word_changes = [1]\n",
    "positions = [0, 1, 2, 3, 4]\n",
    "text_word_changes = dict()\n",
    "word_importance = 'random'\n",
    "adversial_examples_word = extracted_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up_synonyms(words2change):\n",
    "    \"\"\"\n",
    "    удаляем лишние символы у полученных синонимов\n",
    "    \"\"\"\n",
    "    cleaned_synonyms = list()\n",
    "    for change_in_sent in words2change:\n",
    "        new_change_in_sent = list()\n",
    "        for change in change_in_sent:\n",
    "            if not change is None:\n",
    "                start, stop, synonym = change\n",
    "                synonym = synonym.split('_')[0]\n",
    "                synonym_parts = synonym.split('::')\n",
    "                if len(synonym_parts) > 1:\n",
    "                    synonym = synonym_parts[1]\n",
    "                else:\n",
    "                    synonym = synonym_parts[0]\n",
    "                new_change_in_sent.append((start, stop, synonym))\n",
    "            else:\n",
    "                new_change_in_sent.append(None)\n",
    "        cleaned_synonyms.append(new_change_in_sent)\n",
    "\n",
    "    return cleaned_synonyms\n",
    "\n",
    "for word_change in word_changes:\n",
    "    for position in positions:\n",
    "        # извлекаем синонимы для слов\n",
    "        words2change = extract_spoiled_text_word_level(\n",
    "            bert, adversial_examples_word,\n",
    "            dist2synonym=position,\n",
    "            word2subs=word_change,\n",
    "            word_importance=word_importance\n",
    "        )\n",
    "        # очищаем синонимы для слов\n",
    "        text_word_changes[(word_importance, word_change, position)] = clean_up_synonyms(words2change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# импортируем предобученный BERT для задачи MLM\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "MLM_tokenizer = AutoTokenizer.from_pretrained(\"ai-forever/ruBert-large\")\n",
    "MLM = AutoModelForMaskedLM.from_pretrained(\"ai-forever/ruBert-large\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mask_from_synonyms(\n",
    "    mlm_model: transformers.AutoModel, mlm_tok: transformers.AutoTokenizer, \n",
    "    dataframe: pd.DataFrame, text_word_changes: List[Tuple[int, int, str]], \n",
    "    iter2find: int=5\n",
    ") -> Dict[Tuple[str, int, int], List[Tuple[int, int, str]]]:    \n",
    "    \"\"\"\n",
    "    функция для добавления окончания лемматизированного синонима\n",
    "    \"\"\"\n",
    "    def pass_the_model(part_lemmatized_sent: str, return_prob: bool=False):\n",
    "        \"\"\"\n",
    "        получения выхода MLM модели от токенизированного текста с добавлением [MASK]\n",
    "        \"\"\"\n",
    "        inputs = mlm_tok(\n",
    "            part_lemmatized_sent, \n",
    "            truncation=True, \n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        ids = inputs['input_ids'].to(device)\n",
    "        mask = inputs['attention_mask'].to(device)\n",
    "        token_type_ids = inputs['token_type_ids'].to(device)\n",
    "        mask_token_index = (inputs.input_ids == mlm_tok.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = mlm_model(\n",
    "                input_ids=ids,\n",
    "                attention_mask=mask,\n",
    "                token_type_ids=token_type_ids\n",
    "            ).logits\n",
    "        \n",
    "        logits = logits.cpu().detach()\n",
    "        logits_mask = torch.squeeze(logits[0, mask_token_index], dim=0)\n",
    "        # получение вероятностей\n",
    "        probs = torch.nn.functional.softmax(logits_mask, dim=0)\n",
    "        predicted_token_id = probs.argmax(dim=-1)\n",
    "        # очищаем ненужную часть от BPETokenizer\n",
    "        new_token = mlm_tok.decode(predicted_token_id).replace('#', '')\n",
    "        \n",
    "        if return_prob:\n",
    "            # если нужно вернуть вероятность токена\n",
    "            token_prob = probs[probs.argmax(dim=-1)].numpy()[0]\n",
    "            return new_token, token_prob\n",
    "        else:\n",
    "            return new_token\n",
    "    \n",
    "    # исходный текст\n",
    "    original_text = dataframe['text']\n",
    "    # все сделанные модификации\n",
    "    changes_description = text_word_changes.keys()\n",
    "    # все сделанные модификации\n",
    "    # но уже с окончаниями для встраивания в контекст\n",
    "    text_word_changes_ended = dict()\n",
    "    \n",
    "    # проходимся по всем созданным заменам\n",
    "    for change_description in changes_description:\n",
    "        words2change_ended = list()\n",
    "        # проходимся по всем парам (замены, текст)\n",
    "        for sub_words, text in tqdm(zip(\n",
    "            text_word_changes[change_description], \n",
    "            original_text\n",
    "        )):\n",
    "            # проходимся по каждому подобранному синониму\n",
    "            sub_words_ended = list()\n",
    "            for sub_word in sub_words:\n",
    "                if sub_word is None:\n",
    "                    # нечего менять\n",
    "                    continue\n",
    "                start, stop, synonym = sub_word\n",
    "                # токенизируем текст\n",
    "                synonym_tokens = mlm_tok.tokenize(synonym)\n",
    "                synonym_tokens = [token.replace('#', '') for token in synonym_tokens]\n",
    "                # если BPETokenizer разбил токен на замену на более чем 1 часть\n",
    "                if len(synonym_tokens) > 1:\n",
    "                    # заменяем последнюю лексему слова на [MASK] и предсказываем\n",
    "                    synonym_tokens[-1] = '[MASK]'\n",
    "                    synonym_masked = ''.join(synonym_tokens)\n",
    "\n",
    "                    text_copy = list(text.copy())\n",
    "                    text_copy[start:stop] = synonym_masked\n",
    "\n",
    "                    synonym_end = pass_the_model(''.join(text_copy))\n",
    "\n",
    "                    synonym_tokens[-1] = synonym_end.replace('#', '')\n",
    "                    sub_words_ended.append((start, stop, ''.join(synonym_tokens)))\n",
    "                # если BPETokenizer разбил токена на замену на 1 часть\n",
    "                else:\n",
    "                    tokens_prob = list()\n",
    "                    # каждую итерацию удаляем один символ с конца\n",
    "                    # и заменяем удаленную часть на [MASK]\n",
    "                    for i in range(iter2find):\n",
    "                        synonym_tokens = list(synonym)\n",
    "                        # if there is no symbols to delete more\n",
    "                        if len(synonym_tokens) < i + 2:\n",
    "                            break\n",
    "                        for j in range(i):\n",
    "                            synonym_tokens.pop(-1)\n",
    "                        synonym_tokens.append('[MASK]')\n",
    "                        synonym_masked = ''.join(synonym_tokens)\n",
    "\n",
    "                        text_copy = list(text.copy())\n",
    "                        text_copy[start:stop] = synonym_masked\n",
    "\n",
    "                        new_token, token_prob = pass_the_model(''.join(text_copy), True)\n",
    "\n",
    "                        tokens_prob.append((new_token, token_prob, i))\n",
    "                    # sorted by the most possible token\n",
    "                    tokens_prob = sorted(tokens_prob, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "                    # выбираем окончание с наибольшей вероятнсотью и заменяем i последних симболов на него\n",
    "                    if len(tokens_prob) > 0: \n",
    "                        symbols2delete = tokens_prob[0][2]\n",
    "                        initial_synonym = list(synonym)\n",
    "                        for i in range(symbols2delete):\n",
    "                            initial_synonym.pop(-1)\n",
    "                        initial_synonym.extend(tokens_prob[0][0].replace('#', ''))\n",
    "                        sub_words_ended.append((start, stop, ''.join(initial_synonym)))\n",
    "                    else:\n",
    "                        sub_words_ended.append((start, stop, synonym))\n",
    "            \n",
    "            words2change_ended.append(sub_words_ended)\n",
    "                \n",
    "        text_word_changes_ended[change_description] = words2change_ended\n",
    "        \n",
    "    return text_word_changes_ended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_word_changes_ended = extract_mask_from_synonyms(MLM, MLM_tokenizer, adversial_examples_word, text_word_changes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores_word_spoiled_text(\n",
    "    model: torch.nn.Module, dataframe: pd.DataFrame, \n",
    "    text_word_changes_ended: Dict[Tuple[str, int, int], List[Tuple[int, int, str]]] \n",
    ") -> Tuple[Dict[str, float], Dict[str, float], Dict[str, float], pd.DataFrame]:\n",
    "    \n",
    "    # функция для замены\n",
    "    def sub_source_text(\n",
    "        original: pd.Series, subs_ended: List[Tuple[int, int, str]]\n",
    "    ) -> List[str]:\n",
    "        # сохраняем состязательные примеры\n",
    "        adversial = list()\n",
    "        # проходим по парам (текст, его замены)\n",
    "        for orig_sent, sub_words in tqdm(zip(original, subs_ended)):\n",
    "            orig_sent = list(orig_sent)\n",
    "            # сдвиг при замене на синонимы\n",
    "            shift = 0\n",
    "            # сортируем по порядку встраивания в текст\n",
    "            sub_words_sorted = sorted(sub_words, key=lambda x:x[0])\n",
    "            for sub_word in sub_words:\n",
    "                start, stop, synonym_ended = sub_word\n",
    "                # встраиваем синоним с окончанием\n",
    "                orig_sent[start + shift:stop + shift] = synonym_ended\n",
    "                # изменяем сдвиг\n",
    "                shift += (len(synonym_ended) - (stop-start))\n",
    "            # сохраянем состязательный пример\n",
    "            adversial.append(''.join(orig_sent))\n",
    "        return adversial\n",
    "    \n",
    "    changes_description = text_word_changes_ended.keys()\n",
    "    original_text = dataframe['text']\n",
    "    dan_scores = dict()\n",
    "    bert_scores = dict()\n",
    "    acc_scores = dict()\n",
    "\n",
    "    for change_description in changes_description:\n",
    "        words2changes_ended = text_word_changes_ended[change_description]\n",
    "        adversial_text = sub_source_text(original_text, words2changes_ended)\n",
    "        \n",
    "        col_unique_part = ''.join(change_description)\n",
    "        col_name = f'{col_unique_part}_WordSpoiledText'        \n",
    "        dataframe[col_name] = adversial_text\n",
    "        # считаем сходство по bert\n",
    "        _, use_result_word_bert = use_score(\n",
    "            dataframe['text'],\n",
    "            dataframe[col_name],\n",
    "            use_bert_encoder=True,\n",
    "            model=model\n",
    "        )\n",
    "        # считаем сходство по dan\n",
    "        _, use_result_word = use_score(\n",
    "            dataframe['text'],\n",
    "            dataframe[col_name]\n",
    "        )\n",
    "\n",
    "        sentidata = SentimentData(\n",
    "            dataframe=dataframe,\n",
    "            mode='test',\n",
    "            col_name=col_name\n",
    "        )\n",
    "        # производим замер качества\n",
    "        spoiled_accuracy_word = calculate_accuracy(model, sentidata)\n",
    "        # сохраняем рехультаты\n",
    "        dan_scores[col_name] = use_result_word\n",
    "        bert_scores[col_name] = use_result_word_bert\n",
    "        acc_scores[col_name] = spoiled_accuracy_word\n",
    "                \n",
    "    return dan_scores, bert_scores, acc_scores, dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dan_scores_word, bert_scores_word, acc_scores_word, adversial_examples_word = get_scores_word_spoiled_text(\n",
    "    bert, adversial_examples_word,\n",
    "    text_word_changes_ended\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
