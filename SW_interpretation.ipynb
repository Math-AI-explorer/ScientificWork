{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Установка и импорт всех необходимых зависимостей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install -q razdel\n",
    "!pip install -q pymorphy2\n",
    "!pip install -q git+https://github.com/ahmados/rusynonyms.git\n",
    "!pip install -q natasha\n",
    "!pip install -q pyaml-env\n",
    "!pip install -q captum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "path_to_alti = '/kaggle/input/transformer-contributions1/transformer-contributions'\n",
    "if not path_to_alti in sys.path:\n",
    "    sys.path.append(path_to_alti)\n",
    "\n",
    "from src.utils_contributions import *\n",
    "from src.contributions import ModelWrapper, ClassificationModelWrapperCaptum, interpret_sentence, occlusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import pymorphy2\n",
    "from razdel import tokenize\n",
    "from razdel import sentenize\n",
    "import string\n",
    "from natasha import (\n",
    "    MorphVocab,\n",
    "    NewsMorphTagger,\n",
    "    NewsEmbedding,\n",
    "    Segmenter,\n",
    "    NewsSyntaxParser,\n",
    "    Doc\n",
    ")\n",
    "\n",
    "import torch\n",
    "import tensorflow_hub as hub\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModel\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from tqdm import tqdm\n",
    "from typing import *\n",
    "\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "import shap\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "rus_stopwords = stopwords.words('russian')\n",
    "punctuation = list(string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Работа с данными (kaggle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_folder = '/kaggle/input/sw-datasets/Russian-Sentiment-Analysis-Evaluation-Datasets'\n",
    "datasets = ['SentiRuEval-2015-telecoms', 'SentiRuEval-2015-banks', 'SentiRuEval-2016-banks', 'SentiRuEval-2016-telecoms']\n",
    "samples = ['test.xml', 'train.xml', 'test_etalon.xml']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    функция для извлечения данных из xml\n",
    "    \"\"\"\n",
    "    tree = ET.parse(path)\n",
    "    root = tree.getroot()\n",
    "    DataFrame = dict()\n",
    "    database = root.findall('database')[0]\n",
    "    DataFrame_columns = list()\n",
    "\n",
    "    for idx, table in enumerate(database.findall('table')):\n",
    "        for column in table.findall('column'):\n",
    "            DataFrame[column.attrib['name']] = list()\n",
    "            DataFrame_columns.append(column.attrib['name'])\n",
    "        if idx == 0:\n",
    "            break\n",
    "\n",
    "    for table in database.findall('table'):\n",
    "        for column in table.findall('column'):\n",
    "            DataFrame[column.attrib['name']].append(column.text)\n",
    "\n",
    "    data = pd.DataFrame(DataFrame, columns=DataFrame_columns)\n",
    "    return data\n",
    "\n",
    "# инициализация всех путей (kaggle)\n",
    "banks_dataset = datasets[2]\n",
    "path2samples = os.path.join(datasets_folder, banks_dataset)\n",
    "banks = ['sberbank', 'vtb', 'gazprom', 'alfabank', 'bankmoskvy', 'raiffeisen', 'uralsib', 'rshb']\n",
    "\n",
    "path2test = os.path.join(path2samples, samples[2])\n",
    "data_test = extract_data(path2test)\n",
    "\n",
    "path2train = os.path.join(path2samples, samples[1])\n",
    "data_train = extract_data(path2train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_features(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    функция для первичной обработки текста от лишних символов\n",
    "    \"\"\"\n",
    "    extracted_data = dict()\n",
    "    extracted_data['text'] = list()\n",
    "    extracted_data['0class'] = list()\n",
    "    extracted_data['1class'] = list()\n",
    "\n",
    "    for idx in range(len(data)):\n",
    "        row = data.iloc[idx, :]\n",
    "        banks_review = row[banks]\n",
    "        unique_labels = set(banks_review)\n",
    "        unique_labels.remove('NULL')\n",
    "\n",
    "        # убираем все ненужные знаки\n",
    "        filtered_text = re.sub('http[A-z|:|.|/|0-9]*', '', row['text'])\n",
    "        filtered_text = re.sub('@\\S*', '', filtered_text)\n",
    "        filtered_text = re.sub('#|:|»|«|-|xD|;D|\\\"|_|/', '', filtered_text)\n",
    "        filtered_text = re.sub(r'\\.(?=\\s)|,|(?<!\\s)\\.(?!\\s)', ' ', filtered_text)\n",
    "        filtered_text = re.sub(r'[A-Z]|[a-z]', '', filtered_text)\n",
    "        filtered_text = re.sub(r'\\d+', 'число', filtered_text)\n",
    "        filtered_text = re.sub(r'\\s+', ' ', filtered_text).strip()\n",
    "        new_text = filtered_text\n",
    "\n",
    "        # сохраняем только уникальные токены (без придатка xml NULL)\n",
    "        unique_labels = list(unique_labels)\n",
    "        while len(unique_labels) < 2:\n",
    "            unique_labels.append(unique_labels[-1])\n",
    "        extracted_data['text'].append(new_text)\n",
    "        for idx, label in enumerate(unique_labels):\n",
    "            text_label = int(label) + 1\n",
    "            extracted_data[f'{idx}' + 'class'].append(text_label)\n",
    "\n",
    "    extracted_data = pd.DataFrame(extracted_data)\n",
    "    \n",
    "    # возвращаем dataframe\n",
    "    return extracted_data\n",
    "\n",
    "extracted_val = extract_text_features(data_test)\n",
    "extracted_train = extract_text_features(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# пример твита из датасета\n",
    "extracted_val.iloc[3308].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# анализ распределения таргетов на твитах\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 5))\n",
    "plt.subplots_adjust(hspace=0.3, wspace=0.5)\n",
    "fontsize=15\n",
    "\n",
    "sns.countplot(data=extracted_train, x='0class', ax=axes[0])\n",
    "axes[0].set_xlabel('class 0', fontsize=fontsize)\n",
    "axes[0].set_ylabel('count', fontsize=fontsize)\n",
    "axes[0].set_xticks([0, 1, 2], ['Neg', 'Neu', 'Pos'], fontsize=fontsize)\n",
    "axes[0].grid(True)\n",
    "\n",
    "sns.countplot(data=extracted_train, x='1class', ax=axes[1])\n",
    "axes[1].set_xlabel('class 1', fontsize=fontsize)\n",
    "axes[1].set_ylabel('count', fontsize=fontsize)\n",
    "axes[1].set_xticks([0, 1, 2], ['Neg', 'Neu', 'Pos'], fontsize=fontsize)\n",
    "axes[1].grid(True)\n",
    "\n",
    "fig.suptitle('target distribution', fontsize=fontsize)\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Инициализируем модель (fine-tune) для решения нашей задачи классификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_model_name = \"DeepPavlov/distilrubert-base-cased-conversational\"\n",
    "\n",
    "class BERTmy(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self, model_name: str, n_classes: int, \n",
    "        use_tok_type_ids: bool, p: float=0.05\n",
    "    ) -> None:\n",
    "        super(BERTmy, self).__init__()\n",
    "        self.rubert = transformers.AutoModel.from_pretrained(\n",
    "            model_name\n",
    "        )\n",
    "        self.tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "            model_name, \n",
    "            do_lower_case=True,\n",
    "            add_additional_tokens=True\n",
    "        )\n",
    "        self.use_tok_type_ids = use_tok_type_ids\n",
    "        \n",
    "        hidden_size_output = self.rubert.config.hidden_size\n",
    "        self.pre_classifier = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_size_output, hidden_size_output, bias=True),\n",
    "            torch.nn.Dropout(p),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_size_output, hidden_size_output, bias=True),\n",
    "            torch.nn.Dropout(p),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_size_output, n_classes),\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self, input_ids: torch.Tensor, attention_mask: torch.Tensor, \n",
    "        token_type_ids: torch.Tensor=None, output_attentions: bool=False,\n",
    "        output_hidden_states: bool=False, return_dict: bool=True\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
    "        \n",
    "        input_dict = {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'return_dict': True,\n",
    "            'output_attentions': True,\n",
    "            'output_hidden_states': True\n",
    "        }\n",
    "        if self.use_tok_type_ids and not token_type_ids is None:\n",
    "            input_dict['token_type_ids'] = token_type_ids\n",
    "        \n",
    "        rubert_output = self.rubert(**input_dict)\n",
    "\n",
    "        pooled = rubert_output['last_hidden_state']\n",
    "        attentions = rubert_output['attentions']\n",
    "        hid_states = rubert_output['hidden_states']\n",
    "\n",
    "        output_pre_cls = self.pre_classifier(pooled[:, 0, :])\n",
    "        logits = self.classifier(output_pre_cls)\n",
    "\n",
    "        return {\n",
    "            'logits': logits,\n",
    "            'attentions': attentions,\n",
    "            'hidden_states': hid_states\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_hf_cls(\n",
    "    model_load: str, model_type: str, \n",
    "    load_model_weights: bool=False\n",
    ") -> torch.nn.Module:\n",
    "\n",
    "    assert model_type in ['distilbert', 'bert']\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_load, do_lower_case=True,\n",
    "        add_additional_tokens=True\n",
    "    )\n",
    "\n",
    "    if load_model_weights:\n",
    "        model = AutoModel.from_pretrained(model_load)\n",
    "        model_config = model.config\n",
    "    else:\n",
    "        model_config = AutoConfig.from_pretrained(model_load)\n",
    "\n",
    "    model_cls = AutoModelForSequenceClassification.from_config(model_config)\n",
    "    \n",
    "    if load_model_weights:\n",
    "        if model_type == 'distilbert':\n",
    "            model_cls.distilbert = model\n",
    "        elif model_type == 'bert':\n",
    "            model_cls.bert = model\n",
    "        del model\n",
    "\n",
    "    return model_cls, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distilbert_name = \"DeepPavlov/distilrubert-base-cased-conversational\"\n",
    "bert_base_name = \"DeepPavlov/rubert-base-cased\"\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "num_cls = len(pd.unique(extracted_train['0class']))\n",
    "load_tf = True\n",
    "\n",
    "if load_tf:\n",
    "    model_cls, tokenizer = load_model_hf_cls(\n",
    "        distilbert_name, model_type='distilbert', \n",
    "        load_model_weights=True\n",
    "    )\n",
    "    seq_max_len = model_cls.config.max_position_embeddings\n",
    "    hid_dim = model_cls.config.dim\n",
    "    model_cls.dropout = torch.nn.Identity()\n",
    "    model_cls.pre_classifier = torch.nn.Sequential(\n",
    "        torch.nn.Linear(hid_dim, hid_dim, bias=False),\n",
    "        torch.nn.Dropout(0.15),\n",
    "        torch.nn.ReLU()\n",
    "    )\n",
    "    model_cls.classifier = torch.nn.Sequential(\n",
    "        torch.nn.Linear(hid_dim, hid_dim, bias=False),\n",
    "        torch.nn.Dropout(0.15),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(hid_dim, num_cls, bias=False),\n",
    "    )\n",
    "else:\n",
    "    model_cls = BERTmy(model_name=distilbert_name, n_classes=num_cls, use_tok_type_ids=False)\n",
    "    tokenizer = model_cls.tokenizer\n",
    "    seq_max_len = model_cls.rubert.config.max_position_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 24\n",
    "val_batch_size = 24\n",
    "\n",
    "class SentimentDataTransformer(Dataset):\n",
    "    # инициализация датасета\n",
    "    def __init__(\n",
    "        self, texts: List[str], \n",
    "        labels: List[Tuple[int, ...]]=None\n",
    "    ) -> None:\n",
    "        \n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "\n",
    "    # для получения размера датасета\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.texts)\n",
    "\n",
    "    # для получения элемента по индексу\n",
    "    def __getitem__(\n",
    "        self, index: int\n",
    "    ) -> Tuple[Union[str, int]]:\n",
    "\n",
    "        if self.labels is None:\n",
    "            return self.texts[index]\n",
    "\n",
    "        text = self.texts[index]\n",
    "        labels = self.labels[index]\n",
    "        \n",
    "        target1, target2 = labels\n",
    "\n",
    "        return text, target1, target2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class collate_fn_transformers():\n",
    "    \n",
    "    def __init__(\n",
    "        self, tokenizer: AutoTokenizer, \n",
    "        use_labels:bool, use_tok_type_ids: bool\n",
    "    ) -> None:\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.use_tok_type_ids = use_tok_type_ids\n",
    "        self.use_labels = use_labels\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        \n",
    "        if not self.use_labels:\n",
    "\n",
    "            texts = batch\n",
    "\n",
    "            return self.tokenizer(\n",
    "                texts, #truncation=True,\n",
    "                padding=True, add_special_tokens=True,\n",
    "                return_token_type_ids=self.use_tok_type_ids,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "        \n",
    "        texts, target1, target2 = zip(*batch)\n",
    "        \n",
    "        input_ids = self.tokenizer(\n",
    "            texts, #truncation=True,\n",
    "            padding=True, add_special_tokens=True,\n",
    "            return_token_type_ids=self.use_tok_type_ids,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        target1 = torch.tensor(target1)\n",
    "        target2 = torch.tensor(target2)\n",
    "        \n",
    "        return input_ids, target1, target2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Инициализируем наши DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = SentimentDataTransformer(\n",
    "    texts=extracted_train['text'].tolist(),\n",
    "    labels=list(zip(extracted_train['0class'], extracted_train['1class']))\n",
    ")\n",
    "\n",
    "val = SentimentDataTransformer(\n",
    "    texts=extracted_val['text'].tolist(),\n",
    "    labels=list(zip(extracted_val['0class'], extracted_val['1class']))\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train, batch_size=train_batch_size, shuffle=True,\n",
    "    collate_fn=collate_fn_transformers(tokenizer=tokenizer, use_tok_type_ids=False, use_labels=True)\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val, batch_size=val_batch_size, shuffle=False,\n",
    "    collate_fn=collate_fn_transformers(tokenizer=tokenizer, use_tok_type_ids=False, use_labels=True)\n",
    ")\n",
    "loaders = {\n",
    "    'train': train_loader,\n",
    "    'val': val_loader\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Дообучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    epochs: int, model: torch.nn.Module, loaders: Dict[str, DataLoader], \n",
    "    optimizer: torch.optim, scheduler: torch.optim.lr_scheduler, \n",
    "    weights_vector: torch.tensor=None, device: str='cpu'\n",
    ") -> None:\n",
    "    # cross entropy loss\n",
    "    model = model.to(device)\n",
    "    if weights_vector is None:\n",
    "        weights_vector = torch.ones(size=(num_cls,), device=device)\n",
    "    loss_function1 = torch.nn.CrossEntropyLoss(reduction='mean', weight=weights_vector)\n",
    "    loss_function2 = torch.nn.CrossEntropyLoss(reduction='mean', weight=weights_vector)\n",
    "    \n",
    "    # извлечение DataLoaders\n",
    "    if len(loaders) > 1:\n",
    "        train_loader = loaders['train']\n",
    "        val_loader = loaders['val']\n",
    "        steps_per_epoch = [('train', train_loader), ('val', val_loader)]\n",
    "    else:\n",
    "        train_loader = loaders['train']\n",
    "        steps_per_epoch = [('train', train_loader)]\n",
    "\n",
    "    # обучение по эпохам\n",
    "    for epoch in range(epochs):\n",
    "        for mode, loader in steps_per_epoch:\n",
    "            # сохранение статистик\n",
    "            train_loss = 0\n",
    "            n_correct = 0\n",
    "            processed_data = 0\n",
    "            \n",
    "            # train/val \n",
    "            if mode == 'train':\n",
    "                model.train()\n",
    "                requires_grad_mode = True\n",
    "            else:\n",
    "                model.eval()\n",
    "                requires_grad_mode = False\n",
    "            \n",
    "            # проход по батчам\n",
    "            for inputs, trg1, trg2 in tqdm(loader):\n",
    "                # обнуляем градиенты\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # извлечение входных данных для модели\n",
    "                for key, value in inputs.items():\n",
    "                    inputs[key] = value.to(device)\n",
    "                trg1, trg2 = trg1.to(device), trg2.to(device)\n",
    "                inputs['return_dict'] = True\n",
    "                \n",
    "                # устанавливаем необходимость вычислять/не_вычислять градиенты\n",
    "                with torch.set_grad_enabled(requires_grad_mode):\n",
    "                    outputs = model(**inputs)\n",
    "                    preds = torch.argmax(outputs['logits'], dim=1)\n",
    "\n",
    "                    # настраиваем модели на конкретный target\n",
    "                    if all(trg1 == trg2):\n",
    "                        loss1 = loss_function1(outputs['logits'], trg1)\n",
    "                        train_loss += loss1.item()\n",
    "                        n_correct += torch.sum(preds == trg1).cpu().detach().numpy()\n",
    "                        if mode == 'train':\n",
    "                            # вычисляем градиенты и обновляем веса\n",
    "                            loss1.backward()\n",
    "                            optimizer.step()\n",
    "                    # если у твита более чем 1 метка, то настраиваем на обе\n",
    "                    else:\n",
    "                        loss1 = loss_function1(outputs['logits'], trg1) * 0.5\n",
    "                        loss2 = loss_function2(outputs['logits'], trg2) * 0.5\n",
    "                        loss_all = loss1 + loss2\n",
    "                        train_loss += loss_all.item()\n",
    "\n",
    "                        mask_singular = trg1 == trg2\n",
    "                        mask_multiple = trg1 != trg2\n",
    "                        singular = preds[mask_singular]\n",
    "                        n_correct += torch.sum(\n",
    "                            singular == trg1[mask_singular]\n",
    "                        ).cpu().detach().numpy()\n",
    "                        multiple = preds[mask_multiple]\n",
    "                        n_correct += torch.sum(\n",
    "                            (multiple == trg1[mask_multiple]) | (multiple == trg2[mask_multiple])\n",
    "                        ).cpu().detach().numpy()\n",
    "                        if mode == 'train':\n",
    "                            # вычисляем градиенты и обновляем веса\n",
    "                            loss_all.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                    processed_data += len(preds)\n",
    "\n",
    "            # вычисляем ошибку и точность прогноза на эпохе\n",
    "            loader_loss = train_loss / processed_data\n",
    "            loader_acc = n_correct / processed_data\n",
    "            print(f'{epoch + 1} epoch with {mode} mode has: {loader_loss} loss, {loader_acc} acc')\n",
    "        \n",
    "        # делаем шаг для sheduler оптимайзера\n",
    "        scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weigths for classes\n",
    "weights_vector = torch.zeros(size=(num_cls,), device=device)\n",
    "unique_labels, counts = np.unique(\n",
    "    extracted_train['0class'], return_counts=True\n",
    ")\n",
    "sm_count = np.sum(counts)\n",
    "for label, count in zip(unique_labels, counts):\n",
    "    weights_vector[label] = 1 - count/sm_count\n",
    "\n",
    "# train model\n",
    "epochs = 15\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model_cls.pre_classifier.parameters(), 'lr': 8e-4},\n",
    "    {'params': model_cls.classifier.parameters(), 'lr': 8e-4}\n",
    "], lr=1e-6)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "train_model(epochs, model_cls, loaders, optimizer, scheduler, weights_vector, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'distilbert_cls.pth'\n",
    "\n",
    "mode_process = input('Load weights? (y/n)')\n",
    "if mode_process == 'n':\n",
    "    torch.save(model_cls.state_dict(), model_name)\n",
    "elif mode_process == 'y':\n",
    "    model_cls.load_state_dict(torch.load('/kaggle/input/distilbert-w3/distilbert_cls.pth'))\n",
    "else:\n",
    "    assert mode_process in ['n', 'y']\n",
    "model_cls.eval()\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вычисление итоговых показателей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(\n",
    "    model: torch.nn.Module, loader: DataLoader,\n",
    "    device: str='cpu'\n",
    ") -> float:\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    n_correct = 0\n",
    "    processed_data = 0\n",
    "    \n",
    "    # проход по батчам\n",
    "    for inputs, trg1, trg2 in tqdm(loader):\n",
    "\n",
    "        # извлечение входных данных для модели\n",
    "        for key, value in inputs.items():\n",
    "            inputs[key] = value.to(device)\n",
    "        trg1, trg2 = trg1.to(device), trg2.to(device)\n",
    "        inputs['return_dict'] = True\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            preds = torch.argmax(outputs['logits'], dim=1)\n",
    "            mask_singular = trg1 == trg2\n",
    "            mask_multiple = trg1 != trg2\n",
    "            singular = preds[mask_singular]\n",
    "            n_correct += torch.sum(\n",
    "                singular == trg1[mask_singular]\n",
    "            ).cpu().detach().numpy()\n",
    "            multiple = preds[mask_multiple]\n",
    "            if len(multiple) > 0:\n",
    "                n_correct += torch.sum(\n",
    "                    (multiple == trg1[mask_multiple]) | (multiple == trg2[mask_multiple])\n",
    "                ).cpu().detach().numpy()\n",
    "\n",
    "            processed_data += len(preds)\n",
    "        \n",
    "    loader_acc = n_correct / processed_data\n",
    "    \n",
    "    return loader_acc\n",
    "\n",
    "def calculate_f1_class(\n",
    "    model: torch.nn.Module, loader: DataLoader,\n",
    "    class_num: int, device: str='cpu'\n",
    ") -> float:\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    all_preds = list()\n",
    "    groud_truth = list()\n",
    "    \n",
    "    # проход по батчам\n",
    "    for inputs, trg1, trg2 in tqdm(loader):\n",
    "\n",
    "        # извлечение входных данных для модели\n",
    "        for key, value in inputs.items():\n",
    "            inputs[key] = value.to(device)\n",
    "        inputs['return_dict'] = True\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "            preds = torch.argmax(\n",
    "                outputs['logits'], dim=1\n",
    "            ).cpu().numpy()\n",
    "            all_preds.append(preds)\n",
    "            groud_truth.append(trg1.cpu().detach().numpy())\n",
    "\n",
    "    all_preds = np.hstack(all_preds)\n",
    "    groud_truth = np.hstack(groud_truth)\n",
    "    mask = all_preds == class_num\n",
    "    all_preds[mask] = 1\n",
    "    all_preds[~mask] = 0\n",
    "    mask = groud_truth == class_num\n",
    "    groud_truth[mask] = 1\n",
    "    groud_truth[~mask] = 0\n",
    "    \n",
    "    return f1_score(groud_truth, all_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = calculate_accuracy(model_cls, val_loader, device)\n",
    "class_neg_f1 = calculate_f1_class(model_cls, val_loader, 0, device)\n",
    "class_neu_f1 = calculate_f1_class(model_cls, val_loader, 1, device)\n",
    "class_pos_f1 = calculate_f1_class(model_cls, val_loader, 2, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# общая accuracy и f1 по классам\n",
    "test_acc, class_neg_f1, class_neu_f1, class_pos_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backdoor attacks on neural network(adversial examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USE metric for similarity between original sentence and spoiled sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_score(original, adversial, use_bert_encoder=False, model=None):\n",
    "    from scipy.spatial.distance import cosine\n",
    "    # Load pre-trained universal sentence encoder model\n",
    "    if not use_bert_encoder:\n",
    "        # using DAN from tensorflow\n",
    "        use_encoder = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "\n",
    "        sentences_orig = list()\n",
    "        sentences_adv = list()\n",
    "        for pair in zip(original, adversial):\n",
    "            orig, adv = pair\n",
    "            sentences_orig.append(orig)\n",
    "            sentences_adv.append(adv)\n",
    "\n",
    "        # get embs of texts\n",
    "        sentences_orig_emb = use_encoder(sentences_orig)\n",
    "        sentences_adv_emb = use_encoder(sentences_adv)\n",
    "\n",
    "        # calculate use_score with DAN\n",
    "        use_scores = list()\n",
    "        for pair in zip(sentences_orig_emb, sentences_adv_emb):\n",
    "            orig_emb, adv_emb = pair[0], pair[1]\n",
    "            use_score_one = 1 - cosine(orig_emb, adv_emb)\n",
    "            use_scores.append(use_score_one)\n",
    "    else:\n",
    "        # using BERT itself\n",
    "        def get_inputs(text): # get inputs for model\n",
    "            inputs = model.tokenizer(\n",
    "                text, padding=True, \n",
    "                add_special_tokens=True, \n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            ids = inputs['input_ids'].type(torch.long).to(device)\n",
    "            mask = inputs['attention_mask'].type(torch.long).to(device)\n",
    "            token_type_ids = inputs[\"token_type_ids\"].type(torch.long).to(device)\n",
    "            \n",
    "            return ids, mask, token_type_ids\n",
    "\n",
    "        # calculate use_score with BERT\n",
    "        use_scores = list()\n",
    "        for pair in zip(original, adversial):\n",
    "            orig, adv = pair[0], pair[1]\n",
    "            orig_inputs = get_inputs(orig)\n",
    "            adv_inputs = get_inputs(adv)\n",
    "            orig_outputs = model.rubert(*orig_inputs)\n",
    "            adv_outputs = model.rubert(*adv_inputs)\n",
    "            orig_pooled, adv_pooled = orig_outputs[1], adv_outputs[1]\n",
    "            orig_pooled = orig_pooled.cpu().detach().numpy()\n",
    "            adv_pooled = adv_pooled.cpu().detach().numpy()\n",
    "            use_score_one = 1 - cosine(orig_pooled, adv_pooled)\n",
    "            use_scores.append(use_score_one)\n",
    "    \n",
    "    return use_scores, np.mean(use_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention_one_head(tokens, attention_weights, num_layer, num_head):\n",
    "    # works only with batch_size=1\n",
    "    num_layer -= 1\n",
    "    num_head -= 1\n",
    "    assert num_head >= 0 and num_head < len(attention_weights[0][0])\n",
    "    assert num_layer < len(attention_weights) and num_layer >= 0\n",
    "    \n",
    "    attention_layer = attention_weights[num_layer][0].cpu().detach().numpy()\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(9, 6))\n",
    "\n",
    "    g = sns.heatmap(attention_layer[num_head], annot=True, linewidth=0.1, fmt='.1g')\n",
    "    # xlabel='weight_for_embed', ylabel='num_embed'\n",
    "    g.set(title=f'layer: {num_layer + 1}; head: {num_head + 1} attention map')\n",
    "    tickvalues = range(0,len(tokens) + 2)\n",
    "    tokens = ['CLS'] + tokens + ['SEP']\n",
    "    g.set_yticks(ticks=tickvalues ,labels=tokens, rotation='horizontal')\n",
    "    g.set_xticks(ticks=tickvalues ,labels=tokens, rotation='vertical')\n",
    "    ax = g\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adversial_examples_char = pd.read_csv('/kaggle/input/result-data/adversial_examples_char.csv')\n",
    "text_example = extracted_val.iloc[10].text\n",
    "text_example_ins = adversial_examples_char['1_ins_amount_1_SpoiledText'].iloc[10]\n",
    "text_example_del = adversial_examples_char['1_del_amount_1_SpoiledText'].iloc[10]\n",
    "text_example_sub = adversial_examples_char['1_sub_amount_1_SpoiledText'].iloc[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention_for_text(text, num_layer, num_head):\n",
    "    text_seq = bert.tokenizer(\n",
    "        text,\n",
    "        padding=True,\n",
    "        add_special_tokens=True,\n",
    "        return_tensors='pt'\n",
    "    ).to(device)\n",
    "    logits, attention = bert(**text_seq, output_attentions=True)\n",
    "    tokens = bert.tokenizer.tokenize(text)\n",
    "    visualize_attention_one_head(tokens, attention, num_layer, num_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_attention_for_text(text_example_sub, 12, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data for interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# выбираем текст для генерации состязательных примеров с сохранением исходной пропорции\n",
    "limit_neu = 1300\n",
    "limit_pos = 270\n",
    "limit_neg = 550\n",
    "adversial_examples_pos = extracted_val[extracted_val['0class'] == 2]\n",
    "adversial_examples_neu = extracted_val[extracted_val['0class'] == 1]\n",
    "adversial_examples_neg = extracted_val[extracted_val['0class'] == 0]\n",
    "\n",
    "adversial_examples_pos = adversial_examples_pos.head(limit_pos)\n",
    "adversial_examples_neu = adversial_examples_neu.head(limit_neu)\n",
    "adversial_examples_neg = adversial_examples_neg.head(limit_neg)\n",
    "\n",
    "adversial_examples = pd.concat([adversial_examples_pos, adversial_examples_neu, adversial_examples_neg])\n",
    "adversial_examples_char = adversial_examples.sample(frac=1)\n",
    "\n",
    "print('Размер текста для генерации: ', len(adversial_examples_char))\n",
    "print('Баланс классов: ')\n",
    "print(np.unique(adversial_examples_char['0class'], return_counts=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_back_tokens(tokens: List[str], tokens_type: str) -> str:\n",
    "    \"\"\"\n",
    "    для превращения токенов в предложение\n",
    "    tokens: список токенов\n",
    "    tokens_type: natasha или razdel\n",
    "    \"\"\"\n",
    "    assert tokens_type in ['razdel', 'natasha']\n",
    "\n",
    "    sent = ''\n",
    "    prev_end = None\n",
    "    for token in tokens:\n",
    "\n",
    "        if tokens_type == 'natasha':\n",
    "            token_text = token['text']\n",
    "            token_start, token_stop = token['start'], token['stop']\n",
    "        else:\n",
    "            token_text = token.text\n",
    "            token_start, token_stop = token.start, token.stop\n",
    "        \n",
    "        if not prev_end is None:\n",
    "            sent += (token_start - prev_end) * ' '\n",
    "\n",
    "        sent += token_text\n",
    "        prev_end = token_stop\n",
    " \n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cls.eval()\n",
    "model_cls = model_cls.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_texts(texts: List[str], func_type: str):\n",
    "    \"\"\"\n",
    "    for Lime: return probability distribution of text\n",
    "    \"\"\"\n",
    "    assert func_type in ['shap', 'lime']\n",
    "    \n",
    "    if func_type == 'shap':\n",
    "        texts = list(map(lambda x: re.sub(r'\\.{3}', '[MASK]', x), texts))\n",
    "\n",
    "    # get model outputs\n",
    "    dataset = SentimentDataTransformer(texts=texts)\n",
    "    dataloader = DataLoader(\n",
    "        dataset, batch_size=30, shuffle=False,\n",
    "        collate_fn=collate_fn_transformers(\n",
    "            tokenizer=tokenizer, use_labels=False,\n",
    "            use_tok_type_ids=False\n",
    "        )\n",
    "    )\n",
    "    all_probs = list()\n",
    "\n",
    "    for batch in dataloader:\n",
    "        for key, value in batch.items():\n",
    "            batch[key] = value.to(device)\n",
    "        batch['return_dict'] = True\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model_cls(**batch)['logits']\n",
    "\n",
    "        # get probs\n",
    "        probs = torch.nn.functional.softmax(\n",
    "            logits, dim=1\n",
    "        ).cpu().detach().numpy()\n",
    "        all_probs.append(probs)\n",
    "    \n",
    "    return np.vstack(all_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lime_importance(\n",
    "    tokens: List[Tuple[str, int, int]], tokens_type: str, \n",
    "    num_features:int=300, num_samples:int=700, device: str='cpu'\n",
    ") -> List[str]:\n",
    "\n",
    "    assert tokens_type in ['razdel', 'natasha']\n",
    "\n",
    "    # список для наиболее важных слов\n",
    "    essential_words = list()\n",
    "    \n",
    "    def RazdelSplit(text):\n",
    "\n",
    "        return [raz_tok.text for raz_tok in list(tokenize(text))]\n",
    "\n",
    "    def NatashaSplit(text):\n",
    "\n",
    "        segmenter = Segmenter()\n",
    "        text_doc = Doc(text.lower())\n",
    "        text_doc.segment(segmenter)\n",
    "\n",
    "        return [nat_tok['text'] for nat_tok in text_doc]\n",
    "\n",
    "    text_to_explain = gather_back_tokens(tokens, tokens_type)\n",
    "\n",
    "    if tokens_type == 'razdel':\n",
    "        Spliter = RazdelSplit\n",
    "    elif tokens_type == 'natasha':\n",
    "        Spliter = NatashaSplit\n",
    "    # создаем Explainer\n",
    "    explainer = LimeTextExplainer(\n",
    "        class_names=['Neg', 'Neu', 'Pos'],\n",
    "        split_expression=Spliter\n",
    "    )\n",
    "\n",
    "    # \"объясняем\" текст\n",
    "    explanation = explainer.explain_instance(\n",
    "        text_to_explain, partial(predict_texts,func_type='lime'), \n",
    "        num_features=num_features, num_samples=num_samples\n",
    "    )\n",
    "\n",
    "    # создаем mapping из токена в его вес LogReg\n",
    "    explanation_list = explanation.as_list()\n",
    "    tok2weight = {token:weight for token, weight in explanation_list}\n",
    "\n",
    "    # создаем список из токенов, их важности и позиции в тексте\n",
    "    for token in tokens:\n",
    "        if tokens_type == 'razdel':\n",
    "            token_text = token.text.lower()\n",
    "        else:\n",
    "            token_text = token['text'].lower()\n",
    "\n",
    "        essential_words.append((\n",
    "            token, tok2weight[token_text]\n",
    "        ))\n",
    "\n",
    "    # создаем функцию сравнения важности\n",
    "    sort_func = lambda x: np.abs(x[1])\n",
    "    \n",
    "    # сортируем токены по важности\n",
    "    essential_words = sorted(essential_words, key=sort_func, reverse=True)\n",
    "    print(essential_words)\n",
    "\n",
    "    # возвращаем только слова и их позиции в тексте\n",
    "    essential_words = [word for word, _ in essential_words]\n",
    "\n",
    "    return essential_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shap_importance(\n",
    "    tokens: List[str], tokens_type: str,\n",
    "    target: int\n",
    ") -> List[str]:\n",
    "\n",
    "    assert tokens_type in ['razdel', 'natasha']\n",
    "\n",
    "    # восстанавливаем текст из слов\n",
    "    text_to_explain = gather_back_tokens(tokens,tokens_type)\n",
    "\n",
    "    def custom_tokenizer(\n",
    "        text: str, return_offsets_mapping=True\n",
    "    ) -> Dict[str, List[Union[str, Tuple[int, ...]]]]:\n",
    "        \"\"\"Custom tokenizers conform to a subset of the transformers API.\"\"\"\n",
    "        tokens = list(razdel.tokenize(text))\n",
    "        \n",
    "        words = list()\n",
    "        offsets = list()\n",
    "        for token in tokens:\n",
    "            words.append(token.text)\n",
    "            offsets.append((token.start, token.stop))\n",
    "\n",
    "        return {\n",
    "            'input_ids': words,\n",
    "            'offset_mapping': offsets\n",
    "        }\n",
    "\n",
    "    masker = shap.maskers.Text(custom_tokenizer)\n",
    "    explainer = shap.Explainer(\n",
    "        partial(predict_texts,func_type='shap'), masker, \n",
    "        output_names=['Neg', 'Neu', 'Pos']\n",
    "    )\n",
    "    # get shap values for the onliest text\n",
    "    shap_values = explainer([text_to_explain])\n",
    "\n",
    "    tokens_order = shap_values.data[0]\n",
    "    base_values = shap_values.base_values\n",
    "    contributions = np.abs(shap_values.values[0]).sum(axis=1)\n",
    "    essential_words = list(zip(tokens, contributions))\n",
    "    \n",
    "    sort_func = lambda x: x[1]\n",
    "    \n",
    "    essential_words = sorted(\n",
    "        essential_words, key=sort_func, \n",
    "        reverse=True\n",
    "    )\n",
    "    \n",
    "    essential_words = [word for word, _ in essential_words]\n",
    "    \n",
    "    return essential_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### alti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cls_wrapper = ModelWrapper(model_cls)\n",
    "\n",
    "def alti_importance(\n",
    "    tokens: List[Tuple[str, int, int]], tokens_type: str,\n",
    "    measure_tokens_contributions: str\n",
    ") -> List[str]:\n",
    "\n",
    "    assert measure_tokens_contributions in ['cls', 'all_tokens']\n",
    "    assert tokens_type in ['razdel', 'natasha']\n",
    "    \n",
    "    text_to_explain = gather_back_tokens(tokens, tokens_type)\n",
    "        \n",
    "    text_tokens = tokenizer.tokenize(text_to_explain)\n",
    "    text_input = tokenizer(\n",
    "        text_to_explain, return_tensors=\"pt\", \n",
    "        return_token_type_ids=False,\n",
    "        return_offsets_mapping=True\n",
    "    ).to(device)\n",
    "    offset_mapping = text_input['offset_mapping'][0,1:-1,:].cpu().detach()\n",
    "    text_input['return_dict'] = True\n",
    "    del text_input['offset_mapping']\n",
    "\n",
    "    # get words of text\n",
    "    words = list(razdel.tokenize(text_to_explain))\n",
    "    pos_words = [(word.start, word.stop) for word in words]\n",
    "\n",
    "    # create mapping from token to word\n",
    "    cur_index = 0\n",
    "    token_pos_to_word = dict()\n",
    "    for idx, (offset, token) in enumerate(zip(offset_mapping, text_tokens)):\n",
    "        start, _ = offset\n",
    "        while start >= pos_words[cur_index][1]:\n",
    "            cur_index += 1\n",
    "        token_pos_to_word[idx] = words[cur_index]\n",
    "\n",
    "    _, _, _, contributions_data = model_cls_wrapper(text_input)\n",
    "\n",
    "    # get Yi from alti\n",
    "    resultant_norm = torch.norm(\n",
    "        torch.squeeze(contributions_data['resultants']),\n",
    "        p=1, dim=-1\n",
    "    )\n",
    "    # get Cij from alti method\n",
    "    # 'contributions' means Tij\n",
    "    # alti requires scaling = min_sum\n",
    "    normalized_contributions = normalize_contributions(\n",
    "        contributions_data['contributions'], scaling='min_sum',\n",
    "        resultant_norm=resultant_norm\n",
    "    )\n",
    "\n",
    "    # apply attention rollout and get seq of Ci\n",
    "    contributions_mix = compute_joint_attention(normalized_contributions)\n",
    "    # extract Ci after last self-attention layer\n",
    "    joint_attention_layer = -1\n",
    "    contributions_mix_last_hid = contributions_mix[joint_attention_layer]\n",
    "\n",
    "    if measure_tokens_contributions == 'cls':\n",
    "        # contribution to token cls\n",
    "        positions=np.array([0])\n",
    "    else:\n",
    "        positions=np.arange(len(contributions_mix_cls) - 2) + 1\n",
    "\n",
    "    word_to_contribution = defaultdict(float)\n",
    "    for pos in positions:\n",
    "        # get tokens contrubitons\n",
    "        contributions_mix_cur = contributions_mix_last_hid[pos][1:-1]\n",
    "        for idx, contribution in enumerate(contributions_mix_cur):\n",
    "            word_to_contribution[token_pos_to_word[idx]] += contribution\n",
    "    \n",
    "    # функция для сортировки\n",
    "    sort_func = lambda x: x[1]\n",
    "    \n",
    "    essential_words = sorted(\n",
    "        [(word, cont) for word, cont in word_to_contribution.items()],\n",
    "        key=sort_func, reverse=True\n",
    "    )\n",
    "    \n",
    "    essential_words = [word for word, _ in essential_words]\n",
    "    \n",
    "    return essential_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heuristic loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_importance(\n",
    "    tokens: List[Tuple[str, int, int]], \n",
    "    target: Union[int, str], tokens_type: str\n",
    ") -> List[str]:\n",
    "    \n",
    "    assert tokens_type in ['razdel', 'natasha']\n",
    "    \n",
    "    text_to_explain = gather_back_tokens(tokens, tokens_type)\n",
    "    \n",
    "    # список для наиболее важных слов\n",
    "    essential_words = list()\n",
    "\n",
    "    loss = torch.nn.CrossEntropyLoss()\n",
    "    get_inputs = lambda x: tokenizer(\n",
    "        x, padding=True,\n",
    "        add_special_tokens=True,\n",
    "        return_token_type_ids=False,\n",
    "        return_tensors='pt'\n",
    "    ).to(device)\n",
    "\n",
    "    # get inputs and outputs from model\n",
    "    inputs = get_inputs(text_to_explain)\n",
    "    inputs['return_dict'] = True\n",
    "\n",
    "    outputs = model_cls(**inputs)['logits']\n",
    "    target_pt = torch.tensor([target], dtype=torch.long)\n",
    "\n",
    "    # calculate loss for original text\n",
    "    loss_score_integral = loss(\n",
    "        outputs.cpu(), target_pt\n",
    "    )\n",
    "\n",
    "    for idx, token in enumerate(tokens):\n",
    "        # get text without one token\n",
    "        tokens_copy = tokens.copy()\n",
    "        tokens_copy.pop(idx)\n",
    "        text_to_explain = gather_back_tokens(tokens_copy, tokens_type)\n",
    "\n",
    "        # calculate loss without current word\n",
    "        inputs = get_inputs(text_to_explain)\n",
    "        inputs['return_dict'] = True\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model_cls(**inputs)['logits']\n",
    "        loss_score_part = loss(outputs.cpu(), target_pt)\n",
    "        # add our score of change\n",
    "        essential_words.append((\n",
    "            token, (loss_score_part-loss_score_integral).cpu().detach().numpy()\n",
    "        ))\n",
    "\n",
    "    # создаем функцию сравнения важности\n",
    "    sort_func = lambda x: x[1]\n",
    "\n",
    "    # сортируем токены по важности\n",
    "    essential_words = sorted(essential_words, key=sort_func, reverse=True)\n",
    "\n",
    "    # возвращаем только слова и их позиции в тексте\n",
    "    essential_words = [word for word, _ in essential_words]\n",
    "\n",
    "    return essential_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass\n",
    "# TO DO:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### random important words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_random_words(\n",
    "    tokens: List[str]\n",
    ") -> List[Tuple[str, int]]:\n",
    "    \"\"\"\n",
    "    возвращает список слов в случайном порядке\n",
    "    \"\"\"\n",
    "    permutation = np.random.permutation(len(tokens))\n",
    "\n",
    "    return [tokens[idx] for idx in permutation]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
