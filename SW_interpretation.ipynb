{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Установка и импорт всех необходимых зависимостей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install -q razdel\n",
    "!pip install -q pymorphy2\n",
    "!pip install -q git+https://github.com/ahmados/rusynonyms.git\n",
    "!pip install -q natasha\n",
    "!pip install -q pyaml-env\n",
    "!pip install -q captum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "path_to_alti = '/kaggle/input/transformer-contributions1/transformer-contributions'\n",
    "if not path_to_alti in sys.path:\n",
    "    sys.path.append(path_to_alti)\n",
    "\n",
    "from src.utils_contributions import *\n",
    "from src.contributions import ModelWrapper, ClassificationModelWrapperCaptum, interpret_sentence, occlusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import pymorphy2\n",
    "from razdel import tokenize\n",
    "from razdel import sentenize\n",
    "import string\n",
    "from natasha import (\n",
    "    MorphVocab,\n",
    "    NewsMorphTagger,\n",
    "    NewsEmbedding,\n",
    "    Segmenter,\n",
    "    NewsSyntaxParser,\n",
    "    Doc\n",
    ")\n",
    "\n",
    "import torch\n",
    "import tensorflow_hub as hub\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModel\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from tqdm import tqdm\n",
    "from typing import *\n",
    "\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "import shap\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "rus_stopwords = stopwords.words('russian')\n",
    "punctuation = list(string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Работа с данными (kaggle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_folder = '/kaggle/input/sw-datasets/Russian-Sentiment-Analysis-Evaluation-Datasets'\n",
    "datasets = ['SentiRuEval-2015-telecoms', 'SentiRuEval-2015-banks', 'SentiRuEval-2016-banks', 'SentiRuEval-2016-telecoms']\n",
    "samples = ['test.xml', 'train.xml', 'test_etalon.xml']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    функция для извлечения данных из xml\n",
    "    \"\"\"\n",
    "    tree = ET.parse(path)\n",
    "    root = tree.getroot()\n",
    "    DataFrame = dict()\n",
    "    database = root.findall('database')[0]\n",
    "    DataFrame_columns = list()\n",
    "\n",
    "    for idx, table in enumerate(database.findall('table')):\n",
    "        for column in table.findall('column'):\n",
    "            DataFrame[column.attrib['name']] = list()\n",
    "            DataFrame_columns.append(column.attrib['name'])\n",
    "        if idx == 0:\n",
    "            break\n",
    "\n",
    "    for table in database.findall('table'):\n",
    "        for column in table.findall('column'):\n",
    "            DataFrame[column.attrib['name']].append(column.text)\n",
    "\n",
    "    data = pd.DataFrame(DataFrame, columns=DataFrame_columns)\n",
    "    return data\n",
    "\n",
    "# инициализация всех путей (kaggle)\n",
    "banks_dataset = datasets[2]\n",
    "path2samples = os.path.join(datasets_folder, banks_dataset)\n",
    "banks = ['sberbank', 'vtb', 'gazprom', 'alfabank', 'bankmoskvy', 'raiffeisen', 'uralsib', 'rshb']\n",
    "\n",
    "path2test = os.path.join(path2samples, samples[2])\n",
    "data_test = extract_data(path2test)\n",
    "\n",
    "path2train = os.path.join(path2samples, samples[1])\n",
    "data_train = extract_data(path2train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_features(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    функция для первичной обработки текста от лишних символов\n",
    "    \"\"\"\n",
    "    extracted_data = dict()\n",
    "    extracted_data['text'] = list()\n",
    "    extracted_data['0class'] = list()\n",
    "    extracted_data['1class'] = list()\n",
    "\n",
    "    for idx in range(len(data)):\n",
    "        row = data.iloc[idx, :]\n",
    "        banks_review = row[banks]\n",
    "        unique_labels = set(banks_review)\n",
    "        unique_labels.remove('NULL')\n",
    "\n",
    "        # убираем все ненужные знаки\n",
    "        filtered_text = re.sub('http[A-z|:|.|/|0-9]*', '', row['text'])\n",
    "        filtered_text = re.sub('@\\S*', '', filtered_text)\n",
    "        filtered_text = re.sub('#|:|»|«|-|xD|;D|\\\"|_|/', '', filtered_text)\n",
    "        filtered_text = re.sub(r'\\.(?=\\s)|,|(?<!\\s)\\.(?!\\s)', ' ', filtered_text)\n",
    "        filtered_text = re.sub(r'[A-Z]|[a-z]', '', filtered_text)\n",
    "        filtered_text = re.sub(r'\\d+', 'число', filtered_text)\n",
    "        filtered_text = re.sub(r'\\s+', ' ', filtered_text).strip()\n",
    "        new_text = filtered_text\n",
    "\n",
    "        # сохраняем только уникальные токены (без придатка xml NULL)\n",
    "        unique_labels = list(unique_labels)\n",
    "        while len(unique_labels) < 2:\n",
    "            unique_labels.append(unique_labels[-1])\n",
    "        extracted_data['text'].append(new_text)\n",
    "        for idx, label in enumerate(unique_labels):\n",
    "            text_label = int(label) + 1\n",
    "            extracted_data[f'{idx}' + 'class'].append(text_label)\n",
    "\n",
    "    extracted_data = pd.DataFrame(extracted_data)\n",
    "    \n",
    "    # возвращаем dataframe\n",
    "    return extracted_data\n",
    "\n",
    "extracted_val = extract_text_features(data_test)\n",
    "extracted_train = extract_text_features(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# пример твита из датасета\n",
    "extracted_val.iloc[3308].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# анализ распределения таргетов на твитах\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 5))\n",
    "plt.subplots_adjust(hspace=0.3, wspace=0.5)\n",
    "fontsize=15\n",
    "\n",
    "sns.countplot(data=extracted_train, x='0class', ax=axes[0])\n",
    "axes[0].set_xlabel('class 0', fontsize=fontsize)\n",
    "axes[0].set_ylabel('count', fontsize=fontsize)\n",
    "axes[0].set_xticks([0, 1, 2], ['Neg', 'Neu', 'Pos'], fontsize=fontsize)\n",
    "axes[0].grid(True)\n",
    "\n",
    "sns.countplot(data=extracted_train, x='1class', ax=axes[1])\n",
    "axes[1].set_xlabel('class 1', fontsize=fontsize)\n",
    "axes[1].set_ylabel('count', fontsize=fontsize)\n",
    "axes[1].set_xticks([0, 1, 2], ['Neg', 'Neu', 'Pos'], fontsize=fontsize)\n",
    "axes[1].grid(True)\n",
    "\n",
    "fig.suptitle('target distribution', fontsize=fontsize)\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Инициализируем модель (fine-tune) для решения нашей задачи классификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_model_name = \"DeepPavlov/distilrubert-base-cased-conversational\"\n",
    "\n",
    "class BERTmy(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self, model_name: str, n_classes: int, \n",
    "        use_tok_type_ids: bool, p: float=0.05\n",
    "    ) -> None:\n",
    "        super(BERTmy, self).__init__()\n",
    "        self.rubert = transformers.AutoModel.from_pretrained(\n",
    "            model_name\n",
    "        )\n",
    "        self.tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "            model_name, \n",
    "            do_lower_case=True,\n",
    "            add_additional_tokens=True\n",
    "        )\n",
    "        self.use_tok_type_ids = use_tok_type_ids\n",
    "        \n",
    "        hidden_size_output = self.rubert.config.hidden_size\n",
    "        self.pre_classifier = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_size_output, hidden_size_output, bias=True),\n",
    "            torch.nn.Dropout(p),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_size_output, hidden_size_output, bias=True),\n",
    "            torch.nn.Dropout(p),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_size_output, n_classes),\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self, input_ids: torch.Tensor, attention_mask: torch.Tensor, \n",
    "        token_type_ids: torch.Tensor=None, output_attentions: bool=False,\n",
    "        output_hidden_states: bool=False, return_dict: bool=True\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
    "        \n",
    "        input_dict = {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'return_dict': True,\n",
    "            'output_attentions': True,\n",
    "            'output_hidden_states': True\n",
    "        }\n",
    "        if self.use_tok_type_ids and not token_type_ids is None:\n",
    "            input_dict['token_type_ids'] = token_type_ids\n",
    "        \n",
    "        rubert_output = self.rubert(**input_dict)\n",
    "\n",
    "        pooled = rubert_output['last_hidden_state']\n",
    "        attentions = rubert_output['attentions']\n",
    "        hid_states = rubert_output['hidden_states']\n",
    "\n",
    "        output_pre_cls = self.pre_classifier(pooled[:, 0, :])\n",
    "        logits = self.classifier(output_pre_cls)\n",
    "\n",
    "        return {\n",
    "            'logits': logits,\n",
    "            'attentions': attentions,\n",
    "            'hidden_states': hid_states\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_hf_cls(\n",
    "    model_load: str, model_type: str, \n",
    "    load_model_weights: bool=False\n",
    ") -> torch.nn.Module:\n",
    "\n",
    "    assert model_type in ['distilbert', 'bert']\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_load, do_lower_case=True,\n",
    "        add_additional_tokens=True\n",
    "    )\n",
    "\n",
    "    if load_model_weights:\n",
    "        model = AutoModel.from_pretrained(model_load)\n",
    "        model_config = model.config\n",
    "    else:\n",
    "        model_config = AutoConfig.from_pretrained(model_load)\n",
    "\n",
    "    model_cls = AutoModelForSequenceClassification.from_config(model_config)\n",
    "    \n",
    "    if load_model_weights:\n",
    "        if model_type == 'distilbert':\n",
    "            model_cls.distilbert = model\n",
    "        elif model_type == 'bert':\n",
    "            model_cls.bert = model\n",
    "        del model\n",
    "\n",
    "    return model_cls, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distilbert_name = \"DeepPavlov/distilrubert-base-cased-conversational\"\n",
    "bert_base_name = \"DeepPavlov/rubert-base-cased\"\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "num_cls = len(pd.unique(extracted_train['0class']))\n",
    "load_tf = True\n",
    "\n",
    "if load_tf:\n",
    "    model_cls, tokenizer = load_model_hf_cls(\n",
    "        distilbert_name, model_type='distilbert', \n",
    "        load_model_weights=True\n",
    "    )\n",
    "    seq_max_len = model_cls.config.max_position_embeddings\n",
    "    hid_dim = model_cls.config.dim\n",
    "    model_cls.dropout = torch.nn.Identity()\n",
    "    model_cls.pre_classifier = torch.nn.Sequential(\n",
    "        torch.nn.Linear(hid_dim, hid_dim, bias=False),\n",
    "        torch.nn.Dropout(0.15),\n",
    "        torch.nn.ReLU()\n",
    "    )\n",
    "    model_cls.classifier = torch.nn.Sequential(\n",
    "        torch.nn.Linear(hid_dim, hid_dim, bias=False),\n",
    "        torch.nn.Dropout(0.15),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(hid_dim, num_cls, bias=False),\n",
    "    )\n",
    "else:\n",
    "    model_cls = BERTmy(model_name=distilbert_name, n_classes=num_cls, use_tok_type_ids=False)\n",
    "    tokenizer = model_cls.tokenizer\n",
    "    seq_max_len = model_cls.rubert.config.max_position_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 24\n",
    "val_batch_size = 24\n",
    "\n",
    "class SentimentDataTransformer(Dataset):\n",
    "    # инициализация датасета\n",
    "    def __init__(\n",
    "        self, texts: List[str], \n",
    "        labels: List[Tuple[int, ...]]=None\n",
    "    ) -> None:\n",
    "        \n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "\n",
    "    # для получения размера датасета\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.texts)\n",
    "\n",
    "    # для получения элемента по индексу\n",
    "    def __getitem__(\n",
    "        self, index: int\n",
    "    ) -> Tuple[Union[str, int]]:\n",
    "\n",
    "        if self.labels is None:\n",
    "            return self.texts[index]\n",
    "\n",
    "        text = self.texts[index]\n",
    "        labels = self.labels[index]\n",
    "        \n",
    "        target1, target2 = labels\n",
    "\n",
    "        return text, target1, target2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class collate_fn_transformers():\n",
    "    \n",
    "    def __init__(\n",
    "        self, tokenizer: AutoTokenizer, \n",
    "        use_labels:bool, use_tok_type_ids: bool\n",
    "    ) -> None:\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.use_tok_type_ids = use_tok_type_ids\n",
    "        self.use_labels = use_labels\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        \n",
    "        if not self.use_labels:\n",
    "\n",
    "            texts = batch\n",
    "\n",
    "            return self.tokenizer(\n",
    "                texts, #truncation=True,\n",
    "                padding=True, add_special_tokens=True,\n",
    "                return_token_type_ids=self.use_tok_type_ids,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "        \n",
    "        texts, target1, target2 = zip(*batch)\n",
    "        \n",
    "        input_ids = self.tokenizer(\n",
    "            texts, #truncation=True,\n",
    "            padding=True, add_special_tokens=True,\n",
    "            return_token_type_ids=self.use_tok_type_ids,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        target1 = torch.tensor(target1)\n",
    "        target2 = torch.tensor(target2)\n",
    "        \n",
    "        return input_ids, target1, target2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Инициализируем наши DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = SentimentDataTransformer(\n",
    "    texts=extracted_train['text'].tolist(),\n",
    "    labels=list(zip(extracted_train['0class'], extracted_train['1class']))\n",
    ")\n",
    "\n",
    "val = SentimentDataTransformer(\n",
    "    texts=extracted_val['text'].tolist(),\n",
    "    labels=list(zip(extracted_val['0class'], extracted_val['1class']))\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train, batch_size=train_batch_size, shuffle=True,\n",
    "    collate_fn=collate_fn_transformers(tokenizer=tokenizer, use_tok_type_ids=False, use_labels=True)\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val, batch_size=val_batch_size, shuffle=False,\n",
    "    collate_fn=collate_fn_transformers(tokenizer=tokenizer, use_tok_type_ids=False, use_labels=True)\n",
    ")\n",
    "loaders = {\n",
    "    'train': train_loader,\n",
    "    'val': val_loader\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Дообучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    epochs: int, model: torch.nn.Module, loaders: Dict[str, DataLoader], \n",
    "    optimizer: torch.optim, scheduler: torch.optim.lr_scheduler, \n",
    "    weights_vector: torch.tensor=None, device: str='cpu'\n",
    ") -> None:\n",
    "    # cross entropy loss\n",
    "    model = model.to(device)\n",
    "    if weights_vector is None:\n",
    "        weights_vector = torch.ones(size=(num_cls,), device=device)\n",
    "    loss_function1 = torch.nn.CrossEntropyLoss(reduction='mean', weight=weights_vector)\n",
    "    loss_function2 = torch.nn.CrossEntropyLoss(reduction='mean', weight=weights_vector)\n",
    "    \n",
    "    # извлечение DataLoaders\n",
    "    if len(loaders) > 1:\n",
    "        train_loader = loaders['train']\n",
    "        val_loader = loaders['val']\n",
    "        steps_per_epoch = [('train', train_loader), ('val', val_loader)]\n",
    "    else:\n",
    "        train_loader = loaders['train']\n",
    "        steps_per_epoch = [('train', train_loader)]\n",
    "\n",
    "    # обучение по эпохам\n",
    "    for epoch in range(epochs):\n",
    "        for mode, loader in steps_per_epoch:\n",
    "            # сохранение статистик\n",
    "            train_loss = 0\n",
    "            n_correct = 0\n",
    "            processed_data = 0\n",
    "            \n",
    "            # train/val \n",
    "            if mode == 'train':\n",
    "                model.train()\n",
    "                requires_grad_mode = True\n",
    "            else:\n",
    "                model.eval()\n",
    "                requires_grad_mode = False\n",
    "            \n",
    "            # проход по батчам\n",
    "            for inputs, trg1, trg2 in tqdm(loader):\n",
    "                # обнуляем градиенты\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # извлечение входных данных для модели\n",
    "                for key, value in inputs.items():\n",
    "                    inputs[key] = value.to(device)\n",
    "                trg1, trg2 = trg1.to(device), trg2.to(device)\n",
    "                inputs['return_dict'] = True\n",
    "                \n",
    "                # устанавливаем необходимость вычислять/не_вычислять градиенты\n",
    "                with torch.set_grad_enabled(requires_grad_mode):\n",
    "                    outputs = model(**inputs)\n",
    "                    preds = torch.argmax(outputs['logits'], dim=1)\n",
    "\n",
    "                    # настраиваем модели на конкретный target\n",
    "                    if all(trg1 == trg2):\n",
    "                        loss1 = loss_function1(outputs['logits'], trg1)\n",
    "                        train_loss += loss1.item()\n",
    "                        n_correct += torch.sum(preds == trg1).cpu().detach().numpy()\n",
    "                        if mode == 'train':\n",
    "                            # вычисляем градиенты и обновляем веса\n",
    "                            loss1.backward()\n",
    "                            optimizer.step()\n",
    "                    # если у твита более чем 1 метка, то настраиваем на обе\n",
    "                    else:\n",
    "                        loss1 = loss_function1(outputs['logits'], trg1) * 0.5\n",
    "                        loss2 = loss_function2(outputs['logits'], trg2) * 0.5\n",
    "                        loss_all = loss1 + loss2\n",
    "                        train_loss += loss_all.item()\n",
    "\n",
    "                        mask_singular = trg1 == trg2\n",
    "                        mask_multiple = trg1 != trg2\n",
    "                        singular = preds[mask_singular]\n",
    "                        n_correct += torch.sum(\n",
    "                            singular == trg1[mask_singular]\n",
    "                        ).cpu().detach().numpy()\n",
    "                        multiple = preds[mask_multiple]\n",
    "                        n_correct += torch.sum(\n",
    "                            (multiple == trg1[mask_multiple]) | (multiple == trg2[mask_multiple])\n",
    "                        ).cpu().detach().numpy()\n",
    "                        if mode == 'train':\n",
    "                            # вычисляем градиенты и обновляем веса\n",
    "                            loss_all.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                    processed_data += len(preds)\n",
    "\n",
    "            # вычисляем ошибку и точность прогноза на эпохе\n",
    "            loader_loss = train_loss / processed_data\n",
    "            loader_acc = n_correct / processed_data\n",
    "            print(f'{epoch + 1} epoch with {mode} mode has: {loader_loss} loss, {loader_acc} acc')\n",
    "        \n",
    "        # делаем шаг для sheduler оптимайзера\n",
    "        scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model_cls.pre_classifier.parameters(), 'lr': 8e-4},\n",
    "    {'params': model_cls.classifier.parameters(), 'lr': 8e-4}\n",
    "], lr=2e-6)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.8)\n",
    "train_model(epochs, model_cls, loaders, optimizer, scheduler, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'distilbert_cls.pth'\n",
    "\n",
    "mode_process = input('Load weights? (y/n)')\n",
    "if mode_process == 'n':\n",
    "    torch.save(model_cls.state_dict(), model_name)\n",
    "elif mode_process == 'y':\n",
    "    model_cls.load_state_dict(torch.load('/kaggle/input/distilbert-weights/distilbert_cls.pth'))\n",
    "else:\n",
    "    assert mode_process in ['n', 'y']\n",
    "model_cls.eval()\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
