{"cells":[{"cell_type":"markdown","metadata":{},"source":["### Установка и импорт всех необходимых зависимостей"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-03-01T19:18:03.493779Z","iopub.status.busy":"2024-03-01T19:18:03.492992Z","iopub.status.idle":"2024-03-01T19:19:31.704451Z","shell.execute_reply":"2024-03-01T19:19:31.703219Z","shell.execute_reply.started":"2024-03-01T19:18:03.493745Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","mapclassify 2.6.1 requires networkx>=2.7, but you have networkx 2.6.3 which is incompatible.\n","momepy 0.7.0 requires networkx>=2.7, but you have networkx 2.6.3 which is incompatible.\n","momepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\n","osmnx 1.8.1 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\n","scikit-image 0.22.0 requires networkx>=2.8, but you have networkx 2.6.3 which is incompatible.\n","spopt 0.6.0 requires networkx>=2.7, but you have networkx 2.6.3 which is incompatible.\n","spopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["!pip install -q razdel\n","!pip install -q pymorphy2\n","!pip install -q git+https://github.com/ahmados/rusynonyms.git\n","!pip install -q natasha\n","!pip install -q pyaml-env\n","!pip install -q captum"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-03-01T19:19:31.706783Z","iopub.status.busy":"2024-03-01T19:19:31.706485Z","iopub.status.idle":"2024-03-01T19:19:39.368033Z","shell.execute_reply":"2024-03-01T19:19:39.367262Z","shell.execute_reply.started":"2024-03-01T19:19:31.706757Z"},"trusted":true},"outputs":[],"source":["import os\n","import sys\n","\n","path_to_alti = '/kaggle/input/transformer-contributions1/transformer-contributions'\n","if not path_to_alti in sys.path:\n","    sys.path.append(path_to_alti)\n","\n","from src.utils_contributions import *\n","from src.contributions import ModelWrapper, ClassificationModelWrapperCaptum, interpret_sentence, occlusion"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-03-01T19:19:39.369536Z","iopub.status.busy":"2024-03-01T19:19:39.369101Z","iopub.status.idle":"2024-03-01T19:19:52.949527Z","shell.execute_reply":"2024-03-01T19:19:52.948630Z","shell.execute_reply.started":"2024-03-01T19:19:39.369510Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-03-01 19:19:42.198830: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-03-01 19:19:42.198922: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-03-01 19:19:42.338723: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]},{"name":"stdout","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}],"source":["import xml.etree.ElementTree as ET\n","import pandas as pd\n","\n","import nltk\n","from nltk.corpus import stopwords\n","import re\n","import pymorphy2\n","import razdel\n","import string\n","from natasha import (\n","    MorphVocab,\n","    NewsMorphTagger,\n","    NewsEmbedding,\n","    Segmenter,\n","    NewsSyntaxParser,\n","    Doc\n",")\n","\n","import torch\n","import tensorflow_hub as hub\n","from torch.utils.data import Dataset, DataLoader\n","import transformers\n","from transformers import (\n","    AutoTokenizer, AutoConfig,\n","    AutoModelForSequenceClassification, \n","    AutoModel,\n",")\n","import numpy as np\n","from sklearn.metrics import f1_score\n","\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","from tqdm import tqdm\n","from typing import *\n","from functools import partial\n","from collections import defaultdict\n","from IPython.display import clear_output\n","\n","from lime.lime_text import LimeTextExplainer\n","import shap\n","\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","rus_stopwords = stopwords.words('russian')\n","punctuation = list(string.punctuation)"]},{"cell_type":"markdown","metadata":{},"source":["### Работа с данными (kaggle)"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-03-01T19:19:52.953148Z","iopub.status.busy":"2024-03-01T19:19:52.951773Z","iopub.status.idle":"2024-03-01T19:19:52.957891Z","shell.execute_reply":"2024-03-01T19:19:52.957041Z","shell.execute_reply.started":"2024-03-01T19:19:52.953110Z"},"trusted":true},"outputs":[],"source":["datasets_folder = '/kaggle/input/sw-datasets/Russian-Sentiment-Analysis-Evaluation-Datasets'\n","datasets = ['SentiRuEval-2015-telecoms', 'SentiRuEval-2015-banks', 'SentiRuEval-2016-banks', 'SentiRuEval-2016-telecoms']\n","samples = ['test.xml', 'train.xml', 'test_etalon.xml']"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-03-01T19:19:52.959298Z","iopub.status.busy":"2024-03-01T19:19:52.959006Z","iopub.status.idle":"2024-03-01T19:19:53.857017Z","shell.execute_reply":"2024-03-01T19:19:53.856233Z","shell.execute_reply.started":"2024-03-01T19:19:52.959274Z"},"trusted":true},"outputs":[],"source":["def extract_data(path: str) -> pd.DataFrame:\n","    \"\"\"\n","    функция для извлечения данных из xml\n","    \"\"\"\n","    tree = ET.parse(path)\n","    root = tree.getroot()\n","    DataFrame = dict()\n","    database = root.findall('database')[0]\n","    DataFrame_columns = list()\n","\n","    for idx, table in enumerate(database.findall('table')):\n","        for column in table.findall('column'):\n","            DataFrame[column.attrib['name']] = list()\n","            DataFrame_columns.append(column.attrib['name'])\n","        if idx == 0:\n","            break\n","\n","    for table in database.findall('table'):\n","        for column in table.findall('column'):\n","            DataFrame[column.attrib['name']].append(column.text)\n","\n","    data = pd.DataFrame(DataFrame, columns=DataFrame_columns)\n","    return data\n","\n","# инициализация всех путей (kaggle)\n","banks_dataset = datasets[3]\n","path2samples = os.path.join(datasets_folder, banks_dataset)\n","\n","dataset_type = 'telecom'\n","if dataset_type == 'banks':\n","    banks = ['sberbank', 'vtb', 'gazprom', 'alfabank', 'bankmoskvy', 'raiffeisen', 'uralsib', 'rshb']\n","    cols_with_reviews = banks\n","if dataset_type == 'telecom':\n","    mobile_companies = ['beeline', 'mts', 'megafon', 'tele2', 'rostelecom', 'komstar', 'skylink']\n","    cols_with_reviews = mobile_companies\n","\n","path2test = os.path.join(path2samples, samples[2])\n","data_test = extract_data(path2test)\n","\n","path2train = os.path.join(path2samples, samples[1])\n","data_train = extract_data(path2train)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-03-01T19:19:53.858474Z","iopub.status.busy":"2024-03-01T19:19:53.858191Z","iopub.status.idle":"2024-03-01T19:19:58.566001Z","shell.execute_reply":"2024-03-01T19:19:58.565230Z","shell.execute_reply.started":"2024-03-01T19:19:53.858450Z"},"trusted":true},"outputs":[],"source":["def extract_text_features(data: pd.DataFrame, cols: List[str]) -> pd.DataFrame:\n","    \"\"\"\n","    функция для первичной обработки текста от лишних символов\n","    \"\"\"\n","    extracted_data = dict()\n","    extracted_data['text'] = list()\n","    extracted_data['0class'] = list()\n","    extracted_data['1class'] = list()\n","\n","    for idx in range(len(data)):\n","        row = data.iloc[idx, :]\n","        reviews = row[cols]\n","        unique_labels = set(reviews)\n","        unique_labels.remove('NULL')\n","\n","        # убираем все ненужные знаки\n","        filtered_text = re.sub('http[A-z|:|.|/|0-9]*', '', row['text'])\n","        filtered_text = re.sub('@\\S*', '', filtered_text)\n","        filtered_text = re.sub('#|:|»|«|-|xD|;D|\\\"|_|/|%', '', filtered_text)\n","        # filtered_text = re.sub(r'\\.(?=\\s)|,|(?<!\\s)\\.(?!\\s)|\\?', ' ', filtered_text)\n","        # filtered_text = re.sub(r'[A-Z]|[a-z]', '', filtered_text)\n","        # filtered_text = re.sub(r'\\d+', 'число', filtered_text)\n","        filtered_text = re.sub(r'\\s+', ' ', filtered_text).strip()\n","        new_text = filtered_text\n","\n","        # сохраняем только уникальные токены (без придатка xml NULL)\n","        unique_labels = list(unique_labels)\n","        while len(unique_labels) < 2:\n","            unique_labels.append(unique_labels[-1])\n","        extracted_data['text'].append(new_text)\n","        for idx, label in enumerate(unique_labels):\n","            text_label = int(label) + 1\n","            extracted_data[f'{idx}' + 'class'].append(text_label)\n","\n","    extracted_data = pd.DataFrame(extracted_data)\n","    \n","    # возвращаем dataframe\n","    return extracted_data\n","\n","extracted_val = extract_text_features(data_test, cols_with_reviews)\n","extracted_train = extract_text_features(data_train, cols_with_reviews)"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-03-01T19:19:58.567537Z","iopub.status.busy":"2024-03-01T19:19:58.567223Z","iopub.status.idle":"2024-03-01T19:19:58.574408Z","shell.execute_reply":"2024-03-01T19:19:58.573453Z","shell.execute_reply.started":"2024-03-01T19:19:58.567511Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'нас просто не захотели обслуживать,тк сотрудники болтали между собой. На просьбу обратить на нас внимание они ответили игнором'"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["# пример твита из датасета\n","extracted_val.iloc[1000].text"]},{"cell_type":"markdown","metadata":{},"source":["### Инициализируем модель (fine-tune) для решения нашей задачи классификации"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-03-01T19:19:58.575881Z","iopub.status.busy":"2024-03-01T19:19:58.575611Z","iopub.status.idle":"2024-03-01T19:19:58.585974Z","shell.execute_reply":"2024-03-01T19:19:58.585131Z","shell.execute_reply.started":"2024-03-01T19:19:58.575858Z"},"trusted":true},"outputs":[],"source":["def load_model_hf_cls(\n","    model_load: str, model_type: str, \n","    load_model_weights: bool=False\n",") -> torch.nn.Module:\n","\n","    assert model_type in ['distilbert', 'bert']\n","\n","    tokenizer = AutoTokenizer.from_pretrained(\n","        model_load, do_lower_case=True,\n","        add_additional_tokens=True\n","    )\n","\n","    if load_model_weights:\n","        model = AutoModel.from_pretrained(model_load)\n","        model_config = model.config\n","    else:\n","        model_config = AutoConfig.from_pretrained(model_load)\n","\n","    model_cls = AutoModelForSequenceClassification.from_config(model_config)\n","    \n","    if load_model_weights:\n","        if model_type == 'distilbert':\n","            model_cls.distilbert = model\n","        elif model_type == 'bert':\n","            model_cls.bert = model\n","        del model\n","\n","    return model_cls, tokenizer"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-03-01T19:19:58.587273Z","iopub.status.busy":"2024-03-01T19:19:58.586999Z","iopub.status.idle":"2024-03-01T19:20:03.076002Z","shell.execute_reply":"2024-03-01T19:20:03.075082Z","shell.execute_reply.started":"2024-03-01T19:19:58.587251Z"},"trusted":true},"outputs":[],"source":["distilbert_name = \"DeepPavlov/distilrubert-base-cased-conversational\"\n","bert_base_name = \"DeepPavlov/rubert-base-cased\"\n","\n","device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n","num_cls = len(pd.unique(extracted_train['0class']))\n","load_tf = True\n","\n","model_cls, tokenizer = load_model_hf_cls(\n","    distilbert_name, model_type='distilbert', \n","    load_model_weights=False\n",")\n","seq_max_len = model_cls.config.max_position_embeddings\n","hid_dim = model_cls.config.dim\n","model_cls.dropout = torch.nn.Identity()\n","model_cls.pre_classifier = torch.nn.Sequential(\n","    torch.nn.Linear(hid_dim, hid_dim, bias=False),\n","    torch.nn.Dropout(0.15),\n","    torch.nn.ReLU()\n",")\n","model_cls.classifier = torch.nn.Sequential(\n","    torch.nn.Linear(hid_dim, hid_dim, bias=False),\n","    torch.nn.Dropout(0.15),\n","    torch.nn.ReLU(),\n","    torch.nn.Linear(hid_dim, num_cls, bias=False),\n",")\n","\n","clear_output()"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-03-01T19:20:03.079995Z","iopub.status.busy":"2024-03-01T19:20:03.079693Z","iopub.status.idle":"2024-03-01T19:20:03.087759Z","shell.execute_reply":"2024-03-01T19:20:03.086834Z","shell.execute_reply.started":"2024-03-01T19:20:03.079970Z"},"trusted":true},"outputs":[],"source":["train_batch_size = 24\n","val_batch_size = 24\n","\n","class SentimentDataTransformer(Dataset):\n","    # инициализация датасета\n","    def __init__(\n","        self, texts: List[str], \n","        labels: List[Tuple[int, ...]]=None,\n","        labels_amount: int=1\n","    ) -> None:\n","\n","        self.texts = texts\n","        self.labels = labels\n","\n","    # для получения размера датасета\n","    def __len__(self) -> int:\n","        return len(self.texts)\n","\n","    # для получения элемента по индексу\n","    def __getitem__(\n","        self, index: int\n","    ) -> Tuple[Union[str, int]]:\n","\n","        if self.labels is None:\n","            return self.texts[index]\n","\n","        text = self.texts[index]\n","        labels = self.labels[index]\n","        \n","        target1, target2 = labels\n","\n","        return text, target1, target2"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-03-01T19:20:03.089165Z","iopub.status.busy":"2024-03-01T19:20:03.088845Z","iopub.status.idle":"2024-03-01T19:20:03.101566Z","shell.execute_reply":"2024-03-01T19:20:03.100830Z","shell.execute_reply.started":"2024-03-01T19:20:03.089141Z"},"trusted":true},"outputs":[],"source":["class collate_fn_transformers():\n","    \n","    def __init__(\n","        self, tokenizer: AutoTokenizer, \n","        use_labels:bool, use_tok_type_ids: bool\n","    ) -> None:\n","        \n","        self.tokenizer = tokenizer\n","        self.use_tok_type_ids = use_tok_type_ids\n","        self.use_labels = use_labels\n","        \n","    def __call__(self, batch):\n","        \n","        if not self.use_labels:\n","\n","            texts = batch\n","\n","            return self.tokenizer(\n","                texts, #truncation=True,\n","                padding=True, add_special_tokens=True,\n","                return_token_type_ids=self.use_tok_type_ids,\n","                return_tensors='pt'\n","            )\n","        \n","        texts, target1, target2 = zip(*batch)\n","        \n","        input_ids = self.tokenizer(\n","            texts, #truncation=True,\n","            padding=True, add_special_tokens=True,\n","            return_token_type_ids=self.use_tok_type_ids,\n","            return_tensors='pt'\n","        )\n","        target1 = torch.tensor(target1)\n","        target2 = torch.tensor(target2)\n","        \n","        return input_ids, target1, target2"]},{"cell_type":"markdown","metadata":{},"source":["### Инициализируем наши DataLoaders"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-03-01T19:20:03.103739Z","iopub.status.busy":"2024-03-01T19:20:03.102621Z","iopub.status.idle":"2024-03-01T19:20:03.118461Z","shell.execute_reply":"2024-03-01T19:20:03.117613Z","shell.execute_reply.started":"2024-03-01T19:20:03.103714Z"},"trusted":true},"outputs":[],"source":["train = SentimentDataTransformer(\n","    texts=extracted_train['text'].tolist(),\n","    labels=list(zip(extracted_train['0class'], extracted_train['1class']))\n",")\n","\n","val = SentimentDataTransformer(\n","    texts=extracted_val['text'].tolist(),\n","    labels=list(zip(extracted_val['0class'], extracted_val['1class']))\n",")\n","\n","collate_fn_bert = collate_fn_transformers(\n","    tokenizer=tokenizer, use_tok_type_ids=False, \n","    use_labels=True\n",")\n","\n","train_loader = DataLoader(\n","    train, batch_size=train_batch_size,\n","    shuffle=True, collate_fn=collate_fn_bert\n",")\n","val_loader = DataLoader(\n","    val, batch_size=val_batch_size,\n","    shuffle=False, collate_fn=collate_fn_bert\n",")\n","loaders = {\n","    'train': train_loader,\n","    'val': val_loader\n","}"]},{"cell_type":"markdown","metadata":{},"source":["### Дообучение модели"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-03-01T19:20:03.119812Z","iopub.status.busy":"2024-03-01T19:20:03.119529Z","iopub.status.idle":"2024-03-01T19:20:03.137915Z","shell.execute_reply":"2024-03-01T19:20:03.137124Z","shell.execute_reply.started":"2024-03-01T19:20:03.119785Z"},"trusted":true},"outputs":[],"source":["def train_model(\n","    epochs: int, model: torch.nn.Module, loaders: Dict[str, DataLoader], \n","    optimizer: torch.optim, scheduler: torch.optim.lr_scheduler, \n","    weights_vector: torch.tensor=None, device: str='cpu'\n",") -> None:\n","    # cross entropy loss\n","    model = model.to(device)\n","    if weights_vector is None:\n","        weights_vector = torch.ones(size=(num_cls,), device=device)\n","    loss_function1 = torch.nn.CrossEntropyLoss(reduction='mean', weight=weights_vector)\n","    loss_function2 = torch.nn.CrossEntropyLoss(reduction='mean', weight=weights_vector)\n","    \n","    # извлечение DataLoaders\n","    if len(loaders) > 1:\n","        train_loader = loaders['train']\n","        val_loader = loaders['val']\n","        steps_per_epoch = [('train', train_loader), ('val', val_loader)]\n","    else:\n","        train_loader = loaders['train']\n","        steps_per_epoch = [('train', train_loader)]\n","\n","    # обучение по эпохам\n","    for epoch in range(epochs):\n","        for mode, loader in steps_per_epoch:\n","            # сохранение статистик\n","            train_loss = 0\n","            n_correct = 0\n","            processed_data = 0\n","            \n","            # train/val \n","            if mode == 'train':\n","                model.train()\n","                requires_grad_mode = True\n","            else:\n","                model.eval()\n","                requires_grad_mode = False\n","            \n","            # проход по батчам\n","            for inputs, trg1, trg2 in tqdm(loader):\n","                # обнуляем градиенты\n","                optimizer.zero_grad()\n","\n","                # извлечение входных данных для модели\n","                for key, value in inputs.items():\n","                    inputs[key] = value.to(device)\n","                trg1, trg2 = trg1.to(device), trg2.to(device)\n","                inputs['return_dict'] = True\n","                \n","                # устанавливаем необходимость вычислять/не_вычислять градиенты\n","                with torch.set_grad_enabled(requires_grad_mode):\n","                    outputs = model(**inputs)\n","                    preds = torch.argmax(outputs['logits'], dim=1)\n","\n","                    # настраиваем модели на конкретный target\n","                    if all(trg1 == trg2):\n","                        loss1 = loss_function1(outputs['logits'], trg1)\n","                        train_loss += loss1.item()\n","                        n_correct += torch.sum(preds == trg1).cpu().detach().numpy()\n","                        if mode == 'train':\n","                            # вычисляем градиенты и обновляем веса\n","                            loss1.backward()\n","                            optimizer.step()\n","                    # если у твита более чем 1 метка, то настраиваем на обе\n","                    else:\n","                        loss1 = loss_function1(outputs['logits'], trg1) * 0.5\n","                        loss2 = loss_function2(outputs['logits'], trg2) * 0.5\n","                        loss_all = loss1 + loss2\n","                        train_loss += loss_all.item()\n","\n","                        mask_singular = trg1 == trg2\n","                        mask_multiple = trg1 != trg2\n","                        singular = preds[mask_singular]\n","                        n_correct += torch.sum(\n","                            singular == trg1[mask_singular]\n","                        ).cpu().detach().numpy()\n","                        multiple = preds[mask_multiple]\n","                        n_correct += torch.sum(\n","                            (multiple == trg1[mask_multiple]) | (multiple == trg2[mask_multiple])\n","                        ).cpu().detach().numpy()\n","                        if mode == 'train':\n","                            # вычисляем градиенты и обновляем веса\n","                            loss_all.backward()\n","                            optimizer.step()\n","\n","                    processed_data += len(preds)\n","\n","            # вычисляем ошибку и точность прогноза на эпохе\n","            loader_loss = train_loss / processed_data\n","            loader_acc = n_correct / processed_data\n","            print(f'{epoch + 1} epoch with {mode} mode has: {loader_loss} loss, {loader_acc} acc')\n","        \n","        # делаем шаг для sheduler оптимайзера\n","        scheduler.step()"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-03-01T19:20:03.139176Z","iopub.status.busy":"2024-03-01T19:20:03.138895Z","iopub.status.idle":"2024-03-01T19:20:21.943778Z","shell.execute_reply":"2024-03-01T19:20:21.942844Z","shell.execute_reply.started":"2024-03-01T19:20:03.139150Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Load weights? (y/n) y\n"]},{"name":"stdout","output_type":"stream","text":["all is matched!\n"]}],"source":["model_name = 'distilbert_cls.pth'\n","\n","mode_process = input('Load weights? (y/n)')\n","if mode_process == 'n':\n","    torch.save(model_cls.state_dict(), model_name)\n","elif mode_process == 'y':\n","    model_cls.load_state_dict(\n","        torch.load('/kaggle/input/distilbert-teltecoms/distilbert_cls_telecom_2016.pth')\\\n","    )\n","    print('all is matched!')\n","else:\n","    assert mode_process in ['n', 'y']\n","model_cls.eval()\n","None"]},{"cell_type":"markdown","metadata":{},"source":["### Вычисление итоговых показателей"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-03-01T19:20:21.945554Z","iopub.status.busy":"2024-03-01T19:20:21.945212Z","iopub.status.idle":"2024-03-01T19:20:21.961264Z","shell.execute_reply":"2024-03-01T19:20:21.960168Z","shell.execute_reply.started":"2024-03-01T19:20:21.945528Z"},"trusted":true},"outputs":[],"source":["def pass_data_transformers(\n","    model: torch.nn.Module, loader: DataLoader,\n","    loader_labels: bool=False, return_embs: bool=False,\n","    device: str='cpu', verbose: bool=False\n",") -> Tuple[np.array, Optional[np.array], Optional[np.array]]:\n","    \"\"\"\n","    pass data from loader through bert model\n","    \"\"\"\n","    model.eval()\n","    model = model.to(device)\n","\n","    if verbose:\n","        pbar = tqdm(len(loader), leave=True, position=0)\n","    \n","    with torch.no_grad():\n","\n","        all_probs = list()\n","        if loader_labels:\n","            all_labels1 = list()\n","            all_labels2 = list()\n","        if return_embs:\n","            all_embs = list()\n","        for idx, inputs in enumerate(loader):\n","\n","            if loader_labels:\n","                ids, trg1, trg2 = inputs\n","            else:\n","                ids = inputs\n","            ids = ids.to(device)\n","            ids['return_dict'] = True\n","            outputs = model(**ids)\n","            logits = outputs['logits']\n","            if return_embs:\n","                embs = outputs['hidden_states'][-1][:, 0, :]\n","                all_embs.append(embs.cpu().detach().numpy())\n","\n","            probs = torch.nn.functional.softmax(\n","                logits, dim=1\n","            ).cpu().detach().numpy()\n","            all_probs.append(probs)\n","            if loader_labels:\n","                all_labels1.append(trg1.cpu().detach().numpy())\n","                all_labels2.append(trg2.cpu().detach().numpy())\n","\n","            if verbose:\n","                pbar.update(1)\n","                pbar.set_description(f'processed: {idx + 1}')\n","\n","        all_probs = np.vstack(all_probs)\n","        if loader_labels:\n","            all_labels1 = np.hstack(all_labels1)\n","            all_labels2 = np.hstack(all_labels2)\n","        if return_embs:\n","            all_embs = np.vstack(all_embs)\n","\n","    return_data = [all_probs]\n","    if loader_labels:\n","        return_data.append(all_labels1)\n","        return_data.append(all_labels2)\n","    if return_embs:\n","        return_data.append(all_embs)\n","\n","    if len(return_data) > 1:\n","        return tuple(return_data)\n","    else:\n","        return return_data[0]"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-03-01T19:20:21.962894Z","iopub.status.busy":"2024-03-01T19:20:21.962620Z","iopub.status.idle":"2024-03-01T19:20:22.014449Z","shell.execute_reply":"2024-03-01T19:20:22.013655Z","shell.execute_reply.started":"2024-03-01T19:20:21.962871Z"},"trusted":true},"outputs":[],"source":["def calculate_accuracy(\n","    model: torch.nn.Module, loader: DataLoader,\n","    device: str='cpu', verbose: bool=True\n",") -> float:\n","    model.eval()\n","    model = model.to(device)\n","    \n","    # проход по батчам\n","    probs, trg1, trg2 = pass_data_transformers(\n","        model=model, loader=loader, device=device,\n","        loader_labels=True, verbose=verbose\n","    )\n","    preds = np.argmax(probs, axis=1)\n","    mask_singular = trg1 == trg2\n","    mask_multiple = trg1 != trg2\n","    singular = preds[mask_singular]\n","    n_correct = np.sum(singular == trg1[mask_singular])\n","    multiple = preds[mask_multiple]\n","    if len(multiple) > 0:\n","        n_correct += np.sum(\n","            \n","            (multiple == trg1[mask_multiple]) | (multiple == trg2[mask_multiple])\n","        )\n","    loader_acc = n_correct / len(preds)\n","\n","    return loader_acc\n","\n","def calculate_f1_class(\n","    model: torch.nn.Module, loader: DataLoader,\n","    class_num: int, device: str='cpu', verbose: bool=True\n",") -> float:\n","    model.eval()\n","    model = model.to(device)\n","    all_preds = list()\n","    groud_truth = list()\n","\n","    # проход по батчам\n","    probs, trg1, trg2 = pass_data_transformers(\n","        model=model, loader=loader, device=device,\n","        loader_labels=True, verbose=verbose\n","    )\n","    preds = np.argmax(probs, axis=1)\n","\n","    mask_preds = preds == class_num\n","    preds[mask_preds] = 1\n","    preds[~mask_preds] = 0\n","    trg = np.zeros(len(preds))\n","    mask_trg = (trg1 == class_num) | (trg2 == class_num)\n","    trg[mask_trg] = 1\n","    trg[~mask_trg] = 0\n","\n","    return f1_score(trg, preds)"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-03-01T19:20:22.015853Z","iopub.status.busy":"2024-03-01T19:20:22.015572Z","iopub.status.idle":"2024-03-01T19:20:29.426833Z","shell.execute_reply":"2024-03-01T19:20:29.425936Z","shell.execute_reply.started":"2024-03-01T19:20:22.015831Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["processed: 94: : 94it [00:02, 42.70it/s]\n","processed: 94: : 94it [00:01, 56.14it/s]\n","processed: 94: : 94it [00:01, 56.50it/s]\n","processed: 94: : 94it [00:01, 56.37it/s]\n"]}],"source":["test_acc = calculate_accuracy(model_cls, val_loader, device)\n","class_neg_f1 = calculate_f1_class(model_cls, val_loader, 0, device)\n","class_neu_f1 = calculate_f1_class(model_cls, val_loader, 1, device)\n","class_pos_f1 = calculate_f1_class(model_cls, val_loader, 2, device)"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-03-01T19:20:29.428546Z","iopub.status.busy":"2024-03-01T19:20:29.428146Z","iopub.status.idle":"2024-03-01T19:20:29.434997Z","shell.execute_reply":"2024-03-01T19:20:29.434033Z","shell.execute_reply.started":"2024-03-01T19:20:29.428492Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(0.7454383622607922,\n"," 0.7858472998137804,\n"," 0.7371134020618556,\n"," 0.5098901098901099)"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["# общая accuracy и f1 по классам\n","test_acc, class_neg_f1, class_neu_f1, class_pos_f1"]},{"cell_type":"markdown","metadata":{},"source":["## Backdoor attacks on neural network(adversial examples)"]},{"cell_type":"markdown","metadata":{},"source":["### USE metric for similarity between original sentence and spoiled sentence"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-02-18T10:54:07.454131Z","iopub.status.busy":"2024-02-18T10:54:07.453789Z","iopub.status.idle":"2024-02-18T10:54:50.344090Z","shell.execute_reply":"2024-02-18T10:54:50.343260Z","shell.execute_reply.started":"2024-02-18T10:54:07.454100Z"},"trusted":true},"outputs":[],"source":["from scipy.spatial.distance import cosine\n","# Load pre-trained universal sentence encoder model\n","use_encoder = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-03-01T19:20:51.409307Z","iopub.status.busy":"2024-03-01T19:20:51.408921Z","iopub.status.idle":"2024-03-01T19:20:51.420806Z","shell.execute_reply":"2024-03-01T19:20:51.419739Z","shell.execute_reply.started":"2024-03-01T19:20:51.409278Z"},"trusted":true},"outputs":[],"source":["def use_score(\n","    original: List[str], adversial: List[str],\n","    use_bert_encoder: bool=False, bert_batch_size: int=24\n",") -> Tuple[np.array, float]:\n","    # using DAN from tensorflow\n","    if not use_bert_encoder:\n","        global use_encoder\n","        # get embs of texts\n","        orig_emb = use_encoder(original)\n","        adv_emb = use_encoder(adversial)\n","    # using BERT itself\n","    else:\n","        global model_cls\n","        model_cls.eval()\n","        model_cls = model_cls.to(device)\n","        # calculate use_score with BERT\n","        \n","        orig_data = SentimentDataTransformer(texts=original)\n","        adv_data = SentimentDataTransformer(texts=adversial)\n","        collate_fn = collate_fn_transformers(\n","            tokenizer=tokenizer, use_tok_type_ids=False, \n","            use_labels=False\n","        )\n","        orig_loader = DataLoader(\n","            orig_data, batch_size=bert_batch_size,\n","            shuffle=False, collate_fn=collate_fn\n","        )\n","        adv_loader = DataLoader(\n","            adv_data, batch_size=bert_batch_size,\n","            shuffle=False, collate_fn=collate_fn\n","        )\n","\n","        orig_emb = pass_data_transformers(\n","            model=model_cls, loader=orig_loader, device=device,\n","            loader_labels=False, verbose=verbose, return_embs=True\n","        )[-1]\n","        adv_emb = pass_data_transformers(\n","            model=model_cls, loader=adv_loader, device=device,\n","            loader_labels=False, verbose=verbose, return_embs=True\n","        )[-1]\n","\n","    # calculate use_score\n","    orig_lens = np.sum(orig_emb ** 2, axis=1)\n","    adv_lens = np.sum(adv_emb ** 2, axis=1)\n","    cosine_dist = np.sum(orig_emb * adv_emb, axis=1)\n","    use_scores = cosine_dist / (orig_lens * adv_lens)\n","\n","    return use_scores, np.mean(use_scores)"]},{"cell_type":"markdown","metadata":{},"source":["### Prepare data for interpretation"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-03-01T19:20:51.837096Z","iopub.status.busy":"2024-03-01T19:20:51.836717Z","iopub.status.idle":"2024-03-01T19:20:52.200296Z","shell.execute_reply":"2024-03-01T19:20:52.199163Z","shell.execute_reply.started":"2024-03-01T19:20:51.837069Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Кол-во текстов, длина которых больше 3:\"\n","2233\n","Баланс классов:\n","[0 1 2] [0.45096283 0.44872369 0.10031348]\n"]}],"source":["min_length = 3\n","print(f'Кол-во текстов, длина которых больше {min_length}:\"')\n","mask = extracted_val['text'].apply(lambda x: len(list(razdel.tokenize(x)))) > min_length\n","print(np.sum(mask))\n","print('Баланс классов:')\n","unique_labels, counts = np.unique(extracted_val.loc[mask, '0class'], return_counts=True)\n","print(unique_labels, counts / np.sum(mask))\n","adversial_examples = extracted_val[mask].reset_index(drop=True)"]},{"cell_type":"markdown","metadata":{},"source":["## Interpretation"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-03-01T19:20:53.420906Z","iopub.status.busy":"2024-03-01T19:20:53.420525Z","iopub.status.idle":"2024-03-01T19:20:53.428561Z","shell.execute_reply":"2024-03-01T19:20:53.427288Z","shell.execute_reply.started":"2024-03-01T19:20:53.420877Z"},"trusted":true},"outputs":[],"source":["def gather_back_tokens(tokens: List[str], tokens_type: str) -> str:\n","    \"\"\"\n","    для превращения токенов в предложение\n","    tokens: список токенов\n","    tokens_type: natasha или razdel\n","    \"\"\"\n","    assert tokens_type in ['razdel', 'natasha']\n","\n","    sent = ''\n","    prev_end = None\n","    for token in tokens:\n","\n","        if tokens_type == 'natasha':\n","            token_text = token['text']\n","            token_start, token_stop = token['start'], token['stop']\n","        else:\n","            token_text = token.text\n","            token_start, token_stop = token.start, token.stop\n","        \n","        if not prev_end is None:\n","            sent += (token_start - prev_end) * ' '\n","\n","        sent += token_text\n","        prev_end = token_stop\n"," \n","    return sent"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-03-01T19:20:53.806598Z","iopub.status.busy":"2024-03-01T19:20:53.806131Z","iopub.status.idle":"2024-03-01T19:20:53.814953Z","shell.execute_reply":"2024-03-01T19:20:53.813929Z","shell.execute_reply.started":"2024-03-01T19:20:53.806560Z"},"trusted":true},"outputs":[],"source":["model_cls.eval()\n","model_cls = model_cls.to(device)"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-03-01T19:20:54.669302Z","iopub.status.busy":"2024-03-01T19:20:54.668668Z","iopub.status.idle":"2024-03-01T19:20:54.676397Z","shell.execute_reply":"2024-03-01T19:20:54.675437Z","shell.execute_reply.started":"2024-03-01T19:20:54.669272Z"},"trusted":true},"outputs":[],"source":["def predict_texts(texts: List[str], func_type: str, batch_size: int=30):\n","    \"\"\"\n","    for Lime: return probability distribution of text\n","    \"\"\"\n","    assert func_type in ['shap', 'lime']\n","    global tokenizer\n","    global model_cls\n","    global device\n","\n","    if func_type == 'shap':\n","        texts = list(map(lambda x: re.sub(r'\\.{3}', '[MASK]', x), texts))\n","\n","    # get model outputs\n","    dataset = SentimentDataTransformer(texts=texts)\n","    loader = DataLoader(\n","        dataset, batch_size=batch_size, shuffle=False,\n","        collate_fn=collate_fn_transformers(\n","            tokenizer=tokenizer, use_labels=False,\n","            use_tok_type_ids=False\n","        )\n","    )\n","    probs = pass_data_transformers(\n","        model=model_cls, loader=loader, device=device,\n","        loader_labels=False, verbose=False, return_embs=False\n","    )\n","    \n","    return probs"]},{"cell_type":"markdown","metadata":{},"source":["### lime"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-03-01T19:20:55.523986Z","iopub.status.busy":"2024-03-01T19:20:55.523232Z","iopub.status.idle":"2024-03-01T19:20:55.535545Z","shell.execute_reply":"2024-03-01T19:20:55.534575Z","shell.execute_reply.started":"2024-03-01T19:20:55.523954Z"},"trusted":true},"outputs":[],"source":["def lime_importance(\n","    tokens: List[Tuple[str, int, int]], tokens_type: str, \n","    num_features:int=300, num_samples:int=700\n",") -> List[str]:\n","    \"\"\"\n","    measure contribution with lime weights\n","    \"\"\"\n","    assert tokens_type in ['razdel', 'natasha']\n","\n","    def RazdelSplit(text):\n","        return [raz_tok.text for raz_tok in list(razdel.tokenize(text))]\n","\n","    def NatashaSplit(text):\n","        segmenter = Segmenter()\n","        text_doc = Doc(text.lower())\n","        text_doc.segment(segmenter)\n","\n","        return [nat_tok['text'] for nat_tok in text_doc]\n","\n","    # список для наиболее важных слов\n","    essential_words = list()\n","    text_to_explain = gather_back_tokens(tokens, tokens_type)\n","\n","    if tokens_type == 'razdel':\n","        Spliter = RazdelSplit\n","    elif tokens_type == 'natasha':\n","        Spliter = NatashaSplit\n","    # создаем Explainer\n","    explainer = LimeTextExplainer(\n","        class_names=['Neg', 'Neu', 'Pos'],\n","        split_expression=Spliter\n","    )\n","    # \"объясняем\" текст\n","    explanation = explainer.explain_instance(\n","        text_to_explain, partial(predict_texts, func_type='lime'), \n","        num_features=num_features, num_samples=num_samples\n","    )\n","    # создаем mapping из токена в его вес LogReg\n","    explanation_list = explanation.as_list()\n","    tok2weight = {token:weight for token, weight in explanation_list}\n","    # создаем список из токенов, их важности и позиции в тексте\n","    for token in tokens:\n","        if tokens_type == 'razdel':\n","            token_text = token.text.lower()\n","        else:\n","            token_text = token['text'].lower()\n","\n","        essential_words.append((\n","            token, tok2weight[token_text]\n","        ))\n","    # создаем функцию сравнения важности\n","    sort_func = lambda x: np.abs(x[1])\n","    # сортируем токены по важности\n","    essential_words = sorted(essential_words, key=sort_func, reverse=True)\n","    # возвращаем только слова и их позиции в тексте\n","    essential_words = [word for word, _ in essential_words]\n","\n","    return essential_words"]},{"cell_type":"markdown","metadata":{},"source":["### shap"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-03-01T19:20:56.102297Z","iopub.status.busy":"2024-03-01T19:20:56.101946Z","iopub.status.idle":"2024-03-01T19:20:56.112657Z","shell.execute_reply":"2024-03-01T19:20:56.111694Z","shell.execute_reply.started":"2024-03-01T19:20:56.102273Z"},"trusted":true},"outputs":[],"source":["def shap_importance(\n","    tokens: List[str], tokens_type: str, target: int\n",") -> List[str]:\n","    \"\"\"\n","    measure contribution with shap values\n","    \"\"\"\n","    assert tokens_type in ['razdel', 'natasha']\n","\n","    def custom_tokenizer(\n","        text: str, return_offsets_mapping=True\n","    ) -> Dict[str, List[Union[str, Tuple[int, ...]]]]:\n","        \"\"\"\n","        Custom tokenizers conform to a subset \n","        of the transformers API\n","        \"\"\"\n","        tokens = list(razdel.tokenize(text))\n","        \n","        words = list()\n","        offsets = list()\n","        for token in tokens:\n","            words.append(token.text)\n","            offsets.append((token.start, token.stop))\n","\n","        return {\n","            'input_ids': words,\n","            'offset_mapping': offsets\n","        }\n","\n","    # восстанавливаем текст из слов\n","    text_to_explain = gather_back_tokens(tokens,tokens_type)\n","\n","    masker = shap.maskers.Text(custom_tokenizer)\n","    explainer = shap.Explainer(\n","        partial(predict_texts,func_type='shap'), masker, \n","        output_names=['Neg', 'Neu', 'Pos']\n","    )\n","    # get shap values for the onliest text\n","    shap_values = explainer([text_to_explain])\n","\n","    tokens_order = shap_values.data[0]\n","    base_values = shap_values.base_values\n","    contributions = shap_values.values[0].sum(axis=1)\n","    essential_words = list(zip(tokens, contributions))\n","    # создаем функцию сравнения важности\n","    sort_func = lambda x: x[1]\n","    # сортируем токены по важности\n","    essential_words = sorted(essential_words, key=sort_func, reverse=True)\n","    # возвращаем только слова и их позиции в тексте\n","    essential_words = [word for word, _ in essential_words]\n","\n","    return essential_words"]},{"cell_type":"markdown","metadata":{},"source":["### alti"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-03-01T19:20:56.746631Z","iopub.status.busy":"2024-03-01T19:20:56.745914Z","iopub.status.idle":"2024-03-01T19:20:56.760443Z","shell.execute_reply":"2024-03-01T19:20:56.759524Z","shell.execute_reply.started":"2024-03-01T19:20:56.746602Z"},"trusted":true},"outputs":[],"source":["# model_cls_wrapper = ModelWrapper(model_cls)\n","\n","def alti_importance(\n","    tokens: List[Tuple[str, int, int]], tokens_type: str,\n","    measure_tokens_contributions: str\n",") -> List[str]:\n","    \"\"\"\n","    measure contribution with alti decomposition\n","    \"\"\"\n","    assert measure_tokens_contributions in ['cls', 'all_tokens']\n","    assert tokens_type in ['razdel', 'natasha']\n","    global model_cls_wrapper\n","    global tokenier\n","    global model_cls\n","    # восстанавливаем текст из слов\n","    text_to_explain = gather_back_tokens(tokens, tokens_type)\n","\n","    text_tokens = tokenizer.tokenize(text_to_explain)\n","    text_input = tokenizer(\n","        text_to_explain, return_tensors=\"pt\", \n","        return_token_type_ids=False,\n","        return_offsets_mapping=True\n","    ).to(device)\n","    offset_mapping = text_input['offset_mapping'][0,1:-1,:].cpu().detach()\n","    text_input['return_dict'] = True\n","    del text_input['offset_mapping']\n","\n","    # get words of text\n","    pos_words = [(word.start, word.stop) for word in tokens]\n","\n","    # create mapping from token to word\n","    cur_index = 0\n","    token_pos_to_word = dict()\n","    for idx, (offset, token) in enumerate(zip(offset_mapping, text_tokens)):\n","        start, _ = offset\n","        while start >= pos_words[cur_index][1]:\n","            cur_index += 1\n","        token_pos_to_word[idx] = tokens[cur_index]\n","\n","    # get contributions\n","    _, _, _, contributions_data = model_cls_wrapper(text_input)\n","    # get Yi from alti\n","    resultant_norm = torch.norm(\n","        torch.squeeze(contributions_data['resultants']),\n","        p=1, dim=-1\n","    )\n","    # get Cij from alti method\n","    # 'contributions' means Tij\n","    # alti requires scaling = min_sum\n","    normalized_contributions = normalize_contributions(\n","        contributions_data['contributions'], scaling='min_sum',\n","        resultant_norm=resultant_norm\n","    )\n","    # apply attention rollout and get seq of Ci\n","    contributions_mix = compute_joint_attention(normalized_contributions)\n","    # extract Ci after last self-attention layer\n","    joint_attention_layer = -1\n","    contributions_mix_last_hid = contributions_mix[joint_attention_layer]\n","    # define tokens to measure contributions\n","    if measure_tokens_contributions == 'cls':\n","        # contribution to token cls\n","        positions=np.array([0])\n","    else:\n","        # contribution to all tokens\n","        positions=np.arange(1, len(contributions_mix_last_hid) - 1)\n","\n","    word_to_contribution = defaultdict(lambda: 0.0)\n","    for pos in positions:\n","        # get tokens contrubitons\n","        contributions_mix_cur = contributions_mix_last_hid[pos][1:-1]\n","        for idx, contribution in enumerate(contributions_mix_cur):\n","            word_to_contribution[token_pos_to_word[idx]] += contribution\n","\n","    # функция для сортировки\n","    sort_func = lambda x: x[1]\n","    # сортируем токены по важности\n","    essential_words = sorted(\n","        [(word, cont) for word, cont in word_to_contribution.items()],\n","        key=sort_func, reverse=True\n","    )\n","    # возвращаем только слова и их позиции в тексте\n","    essential_words = [word for word, _ in essential_words]\n","    \n","    return essential_words"]},{"cell_type":"markdown","metadata":{},"source":["### Heuristic loss"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2024-03-01T19:20:57.454326Z","iopub.status.busy":"2024-03-01T19:20:57.453480Z","iopub.status.idle":"2024-03-01T19:20:57.465430Z","shell.execute_reply":"2024-03-01T19:20:57.464319Z","shell.execute_reply.started":"2024-03-01T19:20:57.454295Z"},"trusted":true},"outputs":[],"source":["def loss_importance(\n","    tokens: List[Tuple[str, int, int]], \n","    target: Union[int, str], tokens_type: str\n",") -> List[str]:\n","    \"\"\"\n","    measure contribution with loss changes\n","    \"\"\"\n","    assert tokens_type in ['razdel', 'natasha']\n","    global device\n","    global tokenizer\n","    global model_cls\n","    # восстанавливаем текст из слов\n","    text_to_explain = gather_back_tokens(tokens, tokens_type)\n","    # список для наиболее важных слов\n","    essential_words = list()\n","\n","    loss = torch.nn.CrossEntropyLoss()\n","    get_inputs = lambda x: tokenizer(\n","        x, padding=True, add_special_tokens=True,\n","        return_token_type_ids=False, return_tensors='pt'\n","    ).to(device)\n","\n","    # get inputs and outputs from model\n","    inputs = get_inputs(text_to_explain)\n","    inputs['return_dict'] = True\n","    outputs = model_cls(**inputs)['logits']\n","    target_pt = torch.tensor([target], dtype=torch.long)\n","\n","    # calculate loss for original text\n","    loss_score_integral = loss(\n","        outputs.cpu(), target_pt\n","    )\n","    # calculate loss for each token removed\n","    for idx, token in enumerate(tokens):\n","        # get text without one token\n","        tokens_copy = tokens.copy()\n","        tokens_copy.pop(idx)\n","        text_to_explain = gather_back_tokens(tokens_copy, tokens_type)\n","        # calculate loss without current word\n","        inputs = get_inputs(text_to_explain)\n","        inputs['return_dict'] = True\n","        with torch.no_grad():\n","            outputs = model_cls(**inputs)['logits']\n","        loss_score_part = loss(outputs.cpu(), target_pt)\n","        # add our score of change\n","        essential_words.append((\n","            token, (loss_score_part-loss_score_integral).cpu().detach().numpy()\n","        ))\n","\n","    # создаем функцию сравнения важности\n","    sort_func = lambda x: x[1]\n","    # сортируем токены по важности\n","    essential_words = sorted(essential_words, key=sort_func, reverse=True)\n","    # возвращаем только слова и их позиции в тексте\n","    essential_words = [word for word, _ in essential_words]\n","\n","    return essential_words"]},{"cell_type":"markdown","metadata":{},"source":["### attention"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2024-03-01T19:20:58.095730Z","iopub.status.busy":"2024-03-01T19:20:58.095349Z","iopub.status.idle":"2024-03-01T19:20:58.108961Z","shell.execute_reply":"2024-03-01T19:20:58.108058Z","shell.execute_reply.started":"2024-03-01T19:20:58.095700Z"},"trusted":true},"outputs":[],"source":["def attention_importance(\n","    tokens: List[Tuple[str, int, int]], tokens_type: str,\n","    measure_tokens_contributions: str\n",") -> List[str]:\n","    \"\"\"\n","    measure contribution with transformers attention\n","    \"\"\"\n","    assert measure_tokens_contributions in ['cls', 'all_tokens']\n","    assert tokens_type in ['razdel', 'natasha']\n","    global tokenier\n","    global model_cls\n","    # восстанавливаем текст из слов\n","    text_to_explain = gather_back_tokens(tokens, tokens_type)\n","        \n","    text_tokens = tokenizer.tokenize(text_to_explain)\n","    text_input = tokenizer(\n","        text_to_explain, return_tensors=\"pt\", \n","        return_token_type_ids=False,\n","        return_offsets_mapping=True\n","    ).to(device)\n","    offset_mapping = text_input['offset_mapping'][0,1:-1,:].cpu().detach()\n","    text_input['return_dict'] = True\n","    del text_input['offset_mapping']\n","\n","    # get words of text\n","    pos_words = [(word.start, word.stop) for word in tokens]\n","\n","    # create mapping from token to word\n","    cur_index = 0\n","    token_pos_to_word = dict()\n","    for idx, (offset, token) in enumerate(zip(offset_mapping, text_tokens)):\n","        start, _ = offset\n","        while start >= pos_words[cur_index][1]:\n","            cur_index += 1\n","        token_pos_to_word[idx] = tokens[cur_index]\n","    \n","    with torch.no_grad():\n","        contributions_mix_last_hid = compute_rollout(\n","            torch.cat(\n","                model_cls(**text_input)['attentions']\n","            ).mean(dim=1).cpu().detach().numpy()\n","        )[-1]\n","\n","    if measure_tokens_contributions == 'cls':\n","        # contribution to token cls\n","        positions=np.array([0])\n","    else:\n","        # contribution to all tokens\n","        positions=np.arange(1, len(contributions_mix_last_hid) - 1)\n","\n","    word_to_contribution = defaultdict(lambda: 0.0)\n","    for pos in positions:\n","        # get tokens contrubitons\n","        contributions_mix_cur = contributions_mix_last_hid[pos][1:-1]\n","        for idx, contribution in enumerate(contributions_mix_cur):\n","            word_to_contribution[token_pos_to_word[idx]] += contribution\n","\n","    # функция для сортировки\n","    sort_func = lambda x: x[1]\n","    # сортируем токены по важности\n","    essential_words = sorted(\n","        [(word, cont) for word, cont in word_to_contribution.items()],\n","        key=sort_func, reverse=True\n","    )\n","    # возвращаем только слова и их позиции в тексте\n","    essential_words = [word for word, _ in essential_words]\n","\n","    return essential_words"]},{"cell_type":"markdown","metadata":{},"source":["### gradients"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2024-03-01T19:20:58.848026Z","iopub.status.busy":"2024-03-01T19:20:58.847681Z","iopub.status.idle":"2024-03-01T19:20:58.864667Z","shell.execute_reply":"2024-03-01T19:20:58.863568Z","shell.execute_reply.started":"2024-03-01T19:20:58.848001Z"},"trusted":true},"outputs":[],"source":["from captum.attr import LayerIntegratedGradients\n","\n","head_mask = [None] * model_cls.config.num_hidden_layers\n","use_tok_type_ids=False\n","\n","def gradient_importance(\n","    tokens: List[Tuple[str, int, int]], tokens_type: str,\n","    target: int, alpha_steps: int=20\n",") -> List[str]:\n","    \"\"\"\n","    measure contribution with gradients\n","    \"\"\"\n","    assert tokens_type in ['razdel', 'natasha']\n","    global device\n","    global tokenizer\n","    global model_cls\n","    model_cls.eval()\n","    model_cls = model_cls.to(device)\n","\n","    def predict(\n","        input_ids: torch.tensor,\n","        attention_mask: torch.tensor\n","    ) -> torch.tensor:\n","        inputs = {\n","            'input_ids': input_ids,\n","            'attention_mask': attention_mask\n","        }\n","        logits = model_cls(**inputs)['logits']\n","        probs = torch.nn.functional.softmax(logits, dim=1)\n","        return probs\n","        \n","\n","    # восстанавливаем текст из слов\n","    text_to_explain = gather_back_tokens(tokens, tokens_type)\n","    # интегрированные градиенты по слоям модели\n","    lig = LayerIntegratedGradients(predict, model_cls.distilbert.embeddings)\n","    \n","    text_tokens = tokenizer.tokenize(text_to_explain)\n","    text_input = tokenizer(\n","        text_to_explain, return_tensors=\"pt\",\n","        padding=True, add_special_tokens=True,\n","        return_token_type_ids=use_tok_type_ids,\n","        return_offsets_mapping=True\n","    ).to(device)\n","    offset_mapping = text_input['offset_mapping'][0,1:-1,:].cpu().detach()\n","    text_input['return_dict'] = True\n","    del text_input['offset_mapping']\n","    \n","    # get words of text\n","    pos_words = [(word.start, word.stop) for word in tokens]\n","\n","    # create mapping from token to word\n","    cur_index = 0\n","    token_pos_to_word = dict()\n","    for idx, (offset, token) in enumerate(zip(offset_mapping, text_tokens)):\n","        start, _ = offset\n","        while start >= pos_words[cur_index][1]:\n","            cur_index += 1\n","        token_pos_to_word[idx] = tokens[cur_index]\n","\n","    # baseline от которого происходит расчет градиентов\n","    input_size = text_input[\"input_ids\"].size()\n","    baseline = torch.zeros(input_size).type(torch.LongTensor).to(device)\n","    # расчет интегрированных (вдоль пути) градиентов\n","    attributes, deltasa = lig.attribute(\n","        inputs=text_input[\"input_ids\"], baselines=baseline,\n","        additional_forward_args=(text_input[\"attention_mask\"]),\n","        n_steps=alpha_steps, target=torch.tensor(target).to(device),\n","        return_convergence_delta=True,\n","    )\n","    # получаем вклад каждого отдельного токена\n","    attributes = attributes.sum(dim=-1).squeeze(0)[1:-1]\n","    toks_contributions = torch.abs(attributes / torch.norm(attributes))\n","    \n","    word_to_contribution = defaultdict(lambda: 0.0)\n","    for idx, contribution in enumerate(toks_contributions):\n","        word_to_contribution[token_pos_to_word[idx]] += contribution.item()\n","    \n","    # функция для сортировки\n","    sort_func = lambda x: x[1]\n","    # сортируем токены по важности\n","    essential_words = sorted(\n","        [(word, cont) for word, cont in word_to_contribution.items()],\n","        key=sort_func, reverse=True\n","    )\n","    # возвращаем только слова и их позиции в тексте\n","    essential_words = [word for word, _ in essential_words]\n","\n","    return essential_words"]},{"cell_type":"markdown","metadata":{},"source":["### random important words"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2024-03-01T19:20:59.749794Z","iopub.status.busy":"2024-03-01T19:20:59.749049Z","iopub.status.idle":"2024-03-01T19:20:59.755736Z","shell.execute_reply":"2024-03-01T19:20:59.754572Z","shell.execute_reply.started":"2024-03-01T19:20:59.749765Z"},"trusted":true},"outputs":[],"source":["def extract_random_words(\n","    tokens: List[str]\n",") -> List[Tuple[str, int]]:\n","    \"\"\"\n","    возвращает список слов в случайном порядке\n","    \"\"\"\n","    permutation = np.random.permutation(len(tokens))\n","\n","    return [tokens[idx] for idx in permutation]"]},{"cell_type":"markdown","metadata":{},"source":["### extract most important words"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2024-03-01T19:21:05.240935Z","iopub.status.busy":"2024-03-01T19:21:05.240188Z","iopub.status.idle":"2024-03-01T19:21:32.041380Z","shell.execute_reply":"2024-03-01T19:21:32.040546Z","shell.execute_reply.started":"2024-03-01T19:21:05.240896Z"},"trusted":true},"outputs":[],"source":["# подгружаем обученные word2vec rusvectors \n","# from gensim.downloader import load\n","from ru_synonyms import SynonymsGraph\n","from gensim.models import KeyedVectors\n","# rus_vectors = load('word2vec-ruscorpora-300')\n","# clear_output()\n","# rus_vectors.save('rusvectors.d2v')\n","rus_vectors = KeyedVectors.load(\"/kaggle/input/rusvectors-uploaded/rusvectors.d2v\")"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2024-03-01T19:21:32.043402Z","iopub.status.busy":"2024-03-01T19:21:32.043113Z","iopub.status.idle":"2024-03-01T19:21:32.076689Z","shell.execute_reply":"2024-03-01T19:21:32.075689Z","shell.execute_reply.started":"2024-03-01T19:21:32.043377Z"},"trusted":true},"outputs":[],"source":["class SpoilTexts:\n","\n","    def __init__(self) -> None:\n","\n","        self.texts = None\n","        self.words_to_sub_per_text = None\n","        self.sg = SynonymsGraph()\n","        self.morph = pymorphy2.MorphAnalyzer()\n","\n","\n","    @staticmethod\n","    def clean_up_synonyms(\n","        synonyms_per_text: List[List[Tuple[str, int, int]]]\n","    ) -> List[List[Tuple[str, int, int]]]:\n","        \"\"\"\n","        удаляем лишние символы у полученных синонимов\n","        \"\"\"\n","        cleaned_synonyms_per_text = list()\n","        for synonyms in synonyms_per_text:\n","            cleaned_synonyms = list()\n","            for synonym in synonyms:\n","                if synonym is None:\n","                    cleaned_synonyms.append(None)\n","                    continue\n","                word, start, stop = synonym\n","                word = word.split('_')[0]\n","                word_parts = word.split('::')\n","                if len(word_parts) > 1:\n","                    word = word_parts[1]\n","                else:\n","                    word = word_parts[0]\n","                cleaned_synonyms.append((word, start, stop))\n","\n","            cleaned_synonyms_per_text.append(cleaned_synonyms)\n","\n","        return cleaned_synonyms_per_text\n","\n","\n","    def extract_most_valuable_words(\n","        self, texts: List[str], targets: List[Union[str, int]],\n","        method: str, return_words: bool=False, verbose: bool=False\n","    ) -> None:\n","        \"\"\"\n","        уорпдочиваем слова по их важности конкретным методом\n","        \"\"\"\n","        assert method in ['shap', 'lime', 'loss', 'alti', 'random', 'attention', 'gradient']\n","        self.texts = texts\n","\n","        if verbose:\n","            pbar = tqdm(len(self.texts), leave=True, position=0)\n","\n","        self.words_to_sub_per_text = list()\n","        for idx, (text, target) in enumerate(zip(texts, targets)):\n","\n","            tokens = list(razdel.tokenize(text))\n","            if len(tokens) == 1:\n","                self.words_to_sub_per_text.append(None)\n","                continue\n","            tokens_type = 'razdel'\n","\n","            if method == 'shap':\n","                words_to_del_order = shap_importance(tokens, tokens_type, target)\n","            elif method == 'lime':\n","                words_to_del_order = lime_importance(tokens, tokens_type)\n","            elif method == 'alti':\n","                words_to_del_order = alti_importance(tokens, tokens_type, 'cls')\n","            elif method == 'loss':\n","                words_to_del_order = loss_importance(tokens, target, tokens_type)\n","            elif method == 'random':\n","                words_to_del_order = extract_random_words(tokens)\n","            elif method == 'attention':\n","                words_to_del_order = attention_importance(tokens, tokens_type, 'cls')\n","            elif method == 'gradient':\n","                words_to_del_order = gradient_importance(tokens, tokens_type, target)\n","\n","            words_to_del_order = [\n","                (constr.text, constr.start, constr.stop) \n","                for constr in words_to_del_order\n","            ]\n","            self.words_to_sub_per_text.append(words_to_del_order)\n","\n","            if verbose and (idx % 100 == 1):\n","                pbar.set_description(f'processed: {idx + 1}')\n","                pbar.update(1)\n","\n","        if return_words:\n","            return self.words_to_sub_per_text\n","\n","\n","    def spoil_words(\n","        self, wordlen: int, words_to_sub_count: int,\n","        dist_to_synonym: int, topn: int=500, wordlen_syn: int=2,\n","        verbose: bool=False, clean_synonyms: bool=True\n","    ):\n","\n","        assert self.words_to_sub_per_text is not None\n","        assert wordlen > 0\n","        assert words_to_sub_count > 0\n","        assert dist_to_synonym > 0\n","        assert topn > 0\n","\n","        # отслеживание прогресса\n","        if verbose:\n","            pbar = tqdm(len(self.words_to_sub_per_text), leave=False, position=0)\n","        synonyms_per_text = list()\n","        for idx, words_to_sub in enumerate(self.words_to_sub_per_text):\n","\n","            spoiled_words, sub_words = list(), 0\n","            for word, start, stop in words_to_sub:\n","\n","                if sub_words >= words_to_sub_count:\n","                    break\n","                if len(word) < wordlen:\n","                    continue\n","\n","                # выделение нормальной формы слова и части речи\n","                parsed_word = self.morph.parse(word)[0]\n","                normal_form_word = parsed_word.normal_form.lower()\n","                tag = parsed_word.tag.POS\n","                tag = 'none' if tag is None else str(tag)\n","                rus_vectors_word = normal_form_word + f'_{tag.upper()}'\n","                synonyms = None\n","                try:\n","                    # пытаемся подобрать синониму по словарю rusvectors\n","                    synonyms = rus_vectors.most_similar(rus_vectors_word, topn=topn)\n","                except: pass\n","                    \n","                # в случае если нашли синоним\n","                if not synonyms is None:\n","                    # ищем синонимы с той же частью речи\n","                    synonyms_tag = [syn[0] for syn in synonyms if syn[0].split('_')[1] == tag]\n","                    synonyms_tag = list(filter(lambda x: len(x) >= wordlen_syn, synonyms_tag))\n","                    word_synonym = synonyms_tag[dist_to_synonym - 1]\n","                    # сохраняем синоним\n","                    spoiled_words.append((word_synonym, start, stop))\n","                    sub_words += 1\n","                    continue\n","\n","                # если не нашли синоним в rusvectors, то ищем в wordnet\n","                gen = None\n","                try:\n","                    if self.sg.is_in_dictionary(normal_form_word):\n","                        gen = list(self.sg.get_list(normal_form_word))\n","                        gen = list(filter(lambda x: len(x) >= wordlen_syn, gen))\n","                except: pass\n","\n","                if not gen is None and dist_to_synonym - 1 < len(gen):\n","                    word_synonym = gen[dist_to_synonym - 1]\n","                    # save synonym and idxes of word\n","                    spoiled_words.append((word_synonym, start, stop))\n","                    sub_words += 1\n","\n","            spoiled_words = sorted(spoiled_words, key=lambda x: x[1], reverse=False)\n","            synonyms_per_text.append(spoiled_words)\n","\n","            # шаг прогресса\n","            if verbose and (idx % 100 == 1):\n","                pbar.set_description(f'processed: {idx + 1}')\n","                pbar.update(1)\n","\n","        if clean_synonyms:\n","            return self.clean_up_synonyms(synonyms_per_text)\n","        else:\n","            return synonyms_per_text"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2024-03-01T19:21:32.077989Z","iopub.status.busy":"2024-03-01T19:21:32.077735Z","iopub.status.idle":"2024-03-01T19:21:32.114816Z","shell.execute_reply":"2024-03-01T19:21:32.113962Z","shell.execute_reply.started":"2024-03-01T19:21:32.077968Z"},"trusted":true},"outputs":[],"source":["def pass_mlm_model(\n","    mlm_text: str, return_prob: bool=False\n",") -> Tuple[str, Optional[float]]:\n","    \"\"\"\n","    получения выхода MLM модели от токенизированного текста с добавлением [MASK]\n","    \"\"\"\n","    global mlm_tokenizer\n","    global mlm_model\n","    global device\n","    mlm_model.eval()\n","    mask_tok_id = mlm_tokenizer.mask_token_id\n","\n","    inputs = mlm_tokenizer(\n","        mlm_text, return_token_type_ids=True, \n","        truncation=True, padding=True,\n","        return_tensors='pt'\n","    )\n","    mask_tok_ids = (inputs['input_ids'] == mask_tok_id)[0].nonzero(as_tuple=True)[0]\n","\n","    with torch.no_grad():\n","        logits = mlm_model(**inputs.to(device))['logits'].cpu().detach()\n","        inputs = inputs.to('cpu')\n","\n","    logits_mask = torch.squeeze(logits[0, mask_tok_ids], dim=0)\n","    # получение вероятностей\n","    probs = torch.nn.functional.softmax(logits_mask, dim=0)\n","    predicted_token_id = torch.argmax(probs)\n","    # очищаем ненужную часть от BPETokenizer\n","    new_token = mlm_tokenizer.decode(predicted_token_id).replace('#', '')\n","\n","    if return_prob:\n","        # если нужно вернуть вероятность токена\n","        token_prob = probs[predicted_token_id].numpy()\n","        return new_token, token_prob\n","    else:\n","        return new_token"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2024-03-01T19:21:32.117185Z","iopub.status.busy":"2024-03-01T19:21:32.116880Z","iopub.status.idle":"2024-03-01T19:21:32.133005Z","shell.execute_reply":"2024-03-01T19:21:32.132107Z","shell.execute_reply.started":"2024-03-01T19:21:32.117162Z"},"trusted":true},"outputs":[],"source":["def generate_spoiled_words_end(\n","    orig_texts: List[str], synonyms_per_text: List[Tuple[int, int, str]], \n","    iter_to_find: int=5, verbose: bool=False\n",") -> List[Tuple[int, int, str]]:\n","    \"\"\"\n","    функция для добавления окончания лемматизированного синонима\n","    \"\"\"\n","    if verbose:\n","        pbar = tqdm(len(orig_texts), position=0, leave=False)\n","    # проходимся по всем созданным заменам\n","    synonyms_end_per_text = list()\n","    for idx, (orig, synonyms) in enumerate(\n","        zip(orig_texts, synonyms_per_text)\n","    ):\n","\n","        synonyms_end = list()\n","        for synonym in synonyms:\n","            word, start, stop = synonym\n","            # токенизируем текст\n","            word_tokens = mlm_tokenizer.tokenize(word)\n","            word_tokens = [token.replace('#', '') for token in word_tokens]\n","            # если BPETokenizer разбил токен на замену на более чем 1 часть\n","            if len(word_tokens) > 1:\n","                # заменяем последнюю лексему слова на [MASK] и предсказываем\n","                word_tokens[-1] = '[MASK]'\n","                word_masked = ''.join(word_tokens)\n","\n","                text_copy = list(orig)\n","                text_copy[start:stop] = word_masked\n","\n","                word_end = pass_mlm_model(''.join(text_copy))\n","\n","                word_tokens[-1] = word_end.replace('#', '')\n","                synonyms_end.append((''.join(word_tokens), start, stop))\n","\n","            # если BPETokenizer разбил токена на замену на 1 часть\n","            else:\n","                ends_prob = list()\n","                # каждую итерацию удаляем один символ с конца\n","                # и заменяем удаленную часть на [MASK]\n","                for i in range(iter_to_find):\n","                    word_tokens = list(word)\n","                    # if there is no symbols to delete more\n","                    if len(word_tokens) < i + 2:\n","                        break\n","                    for _ in range(i):\n","                        word_tokens.pop(-1)\n","                    word_tokens.append('[MASK]')\n","                    word_masked = ''.join(word_tokens)\n","\n","                    text_copy = list(orig)\n","                    text_copy[start:stop] = word_masked\n","\n","                    new_end, end_prob = pass_mlm_model(''.join(text_copy), True)\n","                    ends_prob.append((new_end, end_prob, i))\n","                # sorted by the most possible token\n","                ends_prob = sorted(ends_prob, key=lambda x: x[1], reverse=True)[0]\n","                mp_end, symbols_to_delete = ends_prob[0], ends_prob[2]\n","\n","                # выбираем окончание с наибольшей вероятнсотью и заменяем i последних симболов на него\n","                if len(ends_prob) > 0: \n","                    initial_word = list(word)\n","                    for _ in range(symbols_to_delete):\n","                        initial_word.pop(-1)\n","                    initial_word.extend(mp_end.replace('#', ''))\n","                    synonyms_end.append((''.join(initial_word), start, stop))\n","                else:\n","                    synonyms_end.append((word, start, stop))\n","\n","        if verbose and (idx % 100 == 0):\n","            pbar.set_description(f'processed: {idx + 1}')\n","            pbar.update(1)\n","        \n","        synonyms_end_per_text.append(synonyms_end)\n","\n","    return synonyms_end_per_text"]},{"cell_type":"code","execution_count":41,"metadata":{"execution":{"iopub.execute_input":"2024-03-01T19:38:55.499419Z","iopub.status.busy":"2024-03-01T19:38:55.499044Z","iopub.status.idle":"2024-03-01T19:38:55.510490Z","shell.execute_reply":"2024-03-01T19:38:55.509440Z","shell.execute_reply.started":"2024-03-01T19:38:55.499389Z"},"trusted":true},"outputs":[],"source":["def generate_adv_texts(\n","    orig_texts: List[str], synonyms_end_per_text: List[Tuple[int, int, str]],\n","    verbose: bool=False\n",") -> List[str]:\n","\n","    if verbose:\n","        pbar = tqdm(len(orig_texts), leave=False, position=0)\n","\n","    adv_texts = list()\n","    for idx, (text, synonyms_end) in enumerate(\n","        zip(orig_texts, synonyms_end_per_text)\n","    ):\n","        # сортируем по встречаемости в тексте\n","        synonyms_end = sorted(synonyms_end, key=lambda x: x[1])\n","        # заменяем исходные слов в тексте испорченными\n","        shift = 0\n","        spoiled_text = list(text.lower())\n","        for word_end, start, stop in synonyms_end:\n","            spoiled_text[start + shift:stop + shift] = word_end\n","            shift += len(word_end) - (stop - start)\n","        spoiled_text = ''.join(spoiled_text)\n","        adv_texts.append(spoiled_text)\n","\n","        if verbose and (idx % 100 == 1):\n","            pbar.set_description(f'processed: {idx + 1}')\n","            pbar.update(1)\n","\n","    return adv_texts"]},{"cell_type":"code","execution_count":118,"metadata":{"execution":{"iopub.execute_input":"2024-02-18T11:32:01.209458Z","iopub.status.busy":"2024-02-18T11:32:01.209071Z","iopub.status.idle":"2024-02-18T11:32:01.928096Z","shell.execute_reply":"2024-02-18T11:32:01.927104Z","shell.execute_reply.started":"2024-02-18T11:32:01.209428Z"},"trusted":true},"outputs":[{"data":{"text/plain":["9"]},"execution_count":118,"metadata":{},"output_type":"execute_result"}],"source":["import gc\n","del mlm_model\n","torch.cuda.empty_cache()\n","gc.collect()"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2024-03-01T19:21:32.148237Z","iopub.status.busy":"2024-03-01T19:21:32.147890Z","iopub.status.idle":"2024-03-01T19:21:49.163591Z","shell.execute_reply":"2024-03-01T19:21:49.162685Z","shell.execute_reply.started":"2024-03-01T19:21:32.148202Z"},"trusted":true},"outputs":[],"source":["# импортируем предобученный BERT для задачи MLM\n","from transformers import AutoTokenizer, AutoModelForMaskedLM\n","model_cls = model_cls.to('cpu')\n","\n","model_name = 'ai-forever/ruBert-base'\n","mlm_tokenizer = AutoTokenizer.from_pretrained(model_name, model_max_length=512)\n","mlm_model = AutoModelForMaskedLM.from_pretrained(model_name)\n","clear_output()"]},{"cell_type":"code","execution_count":79,"metadata":{"execution":{"iopub.execute_input":"2024-03-01T20:14:52.929735Z","iopub.status.busy":"2024-03-01T20:14:52.928984Z","iopub.status.idle":"2024-03-01T20:29:49.101869Z","shell.execute_reply":"2024-03-01T20:29:49.100583Z","shell.execute_reply.started":"2024-03-01T20:14:52.929699Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["processed: 2202: : 23it [14:08, 36.91s/it]\n","                                          \r"]}],"source":["texts = adversial_examples['text'].apply(lambda x: x.lower())\n","targets = adversial_examples['0class']\n","spoil_texts_word = SpoilTexts()\n","method = 'lime'\n","wordlen, words_to_sub_count, dist_to_synonym = 4, 2, 2\n","\n","model_cls, mlm_model = model_cls.to('cpu'), mlm_model.to('cpu')\n","model_cls = model_cls.to('cuda:0')\n","spoil_texts_word.extract_most_valuable_words(\n","    texts=texts, targets=targets, method=method, verbose=True\n",")\n","\n","model_cls, mlm_model = model_cls.to('cpu'), mlm_model.to('cpu')\n","mlm_model = mlm_model.to('cuda:0')\n","synonyms_per_text = spoil_texts_word.spoil_words(\n","    wordlen=wordlen, words_to_sub_count=words_to_sub_count,\n","    dist_to_synonym=dist_to_synonym, verbose=True,\n","    clean_synonyms=True\n",")"]},{"cell_type":"code","execution_count":80,"metadata":{"execution":{"iopub.execute_input":"2024-03-01T20:29:49.105908Z","iopub.status.busy":"2024-03-01T20:29:49.104983Z","iopub.status.idle":"2024-03-01T20:33:10.999384Z","shell.execute_reply":"2024-03-01T20:33:10.998327Z","shell.execute_reply.started":"2024-03-01T20:29:49.105858Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["                                          \r"]}],"source":["synonyms_end_per_text = generate_spoiled_words_end(\n","    orig_texts=texts, synonyms_per_text=synonyms_per_text, verbose=True\n",")\n","model_cls, mlm_model = model_cls.to('cpu'), mlm_model.to('cpu')"]},{"cell_type":"code","execution_count":81,"metadata":{"execution":{"iopub.execute_input":"2024-03-01T20:33:11.000840Z","iopub.status.busy":"2024-03-01T20:33:11.000535Z","iopub.status.idle":"2024-03-01T20:33:11.055719Z","shell.execute_reply":"2024-03-01T20:33:11.054834Z","shell.execute_reply.started":"2024-03-01T20:33:11.000815Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["                                           \r"]}],"source":["adv_texts = generate_adv_texts(\n","    orig_texts=texts, synonyms_end_per_text=synonyms_end_per_text,\n","    verbose=True\n",")"]},{"cell_type":"code","execution_count":83,"metadata":{"execution":{"iopub.execute_input":"2024-03-01T20:33:11.137428Z","iopub.status.busy":"2024-03-01T20:33:11.137059Z","iopub.status.idle":"2024-03-01T20:33:11.142911Z","shell.execute_reply":"2024-03-01T20:33:11.142038Z","shell.execute_reply.started":"2024-03-01T20:33:11.137396Z"},"trusted":true},"outputs":[],"source":["adv_texts_df[f'{method}_{wordlen}_{words_to_sub_count}_{dist_to_synonym}'] = adv_texts"]},{"cell_type":"code","execution_count":87,"metadata":{"execution":{"iopub.execute_input":"2024-03-01T20:33:41.695954Z","iopub.status.busy":"2024-03-01T20:33:41.695576Z","iopub.status.idle":"2024-03-01T20:33:41.781574Z","shell.execute_reply":"2024-03-01T20:33:41.780648Z","shell.execute_reply.started":"2024-03-01T20:33:41.695925Z"},"trusted":true},"outputs":[],"source":["adv_texts_df.to_csv('adv_texts_word.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":3505809,"sourceId":6116952,"sourceType":"datasetVersion"},{"datasetId":3511519,"sourceId":6125391,"sourceType":"datasetVersion"},{"datasetId":4135938,"sourceId":7160860,"sourceType":"datasetVersion"},{"datasetId":4199751,"sourceId":7274163,"sourceType":"datasetVersion"},{"datasetId":4199760,"sourceId":7274165,"sourceType":"datasetVersion"},{"datasetId":4456516,"sourceId":7645481,"sourceType":"datasetVersion"},{"datasetId":4458768,"sourceId":7649537,"sourceType":"datasetVersion"}],"dockerImageVersionId":30646,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
